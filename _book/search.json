[
  {
    "objectID": "model-accuracy.html#readings",
    "href": "model-accuracy.html#readings",
    "title": "3  Model accuracy and fit",
    "section": "3.1 Readings",
    "text": "3.1 Readings\nISLR:\n\nChapter 2.2: Assessing Model Accuracy\nChapter 5.1: Resampling Methods: Cross-Validation"
  },
  {
    "objectID": "model-accuracy.html#how-do-we-estimate-f",
    "href": "model-accuracy.html#how-do-we-estimate-f",
    "title": "3  Model accuracy and fit",
    "section": "3.2 How do we estimate f?",
    "text": "3.2 How do we estimate f?\nto estimate training data with statistical learning methods: - parametric: make some assumption about the functional form of f, e .g. \\(Price = \\beta_0 + \\beta_1 * Tweets\\)\n - for every increase in one tweet, the bitcoin price will increase by 3,543 - methods: linear regression, with quadratic term, with higher-order-polynomials\n\nnon-parametric: not make an explicit assumptions about the functional form of f, e.g. price of bitcoin is the average of the 3 closest points in our data set\n\n\n\nmethods: non-parametric regression (LOESS, \\(y_i\\) from a “local” regression wihthin a window of its nearest neighbors) or K-NN regression ($y_i predicted from the vale of the closest neighbors)\n\n→ so different models to estimate f, which is the best? How to check if the model does a good job? - which predictors is the best? in more complexe models, vertain parameters have to be “tuned”. Which value for these tuned parameters is the best?\n\n3.2.1 What affects our ability to estimate f?\n\nirreducible error\nvariance of Y\nsample size\nmodel & task complexity\n\nExample for different \\(Var(\\epsilon)\\), “noise”\nThe bigger the “noise” the worse the estimate\n\n\nuse a more restrictive model\nbigger sample size\ninclude other variables\nuse a method, where some variables are hold fixed\n\nExample for sample size\nThe bigger the sample size, the better the estimate"
  },
  {
    "objectID": "model-accuracy.html#model-performance",
    "href": "model-accuracy.html#model-performance",
    "title": "3  Model accuracy and fit",
    "section": "3.3 Model performance",
    "text": "3.3 Model performance\nExample:\n\nthe left is underfitting the data, the right one is more close to the observations\nin practice it is good to know that up to a certain number of tweets, the bitcoin price increase slows down\n\n\nHow can you formalize this, compute the model performance?\n\n3.3.1 Measuring the Quality of Fit\nEvaluate the performance of a statistical learning method on a given data set in measuring how well its predictions actually match the observed data.\nFor Regression: mean squared error (MSE)\nThe Mean squared error (MSE) represents the error of the estimator or predictive model created based on the given set of observations in the sample. Intuitively, the MSE is used to measure the quality of the model based on the predictions made on the entire training dataset vis-a-vis the true label/output value. In other words, it can be used to represent the cost associated with the predictions or the loss incurred in the predictions. And, the squared loss (difference between true & predicted value) is advantageous because they exaggerate the difference between the true value and the predicted value.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2,\n\\] If the MSE is smaller, the predicted responses are very close to the true and vice versa.\nWith the training MSE, computed out of the training data (\\(\\hat{f}(x_i)\\) \\(\\approx\\) \\(y_i\\)) we are interested in the accuracy of the predictions with the test set (\\(\\hat{f}(x_0)\\) \\(\\approx\\) \\(y_0\\)) ) → we want to choose the method that gives us the lowest test MSE, not the lowest training MSE, so we compute the average squared prediction error for the test observations:\n\\[\nAve(y_0-\\hat{f}(x_0))^2\n\\] In some settings, test set is available → sample is split in train and test set What, when no test set available? The learning method is chosen that minimizes the training MSE.\nDegrees of freedom: A quantity, that summarizes the flexibility of a curve → the more restricted an estimate is, e. g. an estimate computed with regression, the less degrees of freedom the curve has.\nProblem of overfitting with taking the training MSE for test MSE\nAs model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.\n→ traing MSE is smaller than test MSE. Especially when a less flexible model would have yielded a smaller test MSE.\n\n\n3.3.1.1 Other measures of the model accury besides the MSE\n\nRoot mean squared error (RMSE): \\[\n\\sqrt{MSE}\n\\]\n\nMean absolute error (MAE): \\[\nMAE= \\frac{1}{n} \\sum_{i=1}^{n} |y - \\hat{y})^2\n\\]\n\nMedian absolute error (mAE): \\[\nmAE = median |y - \\hat{y})^2\n\\]\nr-Squared: Proportion of variance explained (R2): R2=correlation(y,y^)2\n\n\\[\nR^2 = correlation (y, \\hat{y})^2\n\\] \\[\nR^2 = \\frac{\\sum (\\hat{y}_i - \\overline{y})^2} {\\sum y_i - \\overline{y})^2}\n\\] \\[\nR^2 = 1- \\frac{MSE}{Var(y)}\n\\] R-Squared is the ratio of the sum of squares regression (SSR) and the sum of squares total (SST). Sum of Squares Regression (SSR) represents the total variation of all the predicted values found on the regression line or plane from the mean value of all the values of response variables. The sum of squares total (SST) represents the total variation of actual values from the mean value of all the values of response variables. R-squared value is used to measure the goodness of fit or best-fit line. The greater the value of R-Squared, the better is the regression model as most of the variation of actual values from the mean value get explained by the regression model. However, we need to take caution while relying on R-squared to assess the performance of the regression model. This is where the adjusted R-squared concept comes into the picture. This would be discussed in one of the later posts. R-Squared is also termed as the coefficient of determination. For the training dataset, the value of R-squared is bounded between 0 and 1, but it can become negative for the test dataset if the SSE is greater than SST. Greater the value of R-squared would also mean a smaller value of MSE. If the value of R-Squared becomes 1 (ideal world scenario), the model fits the data perfectly with a corresponding MSE = 0. As the value of R-squared increases and become close to 1, the value of MSE becomes close to 0.\nR-Squared is also termed the standardized version of MSE. R-squared represents the fraction of variance of the actual value of the response variable captured by the regression model rather than the MSE which captures the residual error.\n\nIn this class focus on MSE, with that we can compute our decision and intuition:\n\nExample: Which one is better?\n\nThe left method is overfitting the data and fit to the noise, difficult to interprete.\nWe want to find a model, that is in the middle, that is “just right”:\n\nSo we should not care too much about the MSE, because we already know this observations, we want to understand new data, predict what will happen and generalize.\n\n\n\n3.3.2 Bias-Variance Trade Off\nexpected test MSE can be decomposed into the sum of three fundamental quantities: variance of \\(\\hat{f}(x_0)\\) , squared bias of \\(\\hat{f}(x_0)\\) and variance of the error term \\(\\epsilon\\).\nThe expected test MSE at \\(x_0\\) is:\n\\[\nE(y_o -\\hat{f}(x_0))^2 =  Var(\\hat{f}(x_0)) + Bias([\\hat{f}(x_0))]^2 + Var(\\epsilon)\n\\]\n\\[\nE(MSE) = E(\\frac{1}{2} \\sum_{i=1}^{n} (y - \\hat{y})^2) \\\\\n= E(\\frac{1}{2} \\sum_{i=1}^{n} ( outcome_i - predicted_i)^2) \\\\\n= Bias^2(model) + Variance(model) Variance(\\epsilon)\n\\]\nThe expected test MSE refers to Ave, that we would obtain if repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). Overall expected test MSE can be computed by averaging \\(E(y_o -\\hat{f}(x_0))^2\\) over all possible values of \\(x_0\\) in the test set. → for getting a small test MSE, we need a method that simultaneously achieves low variance and low bias (because both is squared it can not be negative). Furthermore expected test MSE can never be lower than \\(Var(\\epsilon)\\), the irreducible error.\nVariance: refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. → more flexible models have higher variance, because it is more volatile. It adapts more to specific to the observations. Variance refers the sensitiviy of our model to small fluctuations in the training dataset. Since the training data are used to fit the statistical learning method, different training data sets will result in a different \\(\\hat{f}\\) . High variance often comes from overfitting.\nBias: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. In reality not only X influences Y, there are many other variables and linear relationships are unlikely.\nVariance\\((\\epsilon)\\): Irreducible error\nConsequently, the U-shape is result of 2 competing properties: Bias and Variance. If you choose a more flexible method, the Variance will increase but the bias will decrease and vice versa for a more restrictive model. So you need the ideal degree of flexibility, that your test MSE stays small. → that is the bias-variance trade off\nMSE is influenced by both bias and variance: Model with high bias: Model that is not able to capture the complexity of the phenomena Model with high variance: Model that easily overfits accidental patterns\nin general more flexible models will fit the training data more closely, but they have the problem of overfitting → in the test set you can see the flexible model is printing the noise to and so have at a certain degree higher MSE with increasing flexibility.\n\nComplexity: Possible definitions of complexity:\nAmount of information in data absorbed into model; Amount of compression performed on data by model; Number of effective parameters, relative to effective degrees of freedom in data. For example: More predictors, more complexity; Higher-order polynomial, more complexity\n\nModel complexity vs. interpretability:\n\n\n\n3.3.3 The classification setting\nIn this setting, \\(y_i\\) is not metric and the regression logic is not applicable. Most common approach for quantifying the accuracy of estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations:\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n} I(y_i \\neq\\hat{y_i}).\n\\]\n\\(\\hat{y_i}\\) is class label for the ith observation using \\(\\hat{f}\\). \\(I(y_i \\neq\\hat{y_i})\\) is indicator variable that equals 1 if \\(y_i \\neq\\hat{y_i}\\) and 0 if \\(y_i = \\hat{y_i}\\). If \\(I(y_i \\neq\\hat{y_i} = 0)\\) than the ith observation was classified correctly, otherwise it is misclassified.\nWe are interested in the test error rate: \\[\nAveI(y_0 \\neq\\hat{y_0})\n\\] → the smaller the test error, the better the classifier.\nThe Bayes Classifier\nIt is possible to show that the test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values:\n\\[\nPr(Y = j | X= x_0)\n\\]\nconditional probability, is the probability that \\(Y=j\\) given the observed predictor vector \\(x_0\\).\nIf there are only two categories (binary) the Bayes classifier predict one class if \\(Pr(Y = j | X= x_0) &gt; 0.5\\) and for class two otherwise.\nExample with simulated data, like we have the test set available:\n\nPurple line: here is the probability exactly 50 % → Bayes decision boundary, it is the boundary, from which both observations are assigned to the groups.\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate, the classifier will always choose the class for which the probability is the largest, so:\n\\[\n1- E( max_j Pr(y=j |X))\n\\] for all possible X.\nK-Nearest Neighbors\nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.\nK-Nearest Neighbors (KNN) classifier estimate the conditional distribution of Y given X and then classify a given observation to the class with highest estimated probability.\nGiven a positive integer K and a test observation \\(x_0\\), the KNN classifier first identifies the K points in the training data that are closest to \\(x_0\\), represented by \\(N_0\\).Then it estimates the conditional probability for class j as the fraction of points in \\(N_0\\), whose response values equal j:\n\\[\nPr(Y=j | X= x_0) = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i=j)\n\\] Finally, test observation \\(x_0\\) to the class with the largest probability like the Bayes classifier.\n\nThe choice of K has a drastic effect on the KNN classifier obtained. Figure 2.16 displays two KNN fits to the simulated data from Figure 2.13, using K = 1 and K = 100. When K = 1, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.\n\nJust as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. → As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits."
  },
  {
    "objectID": "model-accuracy.html#how-to-estimate-the-generalization-error-emse-reliably-training-and-test-set",
    "href": "model-accuracy.html#how-to-estimate-the-generalization-error-emse-reliably-training-and-test-set",
    "title": "3  Model accuracy and fit",
    "section": "3.4 How to estimate the generalization error E(MSE) reliably? Training and test set",
    "text": "3.4 How to estimate the generalization error E(MSE) reliably? Training and test set\nWhy do we want to predict on unseen data? Conceptually: We often want to understand/predict a general phenomena, not just the observations we already have Pragmatically: It allows to understand if our model is overfitting the data. Idea = you can’t overfit data if you don’t see that data.\nTry the function with a new data set. Best practice: Use new data from a later time point. In practice it is really difficult to get data from the future, if you would like predict something, so, we hace to use new observations from the intended prediction situation. Often you only have one dataset, which we split in 2.\nDatasets:\n\nTraining: train the model\nobservations used to fit \\(\\hat{f}(x)\\)\nValidation: compare between models, tune models, selscz features\nnew observations from the same source as training data (used several times to select model complexity)\ntest: assess E(MSE) of the final model\nNew observations from the intended prediction situation (to evaluate E(MSE) for your final model) –&gt; Only used once in our final model!\n\n\nSteps:\n\nyou will have overfitting only using the training data set\nusing the training data set to train different models\nusing the validation set to select the best model\nusing the test data set to evluate the accuracy of the model (you can not use the validation set, because it is biased. With the validation data set you have selected the best model, so if you would test it with that, too, it is biased)\ncomputed MSE of the test data set to come to the true MSE, the E(MSE) to show, if the model is reliably\n\n\n\nusing the MSE of our models in a new data set, we can see which model is really the best and now we can see that the left one is not the best one, because of overfitting it is not that good fitting to new data → to volatile to the obervations and has been fitted to the noise\n\nWhen comparing the models, you can see, which model is the best: (in real world we do not have the actual distribution of observations, we estimate them)\n - we can see that the quadratic regression is the best, the KNN regression is overfitting the data.\nExample in practice, if we have one dataset:\n\nShuffle the observations\nDivide into training (85%) and testing (15%)\nTraining –&gt; Divide into training (70%) and validation (15%)\nTune models, evaluating models using MSE on the validation set\nSelect the best model\nUse the training + validation set to retrain the final model\nEvaluate the model using the test data\n\nProblem of bias, because you compare samples, in which the actual worse model fits better with the other samples\n\nThe validation estimate of the test error can be highly variable\nOnly a subset of the observations are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set.\nif performance on validation and performance on test set, start over again\n\n\n\n3.4.1 How to split the datasets\nTraining: Bulk of observations (~50-80%)\nValidation and testing: Smaller subsets (~10-20%) –&gt; Should be representative in order to estimate E(MSE) accurately.\ne.g. without cross-validation\n\nTraining: 50-70%\nValidation: 15-25%\nTest: 15-25%\n\ne.g. with cross-validation\n\nTraining: 70-80% + 5-10 fold cross-validation to separate into training/validation\nTest: 20-30%\n\n\n\n3.4.2 considerations with the test dataest und cross validation\nThe idea is that the \\(MSE_{test}\\) is a good estimate of the \\(E(MSE)\\) (prediction / Bayes error) → This is only true if the test data is similar to the prediction data!\n\nsometimes a wrong model is better than a true model, on average better → selecting a simpler model can be better, if you want to show relationships, because world is too complex\nthese factors together determine what works best:\nhow close the function form of \\(\\hat{f}(x)\\) is to the true \\(f(x)\\).\nthe amount of irreducible variance \\((\\omega^2)\\)\nthe sample size (n)\nthe complexity of model (p/df or equivalent)\n\n\n\n3.4.3 Two alternatives of model selection:\n\nComparing statistical methods (e.g. linear regression vs knn regression)\nComparing models with different predictors included (e.g. linear regression - including predictors [X1, X2] vs [X1, X2, X3] )\nComparing two models with different hyperparameters (e.g. KNN regression using - the closest 3 vs 10 neighbors)"
  },
  {
    "objectID": "model-accuracy.html#resampling-methods-cross-validation",
    "href": "model-accuracy.html#resampling-methods-cross-validation",
    "title": "3  Model accuracy and fit",
    "section": "3.5 Resampling Methods: Cross-Validation",
    "text": "3.5 Resampling Methods: Cross-Validation\n\nMethods for estimating test error rates and thereby choosing the optimal level of flexibility for a given statistical learning method.\ninvolve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\ncross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility\nevaluating model´s perfomance known as model assessment, process of selcting the proper level of flexibility is known as model selection.\nbasic mechanism: a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations\n\nThe following explanations are first made for regression and afterwards for classification\n\n3.5.1 The Validation Set Approach\nRandomly dividing the available set of observations into a model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. → resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.\nThis is not only done once, it is done multiple times and tested than.\nProblem: The results are not that clear, how much polynominals would produce the lowest MSE. In the Figure each training data produces another degree of polynominal → only thing that can be stated for sure is that a linear fit is not adequate. → very variable → too few observations, overestimate the test error rate\n\n\n\n3.5.2 Leave-One-Out Cross Validation (LOOCV)\nOnly a single observation is used for validation. The statistical learning method is fit on the \\(n - 1\\) training observations, and a prediction \\(\\hat{y_1}\\) is made for the excluded observation, using its value \\(x_1\\).\nBecause \\((x_1, y_1)\\) was not used in the fitting process, \\(MSE_1 = (y_1-\\hat{y_1})^2\\) is approximately unbiased estimate for the test error. → because only one observation is highly variable as a check up, same procedure by selecting \\((x_2, y_2)\\) and so on. Tge LOOOCV estimate for the ttest MSE is the average of these \\(n\\) test error estimates:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i=1}^{n} MSE_i\n\\]\nadvantages:\n\nfar less biased, because only one observation is included in each run → not to overestimate the test error rate\nrunning LOOCV multiple times always yields the same results → no randomness in the splits\n\n\nLOOCV has the potential to be expensive to implement, since the model has to be fit \\(n\\) times → shortcut:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{y_i -\\hat{y_i}} {1-h_i})^2\n\\]\n\n3.5.2.1 high leverage points\nLeverage is the influence, that one point has on the regression line, if the point is left out. How huge the leverage of a point is, depends on how far away the observation from other observations is. This is not depend on how much an observation can explain the residual sum of sqares (RSS). It can be, that a point is high in leverage but only explain a little part of RSS and vice versa. Points with a high leverage called high-leverage points. Data points at the edge are more likely to have high leverage than the points in the center.\n\\(h_i\\) is the leverage (In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations. High-leverage points, if any, are outliers with respect to the independent variables):\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2} {\\sum_{i´=1}^{n} (xi´-\\overline{x})^2}\n\\]\nLike ordinary MSE, exept the ith residual is devided by \\(1-h_i\\). Leverage lies between 1/n and 1. Selects the amount that an observation influences its own fit. Hold not in general, in which case the model has to be refit n times.\n\n\n\n3.5.3 k-Fold Cross-Validation\nDivided sets of observations into k groups /folds of approximately equal size First fold is validation set, method is fit on the remaing k-1 folds \\(MSE_1\\) is then computed on the held-out fold → procedure is repeated -times, each time, with a different group of observations is validation set. k-fold CV estimate:\n\\[\nCV_(k) = \\frac{1}{k} \\sum_{i=1}^{k} MSE_i\n\\] Consequently, LOOCV as a special case of the k-fold CV in which k is set to equal n. In practice, k-folds CV often performs using k=5 or k=10. Advantage: LOOCV requires fitting the statistical learning method n times. This has the potential to be computationally expensive → then only 5 or 10 times the learning procedure is fitted, more feasible.\n\n\nonly split in training and test data set\n‘Cross validation’ often used to replace single dev set approach;\nInstead of dividing one time the training dataset (into train/dev), do it many times.\nPerform the train/dev split several times, and average the result.\nUsually K = 5 or K = 10.\nWhen K = N, ‘leave-one-out’;\n\n In all three plots, the two cross-validation estimates are very similar.\nModel assessment: When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest.\nModel selection: But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility—that is, the flexibility level corresponding to the smallest test MSE.\n\n3.5.3.1 Bias-Variance Trade-Off for k-Fold Cross-Validations\nk-fold CV has computational advantage to LOOCV and often gives more accurate estimates of test error rate → because of bias-variance trade-off:\n\nLOOCV has less bias, because almost all observations are used in the training sets every time. k-fold CV in comparison more biased, because each training set exclude more observations.\nLOOCV has higher variance than k-fold CV. In LOOCV averaging the outputs of n fitted models, each of which is trained on an almost identical set of obesrvations → correlation between them is high. Have higher variance, so the test error estimate tends to have higher variance. k-Fold CV with k &lt; n less correlation,overlap bewtween training sets is smaller.\n\nTo summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance."
  },
  {
    "objectID": "model-accuracy.html#in-r",
    "href": "model-accuracy.html#in-r",
    "title": "3  Model accuracy and fit",
    "section": "3.6 in R",
    "text": "3.6 in R\nTraining, Validation, Test:\nlibrary(caret)\n# define the training partition \ntrain_index &lt;- createDataPartition(Boston$medv, p = .7, \n                       list = FALSE, \n                       times = 1)\n\n# split the data using the training partition to obtain training data\nboston_train &lt;- Boston[train_index,]\n\n# remainder of the split is the validation and test data (still) combined \nboston_val_and_test &lt;- Boston[-train_index,]\n\n# split the remaining 30% of the data in a validation and test set\nval_index &lt;- createDataPartition(boston_val_and_test$medv, p = .6, \n                       list = FALSE, \n                       times = 1)\n\nboston_valid &lt;- boston_val_and_test[val_index,]\nboston_test  &lt;- boston_val_and_test[-val_index,]\n\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~70%)\n# validation (~20%)\n# test (~10%)\n\n# Note that creating the partitions using the `y` argument (letting the function know what your dependent variable will be in the analysis), makes sure that when your dependent variable is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.\n\n\n\n#Then train the model 1 only with the train data\nmodel_1 &lt;- lm(medv ~ lstat, data = boston_train)\nsummary(model_1)\n\n\n# Train model 2 only with train data \nmodel_2 &lt;- lm(medv ~ lstat + age + tax, data = boston_train)\nsummary(model_2)\n\n#MSE for trained models only show, how well the model perform on the trained data set! For evaluating which model you should choose, validation data set\n\n  # MSE function for evaluation of accuracy \nmse &lt;- function(y_true, y_pred) {\n  mean((y_true - y_pred)^2)\n}\n\n  # Calculate the MSE for validation\nmodel_1_mse_valid &lt;- mse(y_true = boston_valid$medv, \n                 y_pred = predict(object = model_1, newdata = boston_valid))\nmodel_2_mse_valid &lt;- mse(y_true = boston_valid$medv, \n                 y_pred = predict(model_2, newdata = boston_valid))\n\n# Choose your model (in this case model 2) based on the lower value of validation MSE, because you want the better out-of-sample prediction\n\n# estimate accuracy of your selected model\n\n  #first: train your model again using this time train and validation data\nmodel_2b &lt;- lm(medv ~ lstat + age + tax, data = bind_rows(boston_train, boston_valid))\nsummary(model_2b)\n\n  #second: predict on the test data:\nmodel_2_mse_test &lt;- mse(y_true = boston_test$medv, \n                y_pred = predict(model_2b, newdata = boston_test))\n  #inspect the MSE \nmodel_2_mse_test\n\n  #compute the R(MSE)\nAnother quantity that we calculate is the Root Mean Squared Error (RMSE). It is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis.\nKey point: The RMSE is thus the distance, on average, of a data point from the fitted line, measured along a vertical line.\n# The estimate for the expected amount of error when predicting the median value of a not previously seen town in Boston when  using this model is:\n\nsqrt(model_2_mse_test)\ncross-validation\n# Just for reference, here is the mse() function once more\nmse &lt;- function(y_true, y_pred) mean((y_true - y_pred)^2)\n\ncv_lm &lt;- function(formula, dataset, k) {\n  # We can do some error checking before starting the function\n  stopifnot(is_formula(formula))     # formula must be a formula\n  stopifnot(is.data.frame(dataset))   # dataset must be data frame\n  stopifnot(is.integer(as.integer(k))) # k must be convertible to int\n  \n  # first, add a selection column to the dataset as before\n  n_samples  &lt;- nrow(dataset)\n  select_vec &lt;- rep(1:k, length.out = n_samples)\n  data_split &lt;- dataset %&gt;% mutate(folds = sample(select_vec))\n  \n  # initialise an output vector of k mse values, which we \n  # will fill by using a _for loop_ going over each fold\n  mses &lt;- rep(0, k)\n  \n  # start the for loop\n  for (i in 1:k) {\n   # split the data in train and validation set\n   data_train &lt;- data_split %&gt;% filter(folds != i)\n   data_valid &lt;- data_split %&gt;% filter(folds == i)\n   \n   # calculate the model on this data\n   model_i &lt;- lm(formula = formula, data = data_train)\n   \n   # Extract the y column name from the formula\n   y_column_name &lt;- as.character(formula)[2]\n   \n   # calculate the mean square error and assign it to mses\n   mses[i] &lt;- mse(y_true = data_valid[[y_column_name]],\n             y_pred = predict(model_i, newdata = data_valid))\n  }\n  \n  # now we have a vector of k mse values. All we need is to\n  # return the mean mse!\n  mean(mses)\n}\n\n# use the formula to perfom a cross-validation for the model\ncv_lm(formula = medv ~ lstat + age + tax, dataset = Boston, k = 9)\n  \n# the output is the test MSE \n\n3.6.1 Cross-Validation on Classification Problems\nLOOCV error rate and analogously to that the k-fold CV and validation set error rates:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i_1}^{n} Err_i\n\\] \\[\nErr_i = I(y_i \\neq \\hat{y_i})\n\\]\nlogistic regression has not enough flexibility often, therefore an extension is needed → using a polynomial functions of the predictors, e. g. an quadratic logistic regression model with 2 degrees of freedom:\n\\[\nlog( \\frac{p}{1-p}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X1^2 + \\beta_3 X_2 + \\beta_4X_2^2\n\\]\n\nIn practice, for real data, the Bayes decision boundary and the test error rates are unknown → cross-validation\n\n→ the 10-fold CS error rate provides a pretty good approximation to the test error rate\n→ 10-fold CV error indicates the best value for K, training error rate declies as the method becomes more flexible, so cannot be used to select the optimal value for K.\n\n\n3.6.2 Practice\nIn this lab, you will learn how to plot a linear regression with confidence and prediction intervals, and various tools to assess model fit: calculating the MSE, making train-test splits, and writing a function for cross validation. You can download the student zip including all needed files for practical 3 here.\nWe will use the Boston dataset, which is in the MASS package that comes with R. In addition, we will make use of the caret package in Part 2 to divide the Boston dataset into a training, test, and validation set.\n\nlibrary(ISLR)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(scales)\n\n\nInspect the Boston dataset using the View() function\n\n\nview(Boston)\n?Boston\n\nstarting httpd help server ... done\n\n\nThe Boston dataset contains the housing values and other information about Boston suburbs. We will use the dataset to predict housing value (the variable medv, here the outcome/dependent variable) by socio-economic status (the variable lstat, here the predictor / independent variable).\nmedv → Y - median value of owner occupied homes in $1000 steps - metric variable lstat → X - lower status of popultation - in percent, metric variable\nsample size → 506 rows\nLet’s explore socio-economic status and housing value in the dataset using visualization.\n\nCreate a scatter plot from the Boston dataset with lstat mapped to the x position and medv mapped to the y position. Store the plot in an object called p_scatter.\n\n\np_scatter &lt;- ggplot(Boston, aes(x=lstat, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter\n\n\n\n\n\np_scatter2 &lt;- ggplot(Boston, aes(x=lstat, y=medv*1000))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  scale_y_continuous(labels=scales::label_dollar())+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter2\n\n\n\n\n\n\n3.6.3 Plotting linear regression including a prediction line\nWe’ll start with making and visualizing the linear model. As you know, a linear model is fitted in R using the function lm(), which then returns a lm object. We are going to walk through the construction of a plot with a fit line. During the part done within the lab, we will add prediction and confidence intervals from an lm object to this plot.\nFirst, we will create the linear model. This model will be used to predict outcomes for the current data set, and - further along in this lab - to create new data.\n\nCreate a linear model object called lm_ses using the formula medv ~ lstat and the Boston dataset.\n\n\nlm_ses &lt;- lm(medv ~ lstat, data= Boston)\n\nYou have now trained a regression model with medv (housing value) as the outcome/dependent variable and lstat (socio-economic status) as the predictor / independent variable.\nRemember that a regression estimates \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope) in the following equation:\n\\[\\boldsymbol{y} = \\beta_0 + \\beta_1\\cdot \\boldsymbol{x}_1 + \\boldsymbol{\\epsilon}\\]\n\np_scatter3 &lt;- ggplot(Boston, aes(x=lstat, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  geom_smooth(method=lm, se=TRUE, color=\"slategray2\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nUse the function coef() to extract the intercept and slope from the lm_ses object. Interpret the slope coefficient.\n\n\ncoef(lm_ses)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\n\nThere is a negative correlation between the median value of owner occupied houses and the percentage of population with lower status.\nThe intercept is really high. So with no individuals with lower status lives near by the house, the housing price is 34 000$. If the percentage of people with lower status increases by one, the median value of the owner-occupied homes decreases bei 1000 $.\n\n\nUse summary() to get a summary of the lm_ses object. What do you see? You can use the help file ?summary.lm.\n\n\nsummary(lm_ses)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can now see the more information about the linear regression.\n\nthe weighted residuals, the usual residuals rescaled by the square root of the weights specified in the call to lm\ndegrees of freedom, a 3-vector (p, n-p, p*), the first being the number of non-aliased coefficients, the last being the total number of coefficients.\nthe coefficients a p x 4 matrix with columns for the estimated coefficient, its standard error, t-statistic and corresponding (two-sided) p-value. Aliased coefficients are omitted.\nf statistic (for models including non-intercept terms) a 3-vector with the value of the F-statistic with its numerator and denominator degrees of freedom.\nr.squared \\(R^2\\), the ‘fraction of variance explained by the model’,\n\n\\[\nR^2 = 1 - \\frac{ \\sum{R_i^2}}{ \\sum{(y_i - \\hat{y})^2}}\n\\] where $ is the mean of $y_i}f there is an intercept and zero otherwise.\nWe now have a model object lm_ses that represents the formula\n\\[\\text{medv}_i = 34.55 - 0.95 * \\text{lstat}_i + \\epsilon_i\\]\nWith this object, we can predict a new medv value by inputting its lstat value. The predict() method enables us to do this for the lstat values in the original dataset.\n\nSave the predicted y values to a variable called y_pred\n\n\ny_pred &lt;- predict(lm_ses, newdata=Boston)\nhead(y_pred)\n\n       1        2        3        4        5        6 \n29.82260 25.87039 30.72514 31.76070 29.49008 29.60408 \n\n\n\nCreate a scatter plot with y_pred mapped to the x position and the true y value (Boston$medv) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?\n\n\ndata &lt;- data.frame( medv  = Boston$medv, \n              y_pred = y_pred)\npred_scatter &lt;- ggplot(data, aes(x=y_pred, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Prediction of median value of house\", x= \"prediction\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\npred_scatter\n\n\n\n\nIf the prediction would be perfect, we had a linear regression line, because the real and the predicted value are not the same, although we have used the same data set.\n\n\n3.6.4 Plotting linear regression with confindence or prediction intervals\nWe will continue with the Boston dataset, the created model lm_ses that predicts medv (housing value) by lstat (socio-economic status), and the predicted housing values stored in y_pred.\nIn addition to predicting housing values for values of lstat observed in the Boston dataset, we also can generate predictions from new values using the newdat argument in the predict() method. For that, we need to prepare a data frame with new values for the original predictors.\nOne method of number generation, is through using the function seq(), this function from base R generates a sequence of number using a standardized method. Typically length of the requested sequence divided by the range between from to to. For more information call ?seq.\n\nUse the seq() function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with (data.frame() or tibble()) as its column name lstat. Name the data frame pred_dat.\n\n\npred_dat &lt;- tibble(lstat=seq(0,40, length.out=1000))\n\n\n\nUse the newly created data frame, from Question 8, as the newdata argument to a predict() call for lm_ses. Store it in a variable named y_pred_new.\n\n\n\ny_pred_new &lt;- predict(lm_ses, newdata=pred_dat)\n\nNow, we’ll continue with the plotting part by adding a prediction line to the plot we previously constructed.\n\n\nAdd the vector y_pred_new to the pred_dat data frame with the name medv.\n\n\n\npred_dat &lt;- pred_dat %&gt;% mutate(medv=y_pred_new)\n\n\nAdd a geom_line() to p_scatter from Question 2, with pred_dat as the data argument. What does this line represent?\n\n\np_scatter &lt;- ggplot()+\n  geom_point(data=Boston, mapping =aes(x=lstat, y=medv, alpha=0.5))+\n  geom_line(data= pred_dat,mapping= aes(x=lstat, y=medv))+\n   labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter\n\n\n\n\nThis line represents predicted values of medv for the values of lstat.\n\nThe interval argument can be used to generate confidence or prediction intervals. Create a new object called y_pred_95 using predict() (again with the pred_dat data) with the interval argument set to “confidence”. What is in this object?\n\n\n?predict\ny_pred_95 &lt;- predict(lm_ses, newdata =  pred_dat, interval = \"confidence\")\n\n\nUsing the data from Question 11, and the sequence created in Question 8; create a data frame with 4 columns: medv, lstat, lower, and upper.\n\n\ndata &lt;- cbind(pred_dat, y_pred_95)\ndata &lt;- data %&gt;% select(lstat, medv, lwr, upr)\nhead(data)\n\n       lstat     medv      lwr      upr\n1 0.00000000 34.55384 33.44846 35.65922\n2 0.04004004 34.51580 33.41307 35.61853\n3 0.08008008 34.47776 33.37768 35.57784\n4 0.12012012 34.43972 33.34229 35.53715\n5 0.16016016 34.40168 33.30690 35.49646\n6 0.20020020 34.36364 33.27150 35.45578\n\n\n\nAdd a geom_ribbon() to the plot with the data frame you just made. The ribbon geom requires three aesthetics: x (lstat, already mapped), ymin (lower), and ymax (upper). Add the ribbon below the geom_line() and the geom_points() of before to make sure those remain visible. Give it a nice colour and clean up the plot, too!\n\n\nBoston %&gt;% \n  ggplot(aes(x = lstat, y = medv)) + \n  geom_ribbon(aes(ymin = lwr, ymax = upr), data = data, fill = \"#00008b44\") +\n  geom_point(colour = \"#883321\") + \n  geom_line(data = pred_dat, colour = \"#00008b\", size = 1) +\n  theme_minimal() + \n  labs(x   = \"Proportion of low SES households\",\n     y   = \"Median house value\",\n     title = \"Boston house prices\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nExplain in your own words what the ribbon represents.\n\n\n# The ribbon represents the 95% confidence interval of the fit line.\n# The uncertainty in the estimates of the coefficients are taken into\n# account with this ribbon. \n\n# You can think of it as:\n# upon repeated sampling of data from the same population, at least 95% of\n# the ribbons will contain the true fit line.\n\n\nDo the same thing, but now with the prediction interval instead of the confidence interval.\n\n\n# pred with pred interval\ny_pred_95 &lt;- predict(lm_ses, newdata = pred_dat, interval = \"prediction\")\n\n\n# create the df\ngg_pred &lt;- tibble(\n  lstat = pred_dat$lstat,\n  medv  = y_pred_95[, 1],\n  l95  = y_pred_95[, 2],\n  u95  = y_pred_95[, 3]\n)\n\n# Create the plot\nBoston %&gt;% \n  ggplot(aes(x = lstat, y = medv)) + \n  geom_ribbon(aes(ymin = l95, ymax = u95), data = gg_pred, fill = \"#00008b44\") +\n  geom_point(colour = \"#883321\") + \n  geom_line(data = pred_dat, colour = \"#00008b\", size = 1) +\n  theme_minimal() + \n  labs(x    = \"Proportion of low SES households\",\n     y    = \"Median house value\",\n     title = \"Boston house prices\")\n\n\n\n\nWhile the confidence interval indiciates the uncertainty surrounding the average of y over the sample, the prediction interval quantify the uncertainty for a particular observation. A prediction interval is a type of confidence interval (CI) used with predictions in regression analysis; it is a range of values that predicts the value of a new observation, based on your existing model. Similarly, the prediction interval tells you where a value will fall in the future, given enough samples, a certain percentage of the time. A 95% prediction interval of 100 to 110 hours for the mean life of a battery tells you that future batteries produced will fall into that range 95% of the time. There is a 5% chance that a battery will not fall into this interval. the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty\n\n\n3.6.5 Model fit using the mean square error\nNext, we will write a function to assess the model fit using the mean square error: the square of how much our predictions on average differ from the observed values.\n\nWrite a function called mse() that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.\n\nStart like so:\n\nmse &lt;- function(y_true, y_pred) {\n  mean((y_true - y_pred)^2)\n}\n\nWikipedia may help for the formula.\n\nMake sure your mse() function works correctly by running the following code.\n\nmse(1:10, 10:1)\nIn the code, we state that our observed values correspond to \\(1, 2, ..., 9, 10\\), while our predicted values correspond to \\(10, 9, ..., 2, 1\\). This is graphed below, where the blue dots correspond to the observed values, and the yellow dots correspond to the predicted values. Using your function, you have now calculated the mean squared length of the dashed lines depicted in the graph below. If your function works correctly, the value returned should equal 33.\nVisualiation of this:\n\n\n\n\n\n\nCalculate the mean square error of the lm_ses model. Use the medv column as y_true and use the predict() method to generate y_pred.\n\n\nmse(Boston$medv, predict(lm_ses))\n\n[1] 38.48297\n\n\nIt is not the same values, it is squared, you must use the squared root to get the right unit and not the squared one.\n\nsqrt(mse(Boston$medv, predict(lm_ses)))\n\n[1] 6.203464\n\n\nYou have calculated the mean squared length of the dashed lines in the plot below. As the MSE is computed using the data that was used to fit the model, we actually obtained the training MSE. Below we continue with splitting our data in a training, test and validation set such that we can calculate the out-of sample prediction error during model building using the validation set, and estimate the true out-of-sample MSE using the test set.\n\n\n\n\n\nNote that you can also easily obtain how much the predictions on average differ from the observed values in the original scale of the outcome variable. To obtain this, you take the root of the mean square error. This is called the Root Mean Square Error, abbreviated as RMSE."
  },
  {
    "objectID": "model-accuracy.html#obtaining-train-validation-test-splits",
    "href": "model-accuracy.html#obtaining-train-validation-test-splits",
    "title": "3  Model accuracy and fit",
    "section": "3.7 Obtaining train-validation-test splits",
    "text": "3.7 Obtaining train-validation-test splits\nNext, we will use the caret package and the function createDataPartition() to obtain a training, test, and validation set from the Boston dataset. For more information on this package, see the caret website. The training set will be used to fit our model, the validation set will be used to calculate the out-of sample prediction error during model building, and the test set will be used to estimate the true out-of-sample MSE.\n\nUse the code given below to obtain training, test, and validation set from the Boston dataset.\n\n\nlibrary(caret)\n# define the training partition \ntrain_index &lt;- createDataPartition(Boston$medv, p = .7, \n                       list = FALSE, \n                       times = 1)\n\n# split the data using the training partition to obtain training data\nboston_train &lt;- Boston[train_index,]\n\n# remainder of the split is the validation and test data (still) combined \nboston_val_and_test &lt;- Boston[-train_index,]\n\n# split the remaining 30% of the data in a validation and test set\nval_index &lt;- createDataPartition(boston_val_and_test$medv, p = .6, \n                       list = FALSE, \n                       times = 1)\n\nboston_valid &lt;- boston_val_and_test[val_index,]\nboston_test  &lt;- boston_val_and_test[-val_index,]\n\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~70%)\n# validation (~20%)\n# test (~10%)\n\nNote that creating the partitions using the y argument (letting the function know what your dependent variable will be in the analysis), makes sure that when your dependent variable is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.\nWe will set aside the boston_test dataset for now.\n\nTrain a linear regression model called model_1 using the training dataset. Use the formula medv ~ lstat like in the first lm() exercise. Use summary() to check that this object is as you expect.\n\n\nmodel_1 &lt;- lm(medv ~ lstat, data = boston_train)\nsummary(model_1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = boston_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.053  -3.829  -1.109   2.046  24.584 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.29673    0.64270   53.36   &lt;2e-16 ***\nlstat       -0.93191    0.04365  -21.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.878 on 354 degrees of freedom\nMultiple R-squared:  0.5629,    Adjusted R-squared:  0.5617 \nF-statistic: 455.9 on 1 and 354 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculate the MSE with this object. Save this value as model_1_mse_train.\n\nSys.setenv(LANG = “en”)\n\nmodel_1_mse_train &lt;- mse(boston_train$medv, predict(model_1, boston_train))\nmodel_1_mse_train\n\n[1] 34.36108\n\n\n\nNow calculate the MSE on the validation set and assign it to variable model_1_mse_valid. Hint: use the newdata argument in predict().\n\n\nmodel_1_mse_valid &lt;- mse(boston_valid$medv, predict(model_1, boston_valid))\nmodel_1_mse_valid\n\n[1] 46.27183\n\n\nThis is the estimated out-of-sample mean squared error.\n\nCreate a second model model_2 for the train data which includes age and tax as predictors. Calculate the train and validation MSE.\n\n\nmodel_2 &lt;- lm(medv ~ lstat + age+ tax, data= boston_train)\nmodel_2_mse_train &lt;- mse(y_true= boston_train$medv, y_pred= predict(model_2, boston_train))\nmodel_2_mse_train\n\n[1] 32.94162\n\n\n\nmodel_2_mse_valid &lt;- mse(y_true =boston_valid$medv, y_pred=predict(model_2, boston_valid))\nmodel_2_mse_valid\n\n[1] 44.8726\n\n\nIf you are interested in out-of-sample prediction, the answer may depend on the random sampling of the rows in the dataset splitting: everyone has a different split. However, it is likely that model_2 has both lower training and validation MSE. In choosing the best model, you should base your answer on the validation MSE. Using the out of sample mean square error, we have made a model decision (which parameters to include, only lstat, or using age and tax in addition to lstat to predict housing value). Now we have selected a final model.\n\nCompare model 1 and model 2 in terms of their training and validation MSE. Which would you choose and why?\n\nModel 1 is better, model 2 is overfitting data. Model 1 is better in the validation set and so less biased to the sample split.\nI would choose model\n\nmodel_2b &lt;- lm(medv ~ lstat + age + tax, data = bind_rows(boston_train, boston_valid))\nsummary(model_2b)\n\n\nCall:\nlm(formula = medv ~ lstat + age + tax, data = bind_rows(boston_train, \n    boston_valid))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.733  -3.830  -1.165   1.823  24.676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.385250   0.840368  40.917  &lt; 2e-16 ***\nlstat       -0.979819   0.052614 -18.623  &lt; 2e-16 ***\nage          0.046593   0.013140   3.546 0.000433 ***\ntax         -0.006581   0.002043  -3.220 0.001375 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.973 on 444 degrees of freedom\nMultiple R-squared:  0.5697,    Adjusted R-squared:  0.5668 \nF-statistic:   196 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\n\nIn choosing the best model, you should base your answer on the validation MSE. Using the out of sample mean square error, we have made a model decision (which parameters to include, only lstat, or using age and tax in addition to lstat to predict housing value). Now we have selected a final model.\n\nFor your final model, retrain the model one more time using both the training and the validation set. Then, calculate the test MSE based on the (retrained) final model. What does this number tell you?\n\n\nmodel_2_mse_test &lt;- mse(y_true = boston_test$medv, \n                y_pred = predict(model_2b, newdata = boston_test))\nmodel_2_mse_test\n\n[1] 49.05221\n\n# The estimate for the expected amount of error when predicting \n# the median value of a not previously seen town in Boston when \n# using this model is:\n\nsqrt(model_2_mse_test)\n\n[1] 7.003729\n\n\nAs you will see during the remainder of the course, usually we set apart the test set at the beginning and on the remaining data perform the train-validation split multiple times. Performing the train-validation split multiple times is what we for example do in cross validation (see below). The validation sets are used for making model decisions, such as selecting predictors or tuning model parameters, so building the model. As the validation set is used to base model decisions on, we can not use this set to obtain a true out-of-sample MSE. That’s where the test set comes in, it can be used to obtain the MSE of the final model that we choose when all model decisions have been made. As all model decisions have been made, we can use all data except for the test set to retrain our model one last time using as much data as possible to estimate the parameters for the final model."
  },
  {
    "objectID": "model-accuracy.html#optional-cross-validation-advanced",
    "href": "model-accuracy.html#optional-cross-validation-advanced",
    "title": "3  Model accuracy and fit",
    "section": "3.8 Optional: cross-validation (advanced)",
    "text": "3.8 Optional: cross-validation (advanced)\nThis is an advanced exercise. Some components we have seen before in this lab, but some things will be completely new. Try to complete it by yourself, but don’t worry if you get stuck. If you don’t know about for loops in R, read up on those before you start the exercise, for example by reading the Basics: For Loops tab on the course website.\nUse help in this order:\n\nR help files\nInternet search & stack exchange\nYour peers\nThe answer, which shows one solution\n\nYou may also just read the answer when they have been made available and try to understand what happens in each step.\n\nCreate a function that performs k-fold cross-validation for linear models.\n\nInputs:\n\nformula: a formula just as in the lm() function\ndataset: a data frame\nk: the number of folds for cross validation\nany other arguments you need necessary\n\nOutputs:\n\nMean square error averaged over folds\n\n\n# Just for reference, here is the mse() function once more\nmse &lt;- function(y_true, y_pred) mean((y_true - y_pred)^2)\n\ncv_lm &lt;- function(formula, dataset, k) {\n  # We can do some error checking before starting the function\n  stopifnot(is_formula(formula))     # formula must be a formula\n  stopifnot(is.data.frame(dataset))   # dataset must be data frame\n  stopifnot(is.integer(as.integer(k))) # k must be convertible to int\n  \n  # first, add a selection column to the dataset as before\n  n_samples  &lt;- nrow(dataset)\n  select_vec &lt;- rep(1:k, length.out = n_samples)\n  data_split &lt;- dataset %&gt;% mutate(folds = sample(select_vec))\n  \n  # initialise an output vector of k mse values, which we \n  # will fill by using a _for loop_ going over each fold\n  mses &lt;- rep(0, k)\n  \n  # start the for loop\n  for (i in 1:k) {\n   # split the data in train and validation set\n   data_train &lt;- data_split %&gt;% filter(folds != i)\n   data_valid &lt;- data_split %&gt;% filter(folds == i)\n   \n   # calculate the model on this data\n   model_i &lt;- lm(formula = formula, data = data_train)\n   \n   # Extract the y column name from the formula\n   y_column_name &lt;- as.character(formula)[2]\n   \n   # calculate the mean square error and assign it to mses\n   mses[i] &lt;- mse(y_true = data_valid[[y_column_name]],\n             y_pred = predict(model_i, newdata = data_valid))\n  }\n   \n  # now we have a vector of k mse values. All we need is to\n  # return the mean mse!\n  mean(mses)\n}\n\n\nUse your function to perform 9-fold cross validation with a linear model with as its formula medv ~ lstat + age + tax. Compare it to a model with as formula medv ~ lstat + I(lstat^2) + age + tax.\n\n\ncv_lm(formula = medv ~ lstat + age + tax, dataset = Boston, k = 9)\n\n[1] 37.54383\n\n\n\ncv_lm(formula = medv ~ lstat + I(lstat^2) + age + tax, dataset = Boston, k = 9)\n\n[1] 28.08334"
  },
  {
    "objectID": "model-accuracy.html#conclusions",
    "href": "model-accuracy.html#conclusions",
    "title": "3  Model accuracy and fit",
    "section": "3.9 Conclusions",
    "text": "3.9 Conclusions\nWe want to learn a model of our data\n\nWe don’t want to underfit (will have high MSE –&gt; Because of high bias)\nWe don’t want to overfit (will have high MSE –&gt; Because of high variance) More complex models tend to have higher variance and lower bias\n\nWe need to choose the correct complexity estimating E(MSE) in the validation dataset\n\nCompare between models, tune the hyperparameters of the model or select features\nOr even better, cross-validation\n\nWe can estimate E(MSE) using the test dataset\n\nAllows us to understand how the results of our model generalize to unseen data\nDone only for the best model (sometimes best k models)\nGetting good test data is difficult, unsolved problem Keep in mind that complex models are often less interpretable"
  },
  {
    "objectID": "text-mining.html#readings",
    "href": "text-mining.html#readings",
    "title": "8  Text Mining",
    "section": "8.1 Readings",
    "text": "8.1 Readings\nWelcome to Text Mining with R\n\nChapter 1: The tidy text format – Chapter 2: Sentiment analysis with tidy data\nChapter 3: Analyzing word and document frequency: tf-idf"
  },
  {
    "objectID": "text-mining.html#introduction",
    "href": "text-mining.html#introduction",
    "title": "8  Text Mining",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nLecturer: Ayoub Bagheri\nWhat is text mining?\n\nmost popular: “The discovery by computer of }new, previously unknown information}, by automatically extracting information from different written resources” (Hearst 1999)\nText mining is about looking for patterns in text, in a similar way that data mining can be loosely described as looking for patterns in data.\nText mining describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources (Wikipedia)\n\nWhy text mining?\nText data is everywhere, websites (e.g., news), social media (e.g., twitter), databases (e.g., doctors’ notes), digital scans of printed materials, … - A lot of world’s data is in unstructured text format - Applications in industry: search, machine translation, sentiment analysis, question answering, … - Applications in science: cognitive modeling , understanding bias in language, automated systematic literature reviews, …\nExamples for text mining:\n\nWho was the best friend in Friends?\nusing text mining instead of hand assignment in the automatic detection of ICD10 codes in cardiology discharge letters\n\nunderstanding language is difficult!\n(at least half of them are an open problem)\n\nDifferent things can mean more or less the same ( data science ” vs.statistics\nContext dependency (You have very nice shoes)\nSame words with different meanings (to sanction)\nLexical ambiguity (we saw her duck)\nIrony, sarcasm (You should swallow disinfectant)\nFigurative language (He has a heart of stone)\nNegation (not good ” vs. good ””), spelling variations, jargon, abbreviations\nAll the above is different over languages,99 % of work is on English!\n\nKey problem\n\nText, images, videos is unstructured data, not tidy\nunstructured text: information that either does not have a pre defined data model or is not organized in a pre defined manner.\nfor our analysis we need tidy data\neach variable is a column, each observation a row, each type of observational unit is a table → table with one-token-per-row\ntoken is a meaningful unit of text, such as a word, that we are interested in using for analysis\nhow is data stored in text mining approaches?\nstring, character vectors\ncorpus: contain raw string strings annotated with additional meta data and details\ndocument-term matrix: sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf. In other words: is a mathematical matrix that describes the frequency of terms that occur in a collection of documents\n→ How is it possible to convert this data to structured one?\nanswer: tokenization, the process of splitting text into tokens"
  },
  {
    "objectID": "text-mining.html#preprocessing-data",
    "href": "text-mining.html#preprocessing-data",
    "title": "8  Text Mining",
    "section": "8.3 Preprocessing data",
    "text": "8.3 Preprocessing data\n\napproach for cleaning and noise removal of text data\nbrings text in analyzable form for statistical learning\nis useful, because:\nefficient\nremove stop words - def: words which are filtered out before or after processing of natural language data (text)\nreduce noise\nis tidy and structured\ndimensionality: words as features in columns\nmemory allocation\nincrease performance"
  },
  {
    "objectID": "text-mining.html#typical-steps",
    "href": "text-mining.html#typical-steps",
    "title": "8  Text Mining",
    "section": "8.4 typical steps",
    "text": "8.4 typical steps\nnot all of these are appropriate at all times!\n\ntokenization Tokenization (“text”, “ming”, “is”, “the”, “best”, “!”)\nstemming (“lungs” to “lung”) or Lemmatization (“were”to “is”)\n\ndef: the process for reducing inflected (or sometimes derived) words to their word stem, base or root form generally a written word form\n\nlowercasing (“Disease” to “disease”)\nstopword removal (“text mining is best!”)\npunctual removal (“text mining is the best”)\nnumber removal (“I42” to “I”)\nspell correction (“hart” to “heart”)\n\nExample for steps, one step behind another with a Vector Space Model as a result:"
  },
  {
    "objectID": "text-mining.html#vector-space-model",
    "href": "text-mining.html#vector-space-model",
    "title": "8  Text Mining",
    "section": "8.5 Vector Space Model",
    "text": "8.5 Vector Space Model\nBasic idea: Represent the text as something that makes sense to a computer, makes it readable for the computer\n\nis a collection of vectors\nrepresents documents by concept vectors\neach concept defines one dimension (one dimension can be one word)\nk concepts define a high-dimensional space\nelement of vector corresponds to concept weight\nterms/ words are genereic features that can be extracted from text\ntypically, terms are single words, keywords, n/grams, or phrases\ndocuments are represented as vectors of terms\neach dimension (concept) corresponds to a separate term\n\n\\[\nd= (w_1, \\dots,  w_n)\n\\]\n\n\nThe process of converting text into numbers is called Vectorization\nDistance between the vectors in this concept space illustrate the relationsship between the documents"
  },
  {
    "objectID": "text-mining.html#bag-of-words",
    "href": "text-mining.html#bag-of-words",
    "title": "8  Text Mining",
    "section": "8.6 Bag-of-Words",
    "text": "8.6 Bag-of-Words\nHow can we convert words in numerical values? You need a vocabulary that works for all articles, so a binary approach!\n\nTerms are words (more guenereally we can use n-grams)\nweights are numbers of occurrences of the terms int eh document\nbinary\nterm frequency (TF)\nterm Frequency inverse Document Frequency (TFiDF)\n\nLooking at all words in all articles and give zeros and ones, if the term occur or not:\n\n\n8.6.1 TFiDF\nA term is more discriminative if it occurs a lot but only in fewer documents → shows how often the words occur and how important it is!\n\nHow often? Let \\(n_{d,t}\\) denote the number of times the t-th term appear in the d-th document\n\n\\[\nTF_{d,t} = \\frac{n_{d,t}}{\\sum_{i}n_{d,i}}\n\\]\n\nhow important? Let N denote the number of documents and \\(N_t\\) denote the number of documents containing the t-th term.\n\n\\[\nIDF_t = log (\\frac{N}{N_t})\n\\]\nWhen a word appears in all of the documents, it cancelled out by how important that is.\nAnd then weight with TFiDF:\n\\[\nw_{d,t} = TF_{d,t} * IDF_t\n\\]"
  },
  {
    "objectID": "text-mining.html#overview-of-vsm-models",
    "href": "text-mining.html#overview-of-vsm-models",
    "title": "8  Text Mining",
    "section": "8.7 Overview of VSM models",
    "text": "8.7 Overview of VSM models\n\noverview about the method:\nthree categories of vector space model\n\nbag-of-words\nWe do not care about the order of words, so not about the meaning good to convert to a table high dimensional, high number of zeros (sparse)\ntopics we expect 10 topics or clusters use these topics as vectors\nword embeddings we care about the order and the meaning"
  },
  {
    "objectID": "text-mining.html#text-classification",
    "href": "text-mining.html#text-classification",
    "title": "8  Text Mining",
    "section": "8.8 Text classification",
    "text": "8.8 Text classification\n\nsupervised learning: learning a function that maps an input to an output based on example input-output pairs\n\ninfer function from labeled training data\nuse inferred function to label new instances\n\ncommon: human experts annotate a set of text data as a training set\n\nhand-coded rules\n\nrules based on combinations of words or other features\naccuracy can be high if rules are carefully defined by experts\ndata/domain specific"
  },
  {
    "objectID": "text-mining.html#algorithms",
    "href": "text-mining.html#algorithms",
    "title": "8  Text Mining",
    "section": "8.9 Algorithms",
    "text": "8.9 Algorithms\n\nNaïve Bayes\nLogistic regression\nSupport vector machines\nK nearest neighbors\nNeural networks\nDeep learning"
  },
  {
    "objectID": "text-mining.html#word-represenation",
    "href": "text-mining.html#word-represenation",
    "title": "8  Text Mining",
    "section": "8.10 Word represenation",
    "text": "8.10 Word represenation\n\nhow can we represent the meaning of words?\n\nWords as vectors:\n\ncapture semantics:\nsimilar words should be close to each other in the vector space\nrelation between two vectors should reflect the relationship between the two words\nbe efficient, because smaller number of vectors and dimensions\nbe interpretable"
  },
  {
    "objectID": "text-mining.html#in-r",
    "href": "text-mining.html#in-r",
    "title": "8  Text Mining",
    "section": "8.11 in R",
    "text": "8.11 in R\n## libraries\nlibrary(tidytext)\nlibrary(RColorBrewer)\nlibrary(gutenbergr)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(textdata)\nlibrary(tm)\nlibrary(NLP)\nlibrary(stringi)\nlibrary(e1071)\nlibrary(rpart)\nlibrary(caret)\n\n## Preprocessing tidy data apporoach\n  #each line get a number and each chapter reference a number, too in the book Alice in Wonderland\ntidy_AAIWL &lt;- AAIWL %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n## Tokenization, un-nesting Text\n  #each word in the column text gets is own row\ntidy_AAIWL &lt;- tidy_AAIWL %&gt;% unnest_tokens(word, text)\n\n#count the words\n  #from most used descending \ntidy_AAIWL.count &lt;- tidy_AAIWL %&gt;% count(word, sort=TRUE)\n\n#remove stopwords\ndata(\"stop_words\")\ntidy_AAIWL.stop &lt;- tidy_AAIWL %&gt;% anti_join(stop_words)\n\n#create a word cloud out of it\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))                                        \n\n#Vector Space Model\n  #set the seed to make your partition reproducible\nset.seed(123)\n\ndf_final$Content &lt;- iconv(df_final$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\n  #for documenttermmatrix we need a corpus format, a list ot content and the meta data \ndocs &lt;- Corpus(VectorSource(df_final$Content)) \n\n  ## alter the code from here onwards\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE))  #lower case\n\nview(dtm)\n  #in the rows are the entries, in the columns the frequency of a word\n\n  ## we are not interested in all the words, we are only interested in words that are more ofte used than 10 times\nnot.freq &lt;- findFreqTerms(dtm, lowfreq=11) \n  #run again \ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary = not.freq))  #lower case\n   ## define the training partition \ntrain_index &lt;- createDataPartition(df_final$Category, p = .8,  #category because outcome variable\n                                  list = FALSE, \n                                  times = 1)\n\n  ## split the data using the training partition to obtain training data\ndf_train &lt;- df_final[train_index,]\n\n  ## remainder of the split is the validation and test data (still) combined \ndf_test &lt;- df_final[-train_index,] \n\ndf_train$Content &lt;- iconv(df_train$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\ndf_test$Content &lt;- iconv(df_test$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs_train &lt;- Corpus(VectorSource(df_train$Category)) \ndocs_test &lt;- Corpus(VectorSource(df_test$Category)) \n\n## alter the code from here onwards\ndtm_train &lt;- DocumentTermMatrix(docs_train,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n## alter the code from here onwards\ndtm_test &lt;- DocumentTermMatrix(docs_test,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n#make a data frame \ndtm_train  &lt;- as.data.frame(as.matrix(dtm_train))\ndtm_test  &lt;- as.data.frame(as.matrix(dtm_test))\n\n#now you can run a decision tree on it with\nlibrary(rpart)\nfit_dt &lt;- rpart(cat~., data = dtm_train, method = 'class')\n \n## prediction on training data\npred_train &lt;- predict(fit_dt, dtm_train, type = 'class')\nfit_table  &lt;- table(dtm_train$cat, pred_train, dnn = c(\"Actual\", \"Predicted\"))\nfit_table\n\npred_test       &lt;- predict(fit_dt, dtm_test, type = 'class')\nfit_table_test  &lt;- table(dtm_test$cat, pred_test, dnn = c(\"Actual\", \"Predicted\"))\nfit_table_test\n\n## You can use this table to calculate Accuracy, Sensitivity, Specificity, Pos Pred Value, and Neg Pred Value. There are also many functions available for this purpose, for example the `confusionMatrix` function from the `caret` package.\nDuring this practical, we will cover an introduction to text mining. Topics covered are how to pre-process mined text (in both the tidy approach and using the tm package), different ways to visualize this the mined text, creating a document-term matrix and an introduction to one type of analysis you can conduct during text mining: text classification. As a whole, there are multiple ways to analysis mine & analyze text within R. However, for this practical we will discuss some of the techniques covered in the tm package and in the tidytext package, based upon the tidyverse.\nYou can download the student zip including all needed files for practical 9 here. In addition, for this practical, you will need the following packages:\n\n## General Packages\nlibrary(tidyverse)\n\n## Text Mining\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(RColorBrewer)\nlibrary(gutenbergr)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(textdata)\nlibrary(tm)\nlibrary(NLP)\nlibrary(stringi)\nlibrary(e1071)\nlibrary(rpart)\nlibrary(caret)\n\nFor the first part of the practical, we will be using text mined through the Gutenberg Project; briefly this project contains over 60,000 freely accessible eBooks, which through the package gutenberger, can be easily accessed and perfect for text mining and analysis.\nWe will be looking at several books from the late 1800s, in the mindset to compare and contrast the use of language within them. These books include:\n\nAlice’s Adventures in Wonderland by Lewis Carroll\nThe Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson\n\nDespite being from the late 1800s, these books still are popular and hold cultural significance in TV, Movies and the English Language. To access this novel suitable for this practical the following function should be used:\n\nAAIWL &lt;- gutenberg_download(28885) ## 28885 is the eBook number of Alice in Wonderland\nSCJH  &lt;- gutenberg_download(43)    ## 43 is the eBook number of Dr. Jekyll and Mr. Hyde\n\nAfter having loaded all of these books into your working directory (using the code above), examine one of these books using the View() function. When you view any of these data frames, you will see that these have an extremely messy layout and structure. As a result of this complex structure means that conducting any analysis would be extremely challenging, so pre-processing must be undertaken to get this into a format which is usable.\n\n8.11.1 Pre-Processing Text: Tidy approach\nIn order for text to be used effectively within statistical processing and analysis; it must be pre-processed so that it can be uniformly examined. Typical steps of pre-processing include:\n\nRemoving numbers\nConverting to lowercase\nRemoving stop words\nRemoving punctuation\nStemming\n\nThese steps are important as they allow the text to be presented uniformly for analysis (but remember we do not always need all of them); within this practical we will discuss how to undergo some of these steps.\n\n\n8.11.2 Step 1: Tokenization, un-nesting Text\nWhen we previously looked at this text, as we discovered it was extremely messy with it being attached one line per row in the data frame. As such, it is important to un-nest this text so that it attaches one word per row.\nBefore un-nesting text, it is useful to make a note of aspects such as the line which text is on, and the chapter each line falls within. This can be important when examining anthologies or making chapter comparisons as this can be specified within the analysis.\nIn order to specify the line number and chapter of the text, it is possible to use the mutuate function from the dplyr package.\n\nApply the code below, which uses the mutate function, to add line numbers and chapter references one of the books. Next, use the View() function to examine how this has changed the structure.\n\n\n## Template:\ntidy_AAIWL &lt;- AAIWL %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n\ntidy_SCJH &lt;- SCJH  %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n\n`\nFrom this, it is now possible to pass the function unnest_tokens() in order to split apart the sentence string, and apply each word to a new line. When using this function, you simply need to pass the arguments, word (as this is what you want selecting) and text (the name of the column you want to unnest).\nThe two basic arguments to unnest_tokens used here are column names. First we have the output column name that will be created as the text is unnested into it (word, in this case), and then the input column that the text comes from (text, in this case). Remember that text_df above has a column called text that contains the data of interest.\nAfter using unnest_tokens, we’ve split each row so that there is one token (word) in each row of the new data frame; the default tokenization in unnest_tokens() is for single words, as shown here. Also notice:\nOther columns, such as the line number each word came from, are retained. Punctuation has been stripped. By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).\n\nApply unnest_tokens to your tidied book to unnest this text. Next, once again use the View() function to examine the output.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL &lt;- tidy_AAIWL %&gt;% unnest_tokens(word, text)\n\ntidy_SCJH &lt;- tidy_SCJH %&gt;% unnest_tokens(word, text)\n\nThis results in one word being linked per row of the data frame. The benefit of using the tidytext package in comparison to other text mining packages, is that this automatically applies some of the basic steps to pre-process your text, including removing of capital letters, inter-word punctuation and numbers. However additional pre-processing is required.\n\n\n8.11.3 Intermezzo: Word clouds\nBefore continuing the pre-processing process, let’s have a first look at our text by making a simple visualization using word clouds. Typically these word clouds visualize the frequency of words in a text through relating the size of the displayed words to frequency, with the largest words indicating the most common words.\nTo plot word clouds, we first have to create a data frame containing the word frequencies.\n\nCreate a new data frame, which contains the frequencies of words from the unnested text. To do this, you can make use of the function count().\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL.count &lt;- tidy_AAIWL %&gt;% count(word, sort=TRUE)\n\ntidy_SCJH.count &lt;- tidy_SCJH %&gt;% count(word, sort=TRUE)\n\n\nUsing the wordcloud() function, create a word cloud for your book text. Use the argument max.words within the function to set the maximum number of words to be displayed in the word cloud.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function. Note: Ensure to use the function with(), is used after the piping operator.\n\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))\n\n\n\ntidy_SCJH.count %&gt;% with(wordcloud(word, n, max.words = 100))\n\n\n\n\n\nDiscuss with another individual or group, whether you can tell what text each word clouds come from, based on the popular words which occur.\n\n\n\n8.11.4 Step 2: Removing stop words\nAs discussed within the lecture, stop words are words in any language which have little or no meaning, and simply connect the words of importance. Such as the, a, also, as, were… etc. To understand the importance of removing these stop words, we can simply do a comparison between the text which has had them removed and those which have not been.\nTo remove the stop words, we use the function anti_join(). This function works through un-joining this table based upon the components, which when passed with the argument stop_words, which is a table containing these words across three lexicons. This removes all the stop words from the presented data frame.\n\nUse the function anti_join() to remove stop words from your tidied text attaching it to a new data frame.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ndata(\"stop_words\")\ntidy_AAIWL.stop &lt;- tidy_AAIWL %&gt;% anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_SCJH.stop &lt;- tidy_SCJH %&gt;% anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\nIn order to examine the impact of removing these filler words, we can use the count() function to examine the frequencies of different words. This when sorted, will produce a table of frequencies in descending order. An other option is to redo the wordclouds on the updated data frame containing the word counts of the tidied book text without stop words.\n\nUse the function count() to compare the frequencies of words in the dataframes containing the tidied book text with and without stop words (use sort = TRUE within the count() function), or redo the wordclouds. Do you notice a difference in the (top 10) words which most commonly occur in the text?\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL.count &lt;- tidy_AAIWL.stop %&gt;% count(word, sort=TRUE)\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))"
  },
  {
    "objectID": "text-mining.html#vector-space-model-document-term-matrix",
    "href": "text-mining.html#vector-space-model-document-term-matrix",
    "title": "8  Text Mining",
    "section": "8.12 Vector space model: document-term matrix",
    "text": "8.12 Vector space model: document-term matrix\nIn this part of the practical we will build a text classification model for a multiclass classification task. To this end, we first need to perform text preprocessing, then using the idea of vector space model, convert the text data into a document-term (dtm) matrix, and finally train a classifier on the dtm matrix.\nThe data set used in this part of the practical is the BBC News data set. You can use the provided “news_dataset.rda” for this purpose.  This data set consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:\n\nBusiness\nEntertainment\nPolitics\nSport\nTech\n\n\nUse the code below to load the data set and inspect its first rows.\n\n\nload(\"data/news_dataset.rda\")\nhead(df_final)\n\n  File_Name\n1   001.txt\n2   002.txt\n3   003.txt\n4   004.txt\n5   005.txt\n6   006.txt\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Content\n1 Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\\n\\nTime Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\\n\\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n2                                                                                                                                                                                                                                                                                                                        Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\n\\nWorries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are with Rosneft, so it will have to pay real money to the creditors to avoid seizure of Yugansk assets,\" said Moscow-based US lawyer Jamie Firestone, who is not connected to the case. Menatep Group's managing director Tim Osborne told the Reuters news agency: \"If they default, we will fight them where the rule of law exists under the international arbitration clauses of the credit.\"\\n\\nRosneft officials were unavailable for comment. But the company has said it intends to take action against Menatep to recover some of the tax claims and debts owed by Yugansk. Yukos had filed for bankruptcy protection in a US court in an attempt to prevent the forced sale of its main production arm. The sale went ahead in December and Yugansk was sold to a little-known shell company which in turn was bought by Rosneft. Yukos claims its downfall was punishment for the political ambitions of its founder Mikhail Khodorkovsky and has vowed to sue any participant in the sale.\n4                                                                                                                                                               High fuel prices hit BA's profits\\n\\nBritish Airways has blamed high fuel prices for a 40% drop in profits.\\n\\nReporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of £75m ($141m) compared with £125m a year earlier. Rod Eddington, BA's chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by £106m or 47.3%. BA's profits were still better than market expectation of £59m, and it expects a rise in full-year revenues.\\n\\nTo help offset the increased price of aviation fuel, BA last year introduced a fuel surcharge for passengers.\\n\\nIn October, it increased this from £6 to £10 one-way for all long-haul flights, while the short-haul surcharge was raised from £2.50 to £4 a leg. Yet aviation analyst Mike Powell of Dresdner Kleinwort Wasserstein says BA's estimated annual surcharge revenues - £160m - will still be way short of its additional fuel costs - a predicted extra £250m. Turnover for the quarter was up 4.3% to £1.97bn, further benefiting from a rise in cargo revenue. Looking ahead to its full year results to March 2005, BA warned that yields - average revenues per passenger - were expected to decline as it continues to lower prices in the face of competition from low-cost carriers. However, it said sales would be better than previously forecast. \"For the year to March 2005, the total revenue outlook is slightly better than previous guidance with a 3% to 3.5% improvement anticipated,\" BA chairman Martin Broughton said. BA had previously forecast a 2% to 3% rise in full-year revenue.\\n\\nIt also reported on Friday that passenger numbers rose 8.1% in January. Aviation analyst Nick Van den Brul of BNP Paribas described BA's latest quarterly results as \"pretty modest\". \"It is quite good on the revenue side and it shows the impact of fuel surcharges and a positive cargo development, however, operating margins down and cost impact of fuel are very strong,\" he said. Since the 11 September 2001 attacks in the United States, BA has cut 13,000 jobs as part of a major cost-cutting drive. \"Our focus remains on reducing controllable costs and debt whilst continuing to invest in our products,\" Mr Eddington said. \"For example, we have taken delivery of six Airbus A321 aircraft and next month we will start further improvements to our Club World flat beds.\" BA's shares closed up four pence at 274.5 pence.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Pernod takeover talk lifts Domecq\\n\\nShares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.\\n\\nReports in the Wall Street Journal and the Financial Times suggested that the French spirits firm is considering a bid, but has yet to contact its target. Allied Domecq shares in London rose 4% by 1200 GMT, while Pernod shares in Paris slipped 1.2%. Pernod said it was seeking acquisitions but refused to comment on specifics.\\n\\nPernod's last major purchase was a third of US giant Seagram in 2000, the move which propelled it into the global top three of drinks firms. The other two-thirds of Seagram was bought by market leader Diageo. In terms of market value, Pernod - at 7.5bn euros ($9.7bn) - is about 9% smaller than Allied Domecq, which has a capitalisation of £5.7bn ($10.7bn; 8.2bn euros). Last year Pernod tried to buy Glenmorangie, one of Scotland's premier whisky firms, but lost out to luxury goods firm LVMH. Pernod is home to brands including Chivas Regal Scotch whisky, Havana Club rum and Jacob's Creek wine. Allied Domecq's big names include Malibu rum, Courvoisier brandy, Stolichnaya vodka and Ballantine's whisky - as well as snack food chains such as Dunkin' Donuts and Baskin-Robbins ice cream. The WSJ said that the two were ripe for consolidation, having each dealt with problematic parts of their portfolio. Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Japan narrowly escapes recession\\n\\nJapan's economy teetered on the brink of a technical recession in the three months to September, figures show.\\n\\nRevised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\\n\\nThe government was keen to play down the worrying implications of the data. \"I maintain the view that Japan's economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It's painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.\n  Category Complete_Filename\n1 business  001.txt-business\n2 business  002.txt-business\n3 business  003.txt-business\n4 business  004.txt-business\n5 business  005.txt-business\n6 business  006.txt-business\n\n\n\nFind out about the name of the categories and the number of observations in each of them.\n\n\nt_category &lt;- table(df_final$Category)\n\n\nt_name &lt;- table(df_final$File_Name)\n\n\nConvert the data set into a document-term matrix using the function DocumentTermMatrix() and subsequently use the findFreqTerms() function to keep the terms which their frequency is larger than 10. A start of the code is given below. It is also a good idea to apply some text preprocessing, for this inspect the control argument of the function DocumentTermMatrix() (e.g., convert the words into lowercase, remove punctuations, numbers, stopwords, and whitespaces).\n\n\n### set the seed to make your partition reproducible\nset.seed(123)\n\ndf_final$Content &lt;- iconv(df_final$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs &lt;- Corpus(VectorSource(df_final$Content)) #for documenttermmatrix we need a corpus format, a list ot content and the meta data \n\n## alter the code from here onwards\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE))  #lower case\n\n  #in the rows are the entries, in the columns the frequency of a word\n\n\nnot.freq &lt;- findFreqTerms(dtm, lowfreq=11) ## we are not interested in all the words, we are only interested in words that are used more than 10 times\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary = not.freq))  #lower case\n\n\nPartition the original data into training and test sets with 80% for training and 20% for test.\n\n\n## define the training partition \ntrain_index &lt;- createDataPartition(df_final$Category, p = .8,  #category because outcome variable\n                                  list = FALSE, \n                                  times = 1)\n\n## split the data using the training partition to obtain training data\ndf_train &lt;- df_final[train_index,]\n\n## remainder of the split is the validation and test data (still) combined \ndf_test &lt;- df_final[-train_index,]\n\n\nCreate separate document-term matrices for the training and the test sets using the previous frequent terms as the input dictionary and convert them into data frames.\n\n\ndf_train$Content &lt;- iconv(df_train$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\ndf_test$Content &lt;- iconv(df_test$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs_train &lt;- Corpus(VectorSource(df_train$Category)) \ndocs_test &lt;- Corpus(VectorSource(df_test$Category)) \n\n\n\n\n## alter the code from here onwards\ndtm_train &lt;- DocumentTermMatrix(docs_train,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n## alter the code from here onwards\ndtm_test &lt;- DocumentTermMatrix(docs_test,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n#make a data frame \ndtm_train  &lt;- as.data.frame(as.matrix(dtm_train))\ndtm_test  &lt;- as.data.frame(as.matrix(dtm_test))\n\n\nUse the cbind function to add the categories to the train_dtm data and name the column y.\n\n\ny &lt;- df_train$Category\ndtm_train &lt;- cbind(dtm_train,y)\ntable(dtm_train[5,6216])\n\n\nbusiness \n       1"
  }
]