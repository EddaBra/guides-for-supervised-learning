<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Tree-based Methods – Guides for Supervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./text-mining.html" rel="next">
<link href="./beyond-linearity.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-cd7de1037569933fbb609f06423bd096.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tree-based.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-based Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Guides for Supervised Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-accuracy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model accuracy and fit</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./beyond-linearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Beyond linearity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tree-based.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-based Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text-mining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Text Mining</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#readings" id="toc-readings" class="nav-link active" data-scroll-target="#readings"><span class="header-section-number">7.1</span> Readings</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">7.2</span> Introduction</a></li>
  <li><a href="#growing-decision-trees-from-data" id="toc-growing-decision-trees-from-data" class="nav-link" data-scroll-target="#growing-decision-trees-from-data"><span class="header-section-number">7.3</span> Growing decision trees from data</a>
  <ul class="collapse">
  <li><a href="#tree-building-top-down-and-greedy" id="toc-tree-building-top-down-and-greedy" class="nav-link" data-scroll-target="#tree-building-top-down-and-greedy"><span class="header-section-number">7.3.1</span> Tree-building: top-down and greedy</a></li>
  </ul></li>
  <li><a href="#regression-trees" id="toc-regression-trees" class="nav-link" data-scroll-target="#regression-trees"><span class="header-section-number">7.4</span> Regression trees</a></li>
  <li><a href="#cost-complexity-pruning" id="toc-cost-complexity-pruning" class="nav-link" data-scroll-target="#cost-complexity-pruning"><span class="header-section-number">7.5</span> Cost complexity Pruning</a></li>
  <li><a href="#building-a-tree-summed-up" id="toc-building-a-tree-summed-up" class="nav-link" data-scroll-target="#building-a-tree-summed-up"><span class="header-section-number">7.6</span> Building a tree summed up</a></li>
  <li><a href="#classification-trees" id="toc-classification-trees" class="nav-link" data-scroll-target="#classification-trees"><span class="header-section-number">7.7</span> Classification trees</a></li>
  <li><a href="#compare-trees-vs.-linear-classifiers" id="toc-compare-trees-vs.-linear-classifiers" class="nav-link" data-scroll-target="#compare-trees-vs.-linear-classifiers"><span class="header-section-number">7.8</span> Compare trees vs.&nbsp;linear classifiers</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="header-section-number">7.9</span> Bagging</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="header-section-number">7.10</span> Random forest</a></li>
  <li><a href="#other-methods" id="toc-other-methods" class="nav-link" data-scroll-target="#other-methods"><span class="header-section-number">7.11</span> Other methods</a></li>
  <li><a href="#out-of-bag-error-estimation" id="toc-out-of-bag-error-estimation" class="nav-link" data-scroll-target="#out-of-bag-error-estimation"><span class="header-section-number">7.12</span> “Out-of-bag” error estimation</a></li>
  <li><a href="#variance-importance-measure-mdi" id="toc-variance-importance-measure-mdi" class="nav-link" data-scroll-target="#variance-importance-measure-mdi"><span class="header-section-number">7.13</span> Variance importance measure (MDI)</a>
  <ul class="collapse">
  <li><a href="#permutation-based-feature-importance" id="toc-permutation-based-feature-importance" class="nav-link" data-scroll-target="#permutation-based-feature-importance"><span class="header-section-number">7.13.1</span> Permutation-based feature importance</a></li>
  </ul></li>
  <li><a href="#in-r" id="toc-in-r" class="nav-link" data-scroll-target="#in-r"><span class="header-section-number">7.14</span> in R</a>
  <ul class="collapse">
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="header-section-number">7.14.1</span> Decision Trees</a></li>
  <li><a href="#classification-trees-1" id="toc-classification-trees-1" class="nav-link" data-scroll-target="#classification-trees-1"><span class="header-section-number">7.14.2</span> Classification trees</a></li>
  <li><a href="#building-classification-trees" id="toc-building-classification-trees" class="nav-link" data-scroll-target="#building-classification-trees"><span class="header-section-number">7.14.3</span> Building Classification Trees</a></li>
  <li><a href="#plotting-classification-trees" id="toc-plotting-classification-trees" class="nav-link" data-scroll-target="#plotting-classification-trees"><span class="header-section-number">7.14.4</span> Plotting Classification Trees</a></li>
  <li><a href="#assessing-accuracy-and-pruning-of-classification-trees" id="toc-assessing-accuracy-and-pruning-of-classification-trees" class="nav-link" data-scroll-target="#assessing-accuracy-and-pruning-of-classification-trees"><span class="header-section-number">7.14.5</span> Assessing accuracy and pruning of classification trees</a></li>
  <li><a href="#bagging-and-random-forests" id="toc-bagging-and-random-forests" class="nav-link" data-scroll-target="#bagging-and-random-forests"><span class="header-section-number">7.14.6</span> Bagging and Random Forests</a></li>
  <li><a href="#bagging-1" id="toc-bagging-1" class="nav-link" data-scroll-target="#bagging-1"><span class="header-section-number">7.14.7</span> Bagging</a></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">7.14.8</span> Random Forests</a></li>
  <li><a href="#variable-importance" id="toc-variable-importance" class="nav-link" data-scroll-target="#variable-importance"><span class="header-section-number">7.14.9</span> Variable importance</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">7.15</span> Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-based Methods</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="readings" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="readings"><span class="header-section-number">7.1</span> Readings</h2>
<p><a href="https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf">ISLR</a>:</p>
<ul>
<li>Chapter 8: Tree-based models</li>
</ul>
</section>
<section id="introduction" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="introduction"><span class="header-section-number">7.2</span> Introduction</h2>
<p>Lecturer: Ayoub Bagheri</p>
<p><strong>Basic vocabulary</strong></p>
<ul>
<li><p>can applied to both, regression and classification</p></li>
<li><p><em>predictor space is stratified or segmented into a number of simple regions</em> → in order to make a prediction for a given observation, we typically use the <em>mean of the mode response value</em> for the training observations in the region to which it belongs</p></li>
<li><p>root of the tree / <em>root node</em> is the starting point, it is the first decision, which is defined in a tree → the decisions or better predictors with a certain threshold are the referred as <em>nodes</em> in a tree (also known as leaves)</p></li>
<li><p>The points along the tree where the predictor space is split are referred to as internal nodes.</p></li>
<li><p>has a certain threshold, if the mean response value is higher than that threshold, the observations are assigned to one <em>branch</em>, if its lower, they are assigned to the other branch. Always only two branches (yes, no), binary decisions!</p></li>
<li><p>then, the subgroup, referred to as <em>region</em></p></li>
<li><p>two kinds of trees</p></li>
<li><p>decision trees: a complex decision is broken down in different criteria, to advise a decision. For example buying a car is a complex decision, can be parsed into: Have you tested the car on road? How many miles has been already driven with that car? How old is the car? Is the car environment friendly? And so on.</p></li>
<li><p>prediction trees: Would you survive the titanic? Parse a complex decision into certain criteria?</p></li>
</ul>
</section>
<section id="growing-decision-trees-from-data" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="growing-decision-trees-from-data"><span class="header-section-number">7.3</span> Growing decision trees from data</h2>
<p>Goal is to find regions, that minize the RSS.</p>
<p><em>Key question: how can we find the best split?</em></p>
<p>Recursive (binary) partitioning:</p>
<ol type="1">
<li>Find the split that makes observations as similar as possible on the outcome within that split</li>
</ol>
<ul>
<li>Which feature should I use for splitting? Is this split the best? We do that for each feature. If we found our best split, we apply it and then we asked what is now the best split for the subgroup → stratify the observations in smaller and smaller regions, so that they become as homogeneous as possible</li>
<li>we divide the predictor space, the set of possible values for <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> into <span class="math inline">\(J\)</span>j distinct and non-overlapping regions for <span class="math inline">\(R_1, R_2, \dots, R_J\)</span>. Then, for every observation that falls into the Region <span class="math inline">\(R_j\)</span> we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>. The goal is to find boxes <span class="math inline">\(R_1, R_2, \dots, R_J\)</span> that minimize the RSS.</li>
<li>Unfortunately, it is computationally unfeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting</li>
<li>In order to perform recursive binary splitting, we first select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into regions <span class="math inline">\({X|X_j &lt; s}\)</span> and <span class="math inline">\({X|X_j &gt;= s}\)</span> leads to the greatest possible reduction in RSS.</li>
</ul>
<ol start="2" type="1">
<li>do the first step in each resulting group over and over again.</li>
</ol>
<p>Early stopping: Stratifying in too small groups is an overfitting:</p>
<ul>
<li>we can implement stop points for splitting, so if a certain threshold is reached, we stop splitting.</li>
<li>so stop, when fewer than <span class="math inline">\(n_{min}\)</span> observations in the group (typically 10)</li>
</ul>
<section id="tree-building-top-down-and-greedy" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="tree-building-top-down-and-greedy"><span class="header-section-number">7.3.1</span> Tree-building: top-down and greedy</h3>
<p>Recursive (binary) partitioning is a top-down and greedy algorithm:</p>
<ul>
<li><em>top-down</em>: algorithm begins at the top of the tree with all observations and then successively splits the predictor space. Each split is indicated via two new branches further down on the tree.</li>
<li><em>greedy</em> (gefräßig, gierig): at each step, the best split for that step is made, instead of looking ahead and picking a split that will result in the best tree in a future step. At each split we take the best split possible, we do not look, which splits overall is the best! → so greedy method</li>
</ul>
</section>
</section>
<section id="regression-trees" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="regression-trees"><span class="header-section-number">7.4</span> Regression trees</h2>
<p>The <strong>outcome is continuous</strong>, is a numerical. Instead of predicting class label in each <code>box</code>(partition), we predict the outcome in each partition:</p>
<ul>
<li>mean of the training observations in the partition, to which the test observations belong</li>
<li>cutploints are selected such that splitting the predictor space into the reions leads to the greatest possible reduction in residual sums of squares (RSS)</li>
</ul>
<p><strong>Example:</strong></p>
<p>Wine quality, quality measured on a scale</p>
<p><img src="figures/8.reg.png" class="img-fluid" width="490"></p>
<p>Outcome is a numerical. We start with alcohol as a root node and 100 % of the observations. At a certain threshold we divide the observations into two regions, the yes branch, if alcohol is &lt; 11 and the no one, if alcohol is &gt;=11. Then, the mean value for the divided regions is computed. Is For the one &lt;11, the wine quality is 5.4, where the other group has a quite higher quality with 6.1 and contains 39% of the observations. So instead of yes / no we have numbers as an outcome.</p>
</section>
<section id="cost-complexity-pruning" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="cost-complexity-pruning"><span class="header-section-number">7.5</span> Cost complexity Pruning</h2>
<p>Process described most likely overfits the data → poor test performance</p>
<p><em>Solution 1:</em> build the tree until the decrease in classification error / RSS exceeds some threshold</p>
<p>However, a seemingly worthless split early on in the tree might be followed by a very good split</p>
<p><em>Solution 2:</em> build a very long tree, then prune it back in order to obtain a subtree</p>
<ul>
<li>not efficient to consider every possible subtree</li>
<li>so cost complexity pruning (simply use the idea of the lasso regression in adding a penalty): A penalty <span class="math inline">\(|T| \alpha\)</span> where <span class="math inline">\(|T|\)</span> is the number of final nodes and <span class="math inline">\(\alpha\)</span> is a tuning parameter, is placed on the total number of final nodes:</li>
<li><span class="math inline">\(\alpha = 0\)</span> → subtree equals the original (long) tree as <span class="math inline">\(\alpha\)</span> increases, becomes more expensive to have a tree with many terminal nodes, resulting in a smaller subtree.</li>
<li>use K-fold cross-validation to choose <span class="math inline">\(\alpha\)</span></li>
</ul>
<p>Examples for cost complexity:</p>
<p><img src="figures/8.prun.png" class="img-fluid" width="350"> <img src="figures/8.prun2.png" class="img-fluid" width="350"></p>
</section>
<section id="building-a-tree-summed-up" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="building-a-tree-summed-up"><span class="header-section-number">7.6</span> Building a tree summed up</h2>
<p><img src="figures/8.build.png" class="img-fluid" width="490"></p>
</section>
<section id="classification-trees" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="classification-trees"><span class="header-section-number">7.7</span> Classification trees</h2>
<p>The <strong>outcome is a categorical</strong>: Would you survive the titanic? Would you pass the exam? Is it a cat or a dog on that picture? Quality of a product (when in categories like good, bad and so on)</p>
<p>Example:</p>
<p><img src="figures/8.class.png" class="img-fluid" width="490"></p>
<p>Different predictors are included: sex, pclass, age. Each predictor has a certain threshold or a certain selection of category, which acts as criteria to divide the observations in regions through the branches yes / no.</p>
<p>In the root node the criteria is sex= male. For all observations (100%) it is checked, if they are male or not. 64% of the observations are male, but only have a 0.17 chance to survive the titanic. So they are a <code>No</code> (marked in red). 36 % of the observations are women and have a 0.77 chance to survive, which is above 50 %, so they are a <code>yes</code> (marked in blue).</p>
<p>Then, a next decision level is included, the <code>Pclass</code>. Here, again a certain criteria is set and again, the observations are split into smaller regions. Again, the mean probability of the regions is computed, if they would survive the titanic and it is given, how many observations lay in this region.</p>
<p>A possible predictor can be used multiple times in a tree in using different thresholds or selecting different categories.</p>
<p><em>Key question: how can we find the best split?</em></p>
<p>Just as in the regression setting, we use recursive binary splitting to grow a classification tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.</p>
<p>Luckily, they are many different methods, with which we can explore, what is the best.</p>
<p>One method: <strong>Gini index or Gini impurity</strong>! Measurement, how good the split is compared to others. We do that for all subgroubs(regions) and sum up the values. The decision tree with the smallest value wins!</p>
<p>Criteria for ‘as similar as possible’: reduction in classification error rate such as the Gini impurity or entropy</p>
<p>Gini impurity or Gini index:</p>
<p><span class="math display">\[
G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})
\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{mk}\)</span> represent the proportion of training observations in partitions <em>m</em> with the category <em>k</em>, e.g.&nbsp;split the observations in proportion with the category age, sex, pclass and compute the survival (yes/ no)</p>
<ul>
<li>for each split we look at the impurity of each side and the smaller the better Split can be decided by scientist or for all values of data set &amp; but really time consuming!)</li>
<li>small value: all values in the partition are either close to 0 or to 1</li>
<li>Hence, Gini index is a measure of <em>node impurity</em>; small values indicates that a partition contain predominantly oversvations from a single class</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>Here we can see, how we can split the observations with two predictors into regions, where the mean value of the regions define, if the observations in the region will survive or not:</li>
</ul>
<p><img src="figures/8.class2.png" class="img-fluid" width="400"></p>
<ul>
<li>If the observation has a fare &gt;= 50, the person is likely to survive and if the persion has a fare =&lt; than 50, but is younger than 8, the person has a good chance to survive.</li>
</ul>
<p><img src="figures/8.class3.png" class="img-fluid" width="400"></p>
<ul>
<li><p>we can continue with splitting over and over again, producing smaller and smaller groups → we see, that at a certain point, more nodes make not sense at all, because producing all over the same outcome for subgroups, that would be predicted in the larger subgroup, too.</p></li>
<li><p>Attention! Splitting can only be done in not overlapping squares!</p></li>
</ul>
</section>
<section id="compare-trees-vs.-linear-classifiers" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="compare-trees-vs.-linear-classifiers"><span class="header-section-number">7.8</span> Compare trees vs.&nbsp;linear classifiers</h2>
<p><img src="figures/8.prun2.png" class="img-fluid" width="490"></p>
<ul>
<li>Although the decision trees can solve non-linear, complex relations, the decision trees not always better than linear classifiers.</li>
<li>Because looking for decision boundaries. if we have a linear problem, decision tree fails.</li>
<li>→ only perform well for non-linear relationsships</li>
<li>“Trees are very easy to explain and interpret”</li>
<li>“Automatically detects” nonlinear relationships and interactions</li>
<li>Trees can be easily displayed graphically (sometimes)</li>
<li>Usually worse performance than other common statistical learning methods, because: <em>Prone to overfitting (high variance)</em>: small change in the data can cause a large change in the final estimated tree.</li>
</ul>
<p>Can be improved by combining many trees: Bagging and Random Forests</p>
</section>
<section id="bagging" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="bagging"><span class="header-section-number">7.9</span> Bagging</h2>
<p><strong>Intuition behind bagging</strong>: When you fiddle (herumspielen) with the observations just a little, 1. some things vary a lot, 2. some things stay pretty much the same</p>
<p>Overfittung is caused by 1, but 1 happens randomly, causing predictions to go up or down hapharyardly (willkürlich, wahllos, zufällig). Therefore 1 should be canceled out by fiddling with the observations a little and averaging</p>
<p>Averaging: “Wisdom of the crowd”</p>
<p><strong>Bagging is a form of bootstrap aggregation</strong></p>
<ul>
<li>A general-purpose procedure for reducing the variance of statistical learning methods. It is very useful and often applied in the context of decision trees</li>
<li>when we have multiple sets (<em>n</em>) of independent observations with common variance <span class="math inline">\(\sigma^2\)</span> the variance over sets of the mean of all observations is given by <span class="math inline">\(\sigma^2 /n\)</span> .</li>
<li>hence, averaging a set of independent observations reduces variance. But, what when only training data?</li>
</ul>
<p>We can mimic having multiple training sets by bootstrapping:</p>
<ul>
<li>bootstrapping create resamplesof the sample S m times with replacement and acts like if we have different samples from population</li>
<li>generate <strong><em>B</em></strong> different bootrtrapped training data sets → set the number of trees</li>
<li>train a decision tree on each of the <span class="math inline">\(b^th\)</span> bootrtrapped training set to get a prediction for each observation <span class="math inline">\(x:\hat{f}^{*(b)} (x)\)</span></li>
<li>we average all predictions to obtain, so the output is the average of all trees <em>B</em> <span class="math display">\[
\hat{f_{avg}}(x)= \frac{1}{B} \sum_{b=1}^B \hat{f}^{*(b)} (x)
\]</span></li>
<li>when working with classification trees, we take the majority vote:the overall prediction is the most commonly occurring class among the B predictions.</li>
<li>do not need to prune! grow large trees in each bootstrapped sample and variance is decreased by averaging</li>
</ul>
<p><img src="figures/8.bag.png" class="img-fluid" width="450"></p>
<ul>
<li>three samples with 5 observations, selected with replacement, so randomly sampled in different features.</li>
<li>Result: three random samples, three decision trees for the each random sample</li>
<li>test with a new observation all three trees: for the predicted input we have 3 different predicted values, we take the average of it</li>
</ul>
<p><img src="figures/8.bag2.png" class="img-fluid" width="300"></p>
<ul>
<li>same procedure for classification → the majority wins!</li>
</ul>
</section>
<section id="random-forest" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">7.10</span> Random forest</h2>
<ul>
<li>Bagging works because averaging independent observations reduces the variance to the tune of n.</li>
<li>in bootstrapping, <em>samples taken are independent</em>.</li>
<li>but the <em>predictions</em> from the tree grown on the bootstrapped samples are <em>not independent</em>.</li>
<li>share the same features and can therefore create similarly overfitting in decision rules.</li>
<li>“Wisdom of crowds who peek at each other´s answers”</li>
<li>each sample has the same predictors and features! So the samples are independently but the features are not! &amp;rarr features correlate, “tree correlation” (Breiman 2001)</li>
<li>instead: use a subset of features to decorrelate → random forest try to remove this coorrelation by feature sampling: randomly sampling both rows (bootstrapping) and columns.</li>
<li>As in bagging, we build a number forest of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of <em>m</em> predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those <em>m</em> predictors.</li>
<li>typically <span class="math inline">\(m = /sqrt{p}\)</span> so number of predictors considered at each split approximately equal to total numbers of predictors</li>
<li>Using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors</li>
</ul>
<p>Decorrelation obtained by:</p>
<ul>
<li>When building a tree, instread of using all variables when making a split, take a random selection of <em>m</em> predictors as candidates to base the split on</li>
<li>at each split, take a fresh selection of <em>m</em> predictors <em>m</em> is typically set to <span class="math inline">\(\sqrt{p}\)</span></li>
<li>Similar to bagging, the collection of trees (=forest) is build on bootstrapped training samples</li>
</ul>
<p>Hence, bagging is a special case of a random forest <em>m=p</em></p>
<p><img src="figures/8.ran.png" class="img-fluid" width="300"></p>
<ul>
<li>bootstrap again. not every feature is in every sample</li>
<li>result: three decision trees with different nodes on each decision level.</li>
</ul>
</section>
<section id="other-methods" class="level2" data-number="7.11">
<h2 data-number="7.11" class="anchored" data-anchor-id="other-methods"><span class="header-section-number">7.11</span> Other methods</h2>
<ul>
<li>In <strong>boosting</strong> we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used.</li>
<li>In <strong>Bayesian Additive REgreesion Trees (BART)</strong> , we once again only make use of the original data, and we grow the trees successively. However, each tree is perturbed in order to avoid local minima and achieve a more thorough exploration of the model space.</li>
</ul>
</section>
<section id="out-of-bag-error-estimation" class="level2" data-number="7.12">
<h2 data-number="7.12" class="anchored" data-anchor-id="out-of-bag-error-estimation"><span class="header-section-number">7.12</span> “Out-of-bag” error estimation</h2>
<p>When we do bagging and random forest, there is a very simple way to estimate the test error without the need to perform cross/validation or the validation set approach</p>
<ul>
<li>in both methods, we take multiple bootstrapped samples of the training data. On average, each tree uses about 2/3 of the observations</li>
<li>ramaing 1/3 left observations are referred to as the out-of-bag (OOB) observations. Left out for test</li>
<li>If we want to calculate the error for a particular observation, we can predict the response using each of the trees in which it was OOB. This will give <em>B/3</em> predictions for this observation, which we average. When we do this for all observations we get the OOB error.</li>
<li>It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.</li>
</ul>
<p>And lastly, the OOB score is computed as the number of correctly predicted rows from the out of bag sample. out-of-bag error is an estimate of the error rate (which is 1 - accuracy) that this training approach has for new data from the same distribution. This estimate is based on the predictions that you get for each data point by using only averaging those trees, for which the record was not in the training data.</p>
<p>If you have a low number of trees the OOB error might be not a good estimate of the error rate that training an algorithm like this has on new data, because each tree in RF tends to be underfit and only once you combine enough trees the RF gets better (so if there’s only a small number of trees per record, it may underestimate performance). On the other hand, once you have a huge number of trees it becomes a pretty good estimate like you get from a train-validation split with a lot of data (or cross-validation).</p>
<p><img src="figures/8.compare1.png" class="img-fluid" width="500"></p>
</section>
<section id="variance-importance-measure-mdi" class="level2" data-number="7.13">
<h2 data-number="7.13" class="anchored" data-anchor-id="variance-importance-measure-mdi"><span class="header-section-number">7.13</span> Variance importance measure (MDI)</h2>
<p>In both bagging and random forest, it can be difficult to interpret the resulting model</p>
<ul>
<li>When large number of trees, no longer possible to represent graphically the resulting statistical learning procedure using a single tree</li>
<li>how to find out which predictor(s) are most important?</li>
</ul>
<p>Although the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).</p>
<p>→ variable importance measures: A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees</p>
<ul>
<li>unblackboxing the black box models!</li>
<li>Because when we have hundred trees, it is not easy to follow anymore.</li>
</ul>
<p>It´s not traceable, which variables are important and how they weight in the model overall.</p>
<p>How “important is variable <span class="math inline">\(x_j\)</span> to the prediction?</p>
<ul>
<li>recall that trees are grown by minimizing “impurity” (e.g.&nbsp;Gini, RSS)</li>
<li>idea: record the total amount that impurity is decreased due to splits over <span class="math inline">\(x_j\)</span>, averaged over all <em>B</em> trees</li>
<li>Advantage: obtained for free with estimation. We can do that easily, because we already have a tree</li>
<li>Disadavantage: imporance of features used to overfit inflated, importannce of numerical features inflated</li>
</ul>
<section id="permutation-based-feature-importance" class="level3" data-number="7.13.1">
<h3 data-number="7.13.1" class="anchored" data-anchor-id="permutation-based-feature-importance"><span class="header-section-number">7.13.1</span> Permutation-based feature importance</h3>
<ul>
<li>idea: randomly shuffle one column and observe how much worse it makes the model</li>
<li>advantage: does not have the problems of MDI</li>
<li>disadvantage: can take a while, results vary and ignores correlations among predictors (perfectly correlated features are all “unimportant”)</li>
</ul>
<p><strong>How to interpret?</strong></p>
<p>![]figures/8.imp.png){width=“300”}</p>
<p>The <em>Mean Decrease Accuracy</em> plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The <em>mean decrease in Gini coefficient</em> is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.</p>
</section>
</section>
<section id="in-r" class="level2" data-number="7.14">
<h2 data-number="7.14" class="anchored" data-anchor-id="in-r"><span class="header-section-number">7.14</span> in R</h2>
<pre><code>library(tree) #decision tree

your_tree_model &lt;-tree(bin_qual ~ fixed_acidity + residual_sugar + pH + sulphates, data=wine_train)

Now, let's evaluate whether pruning this tree may lead to an improved tree. For this, you use the function `cv.tree()`. This runs the tree repeatedly, at each step reducing the number of terminal nodes to determine how this impacts the deviation of the data. You will need to use the argument `FUN = prune.misclass` to indicate we are looking at a classification tree. 

library(randomForest) #random Forest

#bagging= max number of predictors are included, randomForest not alle predictors, only subset is included

r_bag &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula 
             data = wine_train,          # Data Set
             mtry = 7,          # Number of predictors to be considered for each split 
             importance = TRUE)  # The Variable importance estimator   
             
library(party) # importance measurement
varimp(model, conditional = TRUE) # shows the important predictors in a model</code></pre>
<p>In this practical we will cover an introduction to building tree-based models, for the purposes of regression and classification. This will build upon, and review the topics covered in the lecture, in addition to Chapter 8: Tree Based Models in Introduction to Statistical Learning.</p>
<p>You can download the student zip including all needed files for practical 8 <a href="https://surfdrive.surf.nl/files/index.php/s/J58fxg4AkOSKTcK">here</a>. For this practical, you will need the following packages:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># General Packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating validation and training set</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Trees</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("tree")</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forests &amp; Bagging </span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("party")</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Throughout this practical, we will be using the Red Wine Quality dataset from <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">Kaggle</a>, introduced within the Moving Beyond Linearity Lecture (Week 6).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>wine_dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/winequality-red.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="decision-trees" class="level3" data-number="7.14.1">
<h3 data-number="7.14.1" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">7.14.1</span> Decision Trees</h3>
<p>When examining classification or regression based problems, it is possible to use decision trees to address them. As a whole, regression and classification trees, follow a similar construction procedure, however their main difference exists in their usage; with regression trees being used for continuous outcome variables, and classification trees being used for categorical outcome variables. The other differences exist at the construction level, with regression trees being based around the average of the numerical target variable, with classification tree being based around the majority vote.</p>
<p>Knowing the difference between when to use a classification or regression tree is important, as it can influence the way you process and produce your decision trees.</p>
<ol type="1">
<li><strong>Using this knowledge about regression and classification trees, determine whether each of these research questions would be best addressed with a regression or classification tree.</strong></li>
</ol>
<p>Hint: Check the data sets in the <em>Help</em> panel in the <code>Rstudio</code> GUI.</p>
<ul>
<li>1a. Using the <code>Hitters</code> data set; You would like to predict the Number of hits in 1986 (<code>Hits</code> Variable), based upon the number of number of years in the major leagues (<code>Years</code> Variable) and the number of times at bat in 1986 (<code>AtBat</code> Variable).
<ul>
<li>Regression, because outcome is numeric variable</li>
</ul></li>
<li>1b. Using the <code>Hitters</code> data set; You would like to predict the players 1987 annual salary on opening day (<code>Salary</code> variable), based upon the number of hits in 1986 (<code>Hits</code> variable), the players league (<code>League</code> variable) and the number of number of years in the major leagues (<code>Years</code> Variable).
<ul>
<li>Regression, because outcome is numeric variable</li>
</ul></li>
<li>1c. Using the <code>Diamonds</code> data set; You would like to predict the quality of the diamonds cut (<code>cut</code> variable) based upon the price in US dollars (<code>price</code> variable) and the weight of the diamond (<code>carat</code> variable).
<ul>
<li>classification, because outcome is categorical</li>
</ul></li>
<li>1d. Using the <code>Diamonds</code> data set; You would like to predict the price of the diamond in US dollars (<code>price</code> variable), based upon the diamonds colour (<code>color</code> variable) and weight (<code>carat</code> variable).
<ul>
<li>Regression</li>
</ul></li>
<li>1e. Using the <code>Diamonds</code> data set; You would like to predict how clear a diamond would be (<code>clarity</code> variable), based upon its price in US dollars (<code>price</code> variable) and cut (<code>cut</code> variable).
<ul>
<li>classification</li>
</ul></li>
<li>1f. Using the <code>Titanic</code> data set; You would like to predict the survival of a specific passenger (<code>survived</code> variable), based upon their class (<code>Class</code> variable), sex (<code>Sex</code> variable) and age (<code>Age</code> variable).
<ul>
<li>Classification</li>
</ul></li>
</ul>
</section>
<section id="classification-trees-1" class="level3" data-number="7.14.2">
<h3 data-number="7.14.2" class="anchored" data-anchor-id="classification-trees-1"><span class="header-section-number">7.14.2</span> Classification trees</h3>
<p>Before we start <em>growing</em> our own decision trees, let us first explore the data set we will be using for these exercises. This as previously mentioned is a data set from <a href="https://www.kaggle.com/">Kaggle</a>, looking at the Quality of Red Wine from Portugal. Using the functions <code>str()</code> and <code>summary()</code>, explore this data set (<code>wine_dat</code>).</p>
<p>As you can see this contains over 1500 observations across 12 variables, of which 11 can be considered continuous, and 1 can be considered categorical (<code>quality</code>).</p>
<p>Now we have explored the data this practical will be structured around, let us focus on how to <em>grow</em> classification trees. The research question we will investigate is whether we can predict wine quality, classified as <em>Good</em> (Quality &gt; 5) or <em>Poor</em> (Quality &lt;= 5), by the Fixed Acidity (<code>fixed_acidity</code>), amount of residual sugars (<code>residual_sugar</code>), pH (<code>pH</code>) and amount of sulphates (<code>sulphates</code>).</p>
<p>Before we <em>grow</em> this tree, we must create an additional variable, which indicates whether a wine is of <em>Good</em> or <em>Poor</em> quality, based upon the quality of the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wine_dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> fixed_acidity   volatile_acidity  citric_acid    residual_sugar  
 Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  
 1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  
 Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  
 Mean   : 8.32   Mean   :0.5278   Mean   :0.271   Mean   : 2.539  
 3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  
 Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  
   chlorides       free_sulfur_dioxide total_sulfur_dioxide    density      
 Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  
 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  
 Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  
 Mean   :0.08747   Mean   :15.87       Mean   : 46.47       Mean   :0.9967  
 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9978  
 Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  
       pH          sulphates         alcohol         quality     
 Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  
 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  
 Median :3.310   Median :0.6200   Median :10.20   Median :6.000  
 Mean   :3.311   Mean   :0.6581   Mean   :10.42   Mean   :5.636  
 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  
 Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(wine_dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   1599 obs. of  12 variables:
 $ fixed_acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
 $ volatile_acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
 $ citric_acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
 $ residual_sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
 $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
 $ free_sulfur_dioxide : num  11 25 15 17 11 13 15 15 9 17 ...
 $ total_sulfur_dioxide: num  34 67 54 60 34 40 59 21 18 102 ...
 $ density             : num  0.998 0.997 0.997 0.998 0.998 ...
 $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
 $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
 $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
 $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...</code></pre>
</div>
</div>
<ol start="2" type="1">
<li><strong>Using the code below, create a new variable <code>bin_qual</code> (short for binary quality) as part of the <code>wine_dat</code> data set.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>wine_dat<span class="sc">$</span>bin_qual <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(wine_dat<span class="sc">$</span>quality <span class="sc">&lt;=</span> <span class="st">"5"</span>, <span class="st">"Poor"</span>, <span class="st">"Good"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>wine_dat<span class="sc">$</span>bin_qual <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_dat<span class="sc">$</span>bin_qual)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will split the data set into a <em>training</em> set and a <em>validation</em> set (for this practical, we are not using a test set). As previously discussed in other practicals these are incredibly important as these are what we will be using to develop (or train) our model before confirming them. <em>As a general rule of thumb for machine learning, you should use a 80/20 split, however in reality use a split you are most comfortable with!</em></p>
<ol start="3" type="1">
<li><strong>Use the code given below to set a seed of 1003 (for reproducibility) and construct a training and validation set.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1003</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(wine_dat<span class="sc">$</span>bin_qual, <span class="at">p =</span> .<span class="dv">8</span>, </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">list =</span> <span class="cn">FALSE</span>, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>wine_train <span class="ot">&lt;-</span> wine_dat[train_index,]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>wine_valid <span class="ot">&lt;-</span> wine_dat[<span class="sc">-</span>train_index,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This should now give you the split data sets of train &amp; validate, containing 1278 and 321 observations respectively.</p>
</section>
<section id="building-classification-trees" class="level3" data-number="7.14.3">
<h3 data-number="7.14.3" class="anchored" data-anchor-id="building-classification-trees"><span class="header-section-number">7.14.3</span> Building Classification Trees</h3>
<p>Now that you have split the quality of the wine into this dichotomous pair and created a training and validation set, you can <em>grow</em> a classification tree. In order to build up a classification tree, we will be using the function <code>tree()</code> from the <a href="https://cran.r-project.org/web/packages/tree/index.html"><code>tree</code></a> package, it should be noted although there are multiple different methods of creating decision trees, we will focus on the <code>tree()</code> function. As such this requires the following minimum components:</p>
<ul>
<li>formula</li>
<li>data</li>
<li>subset</li>
</ul>
<p>When <em>growing</em> a tree using this function, it works in a similar way to the <code>lm()</code> function, regarding the input of a formula, specific of the data and additionally how the data should be sub-setted.</p>
<ol start="4" type="1">
<li><strong>Using the <code>tree()</code> function, grow a tree to investigate whether we can predict wine quality classified as <em>Good</em> (Quality &gt; 5) or <em>Poor</em> (Quality &lt;= 5), by <code>fixed_acidity</code>, <code>residual_sugar</code>, <code>pH</code> and <code>sulphates</code>.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>your_tree_model <span class="ot">&lt;-</span><span class="fu">tree</span>(bin_qual <span class="sc">~</span> fixed_acidity <span class="sc">+</span> residual_sugar <span class="sc">+</span> pH <span class="sc">+</span> sulphates, <span class="at">data=</span>wine_train)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>?tree</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on topic 'tree' was found in the following packages:

  Package               Library
  xfun                  /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/xfun/0.49/8687398773806cfff9401a2feca96298
  tree                  /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/tree/1.0-44/62b3f891c7f9be249e426bfc6cdb2eac
  cli                   /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/cli/3.6.3/b21916dd77a27642b447374a5d30ecf3


Using the first match ...</code></pre>
</div>
</div>
</section>
<section id="plotting-classification-trees" class="level3" data-number="7.14.4">
<h3 data-number="7.14.4" class="anchored" data-anchor-id="plotting-classification-trees"><span class="header-section-number">7.14.4</span> Plotting Classification Trees</h3>
<p>When plotting decision trees, most commonly this uses the <code>base</code> R’s <code>plot()</code> function, rather than any <code>ggplot()</code> function. As such to plot a regression tree, you simply need to run the function <code>plot()</code> including the model as its main argument.</p>
<ol start="5" type="1">
<li><strong>Using the <code>plot()</code> function, plot the outcome object of your regression tree.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(your_tree_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tree-based_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>As you can see when you plot this, this only plots the empty decision tree, as such you will need to add a <code>text()</code> function separately.</p>
<ol start="6" type="1">
<li><strong>Repeat plotting the outcome object of your regression tree, with in the next line adding the<code>text()</code> function, with as input <code>your_tree_model</code> and <code>pretty = 0</code>.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(your_tree_model)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(your_tree_model, <span class="at">pretty=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tree-based_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This now adds the text to the to the decision tree allowing it to be specified visually.</p>
<p>Although plotting the regression tree can be useful for displaying how a topic is split, it only goes some way to answering the research question presented. As such, additional steps are required to ensure that the tree is efficiently fitted.</p>
<p>Firstly, you can explore the layout of the current model using the <code>summary()</code> function. This displays the predictors used within the tree; the number of terminal nodes; the residual mean deviance and the distribution of the residuals.</p>
<ol start="7" type="1">
<li><strong>Using the <code>summary()</code> function, examine the current decision tree, and report the number of terminal nodes and the residual mean deviance.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(your_tree_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification tree:
tree(formula = bin_qual ~ fixed_acidity + residual_sugar + pH + 
    sulphates, data = wine_train)
Variables actually used in tree construction:
[1] "sulphates"
Number of terminal nodes:  5 
Residual mean deviance:  1.235 = 1575 / 1275 
Misclassification error rate: 0.3336 = 427 / 1280 </code></pre>
</div>
</div>
</section>
<section id="assessing-accuracy-and-pruning-of-classification-trees" class="level3" data-number="7.14.5">
<h3 data-number="7.14.5" class="anchored" data-anchor-id="assessing-accuracy-and-pruning-of-classification-trees"><span class="header-section-number">7.14.5</span> Assessing accuracy and pruning of classification trees</h3>
<p>During the homework part, you have fitted a classification tree on the Red Wine Quality dataset. In this first part during the lab, we will continue with your fitted tree, and inspect its overall accuracy and improvement through pruning. To examine the overall accuracy of the model, you should determine the prediction accuracy both for the training and the validation set. Using the predicted and observed outcome values, we can construct a confusion matrix, similar to what we’ve done in week 5 on Classification methods.</p>
<p>So let us build a confusion matrix for the training subset. To begin, once more you need to calculate the predicted value under the model. However, now you need to specify that you want to use <code>type = "class"</code>; before forming a table between the predicted values and the actual values. As shown below</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the predictions </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    yhat_wine_train <span class="ot">&lt;-</span> <span class="fu">predict</span>(your_tree_model, <span class="at">newdata =</span> wine_train, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the observed outcomes of the training data </span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    qual_train <span class="ot">&lt;-</span> wine_train[, <span class="st">"bin_qual"</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the cross table:</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    tab_wine_train <span class="ot">&lt;-</span> <span class="fu">table</span>(yhat_wine_train, qual_train)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    tab_wine_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               qual_train
yhat_wine_train Good Poor
           Good  511  254
           Poor  173  342</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(tab_wine_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1280</code></pre>
</div>
</div>
<p>The obtained confusion matrix indicates the frequency of a wine predicted as good while it is actually good or poor, and the frequency of wine predicted as poor while actually being good or poor. The frequencies in the confusion matrix are used to determine the accuracy through the formula:</p>
<p>Accuracy = (Total Correct-True Predict [1,1] + Total Correct-False Predictions [2,2]) / total number of items</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Accuracy accordingly: </span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  accuracy_wine_train <span class="ot">&lt;-</span> (tab_wine_train[<span class="dv">1</span>] <span class="sc">+</span> tab_wine_train[<span class="dv">4</span>]) <span class="sc">/</span> <span class="dv">1280</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  accuracy_wine_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6664062</code></pre>
</div>
</div>
<p>From this, you can see that this model has an accuracy of around 67% meaning that 67% of the time, it is able to correctly predict from the predictors whether a wine will be of <em>good</em> or <em>poor</em> quality.</p>
<ol start="8" type="1">
<li><strong>Using this format, create a confusion matrix for the validation subset, and calculate the associated accuracy.</strong></li>
</ol>
<p><em>Hint</em>: you can obtain the predicted outcomes for the validation set using <code>predict(your_tree_model, newdata = wine_valid, type = "class")</code> and you can extract the observed outcomes of the validation data using <code>wine_valid[, "bin_qual"]</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>yhat_wine_valid <span class="ot">&lt;-</span> <span class="fu">predict</span>(your_tree_model, <span class="at">newdata =</span> wine_valid, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain the observed outcomes to the validation set</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>qual_valid <span class="ot">&lt;-</span>wine_valid[, <span class="st">"bin_qual"</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Create cross table</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>tab_wine_valid <span class="ot">&lt;-</span> <span class="fu">table</span>(yhat_wine_valid, qual_valid)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>tab_wine_valid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               qual_valid
yhat_wine_valid Good Poor
           Good  126   62
           Poor   45   86</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(tab_wine_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 319</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  accuracy_wine_valid <span class="ot">&lt;-</span> (tab_wine_valid[<span class="dv">1</span>] <span class="sc">+</span> tab_wine_valid[<span class="dv">4</span>]) <span class="sc">/</span> <span class="dv">319</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  accuracy_wine_valid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6645768</code></pre>
</div>
</div>
<p>On the validation data set it performs slightly less well.</p>
<p>Now, let’s evaluate whether pruning this tree may lead to an improved tree. For this, you use the function <code>cv.tree()</code>. This runs the tree repeatedly, at each step reducing the number of terminal nodes to determine how this impacts the deviation of the data. You will need to use the argument <code>FUN = prune.misclass</code> to indicate we are looking at a classification tree.</p>
<ol start="9" type="1">
<li><strong>Run the model through the <code>cv.tree()</code> function and examine the outcome using the <code>plot()</code> function using the code below.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine the cv.tree</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  cv_quality <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(your_tree_model, <span class="at">FUN=</span>prune.misclass)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>?cv.tree</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the size vs dev</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(cv_quality<span class="sc">$</span>size, cv_quality<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">'b'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When you have run this code, you should observe a graph, which plots the <code>size</code> (the amount of nodes) against the <code>dev</code> (cross-validation error rate). This indicates how this error rate changes depending on how many nodes are used. In this case you should be able to observe a steep drop in <code>dev</code> between 1 and 2, before it slowing down from 2 to 5 (the maximum number of nodes used). If you would further like to inspect this, you could compare the accuracy (obtained from the confusion matrices) between these different models, to see which is best fitting. In order to prune the decision tree, you simply use the function <code>prune.misclass()</code>, providing both the model and <code>best = number of nodes</code> as your arguments.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>?prune.misclass</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>model_tree2 <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(your_tree_model, <span class="at">best=</span> <span class="dv">2</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_tree2);<span class="fu">text</span>(model_tree2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tree-based_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Note that in the same way as growing classification trees, you can use the function <code>tree()</code> to grow regression trees. Regarding the input of the function <code>tree()</code> nothing has to be changed: the function detects whether the outcome variable is categorical as seen in the above example, applying classification trees, or continuous, applying regression trees. Differences arise at evaluating the decision tree (inspecting the confusion matrix and accuracy for classification trees or inspecting the MSE for regression trees) and at pruning. To prune a classification tree, you use the function <code>prune.mislcass()</code>, while for regression trees the function <code>prune.tree()</code> is used.</p>
</section>
<section id="bagging-and-random-forests" class="level3" data-number="7.14.6">
<h3 data-number="7.14.6" class="anchored" data-anchor-id="bagging-and-random-forests"><span class="header-section-number">7.14.6</span> Bagging and Random Forests</h3>
<p>When examining the techniques of Bagging and Random Forests, it is important to remember that Bagging is simply a specialized case of Random Forests where the number of predictors randomly sampled as candidates at each split is equal to the number of predictors available, and the number of considered splits is equal to the number of predictors.</p>
<p>So for example, if you were looking to predict the quality of wine (as we have done during the classification tree section), based upon the predictors fixed acidity (<code>fixed_acidity</code>), citric acid (<code>citric_acid</code>), residual sugars (<code>residual_sugar</code>), pH (<code>pH</code>), total sulfur dioxide content (<code>total_sulfar_dioxide</code>), density (<code>density</code>) and alcohol (<code>alcohol</code>) content. If we were to undergo the bagging process we would limit the number of splits within the analysis to 7, whereas within random forest it could be any number of values you choose.</p>
<p>As such, the process of doing Bagging or Random Forests is similar, and both will be covered. When using these methods we get an additional measure for model accuracy in addition to the MSE: the out of bag (OOB) estimator. Also, we can use the variable importance measure to inspect which predictors in our model contribute most to accurately predicting our outcome.</p>
<p>Note that we will focus on a classification example, while the ISLR textbook focuses on a regression tree example.</p>
</section>
<section id="bagging-1" class="level3" data-number="7.14.7">
<h3 data-number="7.14.7" class="anchored" data-anchor-id="bagging-1"><span class="header-section-number">7.14.7</span> Bagging</h3>
<p>Both Bagging and Random Forest are based around the function <code>randomForest()</code> from the <a href="https://cran.r-project.org/package=randomForest">randomForest</a> package. The function requires the following components:</p>
<pre><code>randomForest(formula = ???,       # Tree Formula 
             data = ???,          # Data Set
             subset = ???,        # How the data set should be subsetted 
             mtry = ???,          # Number of predictors to be considered for each split 
             importance = TRUE,   # The Variable importance estimator
             ntree = ???)         # The number of trees within the random forest</code></pre>
<p>In the case of bagging, the argument <code>mtry</code> should be set to the quantity of the predictors used within the model.</p>
<ol start="10" type="1">
<li><strong>Create a bagging model for the research question: can we predict quality of wine <code>bin_qual</code>, by <code>fixed_acidity</code>, <code>citric_acid</code>, <code>residual_sugar</code>, <code>pH</code>, <code>total_sulfur_dioxide</code>, <code>density</code> and <code>alcohol</code> and inspect the output. Omit <code>ntree</code> from the functions input for now.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>r_bag <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="at">formula =</span> bin_qual <span class="sc">~</span> fixed_acidity <span class="sc">+</span> citric_acid <span class="sc">+</span> residual_sugar <span class="sc">+</span> pH <span class="sc">+</span> total_sulfur_dioxide <span class="sc">+</span> density <span class="sc">+</span> alcohol,       <span class="co"># Tree Formula </span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> wine_train,          <span class="co"># Data Set</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">mtry =</span> <span class="dv">7</span>,          <span class="co"># Number of predictors to be considered for each split </span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">importance =</span> <span class="cn">TRUE</span>)  <span class="co"># The Variable importance estimator       </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>How do we interpret the output of this classification example? From this output, you can observe several different components. Firstly, you should be able to observe that it recognizes that this is a <em>Classification</em> forest, with 500 trees (the default setting for number of trees) and 7 variables tried at each split. In addition, the OOB estimate is provided in the output as well as a classification confusion matrix.</p>
<p>Let us examine the the accuracy level of our initial model, and compare it to the accuracy of models with a varying number of trees used.</p>
<ol start="11" type="1">
<li><strong>Calculate the accuracy of the bagged forest you just fitted</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>acc_r_bag <span class="ot">&lt;-</span> (r_bag<span class="sc">$</span>confusion[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">+</span> (r_bag<span class="sc">$</span>confusion[<span class="dv">2</span>,<span class="dv">2</span>]) <span class="sc">/</span> <span class="dv">1280</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s have a look at the Out of Bag (OOB) estimator of error rate. The OOB estimator of the error rate is provided automatically with the latest version of <code>randomForest()</code>, and can be used as a valid <em>estimator</em> of the test error of the model. In the OOB estimator of the error rate, the left out data at each bootstrapped sample (hence, Out of Bag) is used as the validation set. That is, the response for each observation is predicted using each of the trees in which that observation was OOB. This score, like other indicates of accuracy deviation, you will want as low as possible, since it indicates the error rate.</p>
<ol start="12" type="1">
<li><strong>Inspect the OOB scores of the bagged forest you fitted.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#you can read that of the model</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">#500 Trees = 22.46%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="13" type="1">
<li><strong>Fit two additional models, in which you set the number of trees used to 100 and 10000, and inspect the OOB scores. Which has the highest and lowest OOB estimate?</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">#specifying the number of trees</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>r_bag_100 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="at">formula =</span> bin_qual <span class="sc">~</span> fixed_acidity <span class="sc">+</span> citric_acid <span class="sc">+</span> residual_sugar <span class="sc">+</span> pH <span class="sc">+</span> total_sulfur_dioxide <span class="sc">+</span> density <span class="sc">+</span> alcohol,       <span class="co"># Tree Formula </span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> wine_train,          <span class="co"># Data Set</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">mtry =</span> <span class="dv">7</span>,          <span class="co"># Number of predictors to be considered for each split </span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">importance =</span> <span class="cn">TRUE</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">ntree =</span> <span class="dv">100</span>)  <span class="co"># The Variable importance estimator   </span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">#specifying the number of trees</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>r_bag_10000 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="at">formula =</span> bin_qual <span class="sc">~</span> fixed_acidity <span class="sc">+</span> citric_acid <span class="sc">+</span> residual_sugar <span class="sc">+</span> pH <span class="sc">+</span> total_sulfur_dioxide <span class="sc">+</span> density <span class="sc">+</span> alcohol,       <span class="co"># Tree Formula </span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> wine_train,          <span class="co"># Data Set</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">mtry =</span> <span class="dv">7</span>,          <span class="co"># Number of predictors to be considered for each split </span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">importance =</span> <span class="cn">TRUE</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>             <span class="at">ntree =</span> <span class="dv">10000</span>)  <span class="co"># The Variable importance estimator  </span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">#read it again from the output!</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(r_bag)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tree-based_files/figure-html/bag_vary_nt-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forests" class="level3" data-number="7.14.8">
<h3 data-number="7.14.8" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">7.14.8</span> Random Forests</h3>
<p>The main difference between Bagging and Random Forests is that the number of predictors randomly sampled as candidates at each split is not equal to the number of predictors available. In this case, typically (by default from the <code>randomForest()</code> function), they determine <code>mtry</code> to be 1/3 the number of available predictors for regression trees and the square root of the number of available predictors for classification trees.</p>
<ol start="14" type="1">
<li><strong>Using the <code>randomForest()</code> function, construct a random forest model using <code>mtry = 3</code> and <code>ntree = 500</code>.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>r_forest <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="at">formula =</span> bin_qual <span class="sc">~</span> fixed_acidity <span class="sc">+</span> citric_acid <span class="sc">+</span> residual_sugar <span class="sc">+</span> pH <span class="sc">+</span> total_sulfur_dioxide <span class="sc">+</span> density <span class="sc">+</span> alcohol,       <span class="co"># Tree Formula </span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> wine_train,          <span class="co"># Data Set</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">mtry =</span> <span class="dv">3</span>,          <span class="co"># Number of predictors to be considered for each split </span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">importance =</span> <span class="cn">TRUE</span>,</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">ntree =</span> <span class="dv">500</span>)  <span class="co"># The Variable importance estimator  </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="15" type="1">
<li><strong>Inspect fitted random forest model and the corresponding the OOB estimate of the random forest model and compare to the OOB estimate of the bagged forest model with <code>ntree = 500</code>.</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 21.33 %</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The OOB estimate is really close to the bagging model. Randomforest prevents overfitting, but because we do not have a lot of predictors here, it is not the case here.</p>
</section>
<section id="variable-importance" class="level3" data-number="7.14.9">
<h3 data-number="7.14.9" class="anchored" data-anchor-id="variable-importance"><span class="header-section-number">7.14.9</span> Variable importance</h3>
<p>The final (optional) part of this practical will look into how to actually interpret Random Forests, using the Variable Importance Measure. As you have probably worked out from section so far, physically representing these forests is incredibly difficult and harder to interpret them, in comparison to solo trees. As such, although creating random forests improves the prediction accuracy of a model, this is at the expense of interpretability. Therefore, to understand the role of different predictors within the forests as a whole it is important to examine the measure of Variable Importance.</p>
<p>Overall, when looking at Regression Trees, this Variable Importance measure is calculated using the residual sum of squares (RSS) and via the Gini Index for classification trees. Conveniently, the correct version will be determined by the <code>randomForest()</code> function, as it can recognize whether you are creating a regression or classification tree forest.</p>
<p>In order to call this measure, we simply need to call the model into the function <code>importance()</code>. Within our case (looking at a classification forest) this will produce four columns, the binary outcome (Good/Poor) in addition to the Mean Decrease in Accuracy and the Mean Decrease in Gini Index. This is by contrast to those which you will find when examining Regression trees, examples of which can be found in ISLR Section 8.3.3.</p>
<p>In order to best interpret these findings, it is possible to plot, how important each predictor is using the function <code>varImpPlot()</code>. This will produce a sorted plot which will show the most to least important variables.</p>
<ol start="16" type="1">
<li><strong>Using your random forest model, examine the importance of the predictors using <code>importance()</code> and use <code>varImpPlot()</code> to plot the result. Which predictor is most important to predict the quality of Wine?</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">#table</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>forest_imp <span class="ot">&lt;-</span> <span class="fu">importance</span>(r_forest)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co">#plot</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(r_forest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tree-based_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is a fundamental outcome of the random forest and it shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.</p>
<p>GINI: GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. Permuting a useful variable, tend to give relatively large decrease in mean gini-gain. GINI importance is closely related to the local decision function, that random forest uses to select the best available split. Therefore, it does not take much extra time to compute. On the other hand, mean gini-gain in local splits, is not necessarily what is most useful to measure, in contrary to change of overall model performance. Gini importance is overall inferior to (permutation based) variable importance as it is relatively more biased, more unstable and tend to answer a more indirect question.</p>
<p>E. g. alcohol is really relevant. Gini checks, how much the homogenity in nodes decreases, if a certain predictor is removed. The higher the the homogenity effect of one predictor is, the higher is the mean decrease in Gini.</p>
</section>
</section>
<section id="conclusions" class="level2" data-number="7.15">
<h2 data-number="7.15" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">7.15</span> Conclusions</h2>
<ul>
<li>Decision trees are simple and useful for interpretation</li>
<li>however, prone to overfitting. Solutions: pruning, bagging and random forests</li>
<li>Bagging: fit multiple trees to bootstrapped samples of the data, combine to yield a single consensus prediction</li>
<li>Random Forest: fit trees to bootstrapped samples from the data AND sample predictors. Combine all trees to yield a consensus prediction</li>
<li>When using bagging and random forests</li>
<li>We can approximate the test error using the Out of Bag (OOB) estimate of the error</li>
<li>Which predictor is most influential on the outcome can be inferred from variable importance measures</li>
<li>Random forests often show top-tier performance out of the box, but the resulting model is difficult to interpret</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./beyond-linearity.html" class="pagination-link" aria-label="Beyond linearity">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Beyond linearity</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./text-mining.html" class="pagination-link" aria-label="Text Mining">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Text Mining</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>