[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guides for Supervised Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto Book provides guidance for machine learning methods and advanced data visualiziation in R. The book is based on the course “Applied Data Analysis & Visualisation” from the Department of Methodology and Statistics (University Utrecht).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Parametric Methods\nThis chapter gives an introduction to machine learning.\nReadings: ISLR\nData Set Website Kaggle: https://www.kaggle.com/datasets\nWhat is Statical Learning?\nData analysis goals:\nModes & Examples:\nDifferent objections in Data Science and Economics for estimating the function of Y\nData analysis: estimating a model f to summarize our data, composed of an outcome Y and a set of predictors \\(X:\\hat{Y} =f(\\hat{Y})\\). Two main reasons to estimate f.\n\\[\n    \\hat{Y} =\\hat{f}(X)\n\\]\nThe accuracy of \\(\\hat{Y}\\) for Y depends on reducible and irreducible error. First can be reduced using the most appropriate statistical learning technique. irreducible because of the error term, that has variability \\(0\\), because this one include unmeasured variables that are useful in predicting Y with not measurable variation.\nonly the reducible error can be minimized by the right techniques, keep in mind, always irreducible error, which is unknown!\nData science: mostly interested in prediciton, so in \\(\\hat{Y}\\).\nEmpirical research /economics: mostly interested in inference (i.e., explanation): understanding the relationship beween X and Y → linear regression.\nReduces the problem of estimating f down to an estimating set of parameters. Two steps:\n\\[\nf(X)= \\beta_0 +\\beta_1 X_1 + \\beta_2X_2 + \\dots, \\beta_pX_p\n\\]\n\\[\nY \\sim \\beta_0 +\\beta_1 X_1 + \\beta_2X_2 + \\dots, \\beta_pX_p\n\\]\nMost used method: ordninary least squares. Mostly a linear model. Has the problem, that often linear models can´t grasp the entirely abitrary funktion f\n→ address this problem with more flexible models, but requires a greater number of parameters → problem of overfitting data, which means the model follow the errors / noise too closely.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#parametric-methods",
    "href": "intro.html#parametric-methods",
    "title": "1  Introduction",
    "section": "",
    "text": "f is a linear of x (choose a parametric form)\n\n\n\nnow, you have to estimate the parameters, the \\(\\beta\\) with the train data, that fits a model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#non-parametric-methods",
    "href": "intro.html#non-parametric-methods",
    "title": "1  Introduction",
    "section": "1.2 Non-parametric Methods",
    "text": "1.2 Non-parametric Methods\nNo explicit assumptions are made for the f. No predictors are collected beforehand. They have the freedom to choose any functional form from the training data. As a result, for parametric models to estimate the mapping function they require much more data than parametric ones. Instead seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. → advantage is a more accurate fit for a wider range of possible shapes for f. → disadvantage is, that far more observations are necessary to estimate an accurate f. Most used method: thin-plate spline with a selected level of smoothness, that defines the level of roughness to the fit.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#model-accuracy-vs-interpretability",
    "href": "intro.html#model-accuracy-vs-interpretability",
    "title": "1  Introduction",
    "section": "1.3 Model accuracy vs interpretability",
    "text": "1.3 Model accuracy vs interpretability\nDeep Learning is 90 % Blackbox, 10 % is not. It is not obvious, what happen, because there are multiple, thousands of coefficients. We see the input and the outcome, but we can not figure out the certain coefficients and parameter. Many methods have different levels of flexibility, means how many parameters are estimated. Linear regression for example really inflexible, the thin plate splines are much more flexible. High model complexity in it means high flexibility! Area of conflict between Interpretability and flexibility and methods in this area:\n\nWhich model is adequate depends on the research subject. Not always is a very flexible approach the preffered one. A linear model, which is more restrictive,is very adequate for inference, because it is more interpretable. → with a restrictive model it is easier to understand the relationship between \\(Y\\) and \\(X_1, X_2, \\dots, X_p\\). With a flexible model it is really difficult to understand how any individual predictor is associated with the response.\n\nlasso relies on a linear model, but uses an alternative fitting procedure for estimating the coefficients, less flexible approach than linear regression, because a number of parameters set to zero.\nGAM or generalized additive models allow for certain non-linear relationships\nbagging, boosting, support vector machines and neural networks (deep learning) are fully non-linear methods.\n\nHow can we measure the accuracy of a model?\n\n\nWe should stay by 5 to 7 predictors in the flexibility/ complexity, because otherwise the model is overfed.\nBias trad off variance:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#methods-to-find-patterns-and-relationsships",
    "href": "intro.html#methods-to-find-patterns-and-relationsships",
    "title": "1  Introduction",
    "section": "1.4 Methods to find patterns and relationsships",
    "text": "1.4 Methods to find patterns and relationsships\nExplorative data analysis\nDescribing interesting patterns: use graphs, summaries, to understand subgroups, detect anomalies (“outliers”), understand the data\nExamples: boxplot, five-number summary, histograms, scatterplots…\n\n\n1.4.1 Unsupervised learning\nInputs, but no outputs. Try to learn structure and relationships from these data, like detecting unobserved groups (clusters) from the data. For every observation there is a measurement in x, but there is no associated response y_i, there is no response variable to predict → we are working blind, no response variable to supervise the analysis.\n\nFor detecting relationships between variables and observations → finding clusters in data\n\non the basis of \\(x_1, \\dots, x_n\\) in which relatively distinct groups fall the observations.\n\nDimension reduction methods:\n\nPrincipal components analysis\nFactor analysis\nRandom projections\n\nClustering:\n\nK-means clustering\nHierarchical clustering\n\n\n\n\n1.4.2 Supervised learning\nBuilding a statistical for predicting/ estimating an output based on one or more inputs. The output is predefined: Is it this or that? What outcome should be generated?\nFor each observation of the predictor measurement is an associated response measurement \\(y_i\\). On this basis a model is searched to accurately predicting the respones for future observations (prediction) or better understanding the relationsship between the response and the predictors (inference).\nExamples:\n\nSpam Classification of e-mail\nface recognition over images\nmedical diagnosis systems for patients\n\nMethods include:\n\n(logistic) Regression\ndecision trees/ random forests\nsupport vector machines\nneural networks\n\n\nsupervised learning: classification vs. regression:\nClassification:\npredict to which category an observation belongs (qualitative, categorical outcomes) -&gt; cancer: yes or no?, a boy or a girl?\nRegression:\npredict a quantitative (continuous outcome) -&gt; money, weight, amount of rain\nsemi-supervised learning problem:\nSometimes not that clear, if supervised or unsupervised learning is the better solution. This is the case, if we have more observations with predictor measurements and a response measurement than observations with measurements only.\nHere statistical learning is used to incorporate both kind of observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "2  Visualization",
    "section": "",
    "text": "3 Why data visualisations\nDatVis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#how-to-communicate-via-visualization",
    "href": "visualization.html#how-to-communicate-via-visualization",
    "title": "2  Visualization",
    "section": "3.1 How to communicate via visualization?",
    "text": "3.1 How to communicate via visualization?\n\nHow do we make sure that the graphs we make transfer:\nThe right part of the data, and; with the less effort possible? ( minimizes cognitive load)\nFirst step in a data visualization task: Write down the main message you want to convey\n\nCentral questions:\n\nWhat are the main elements of a graph? (labels, dots, bars, facets …)\n\n\n\nWhat type of plot should you use?\n\n\nBarplots for a categorical and a numerical variable, compare the frequency\nScatterplots for 2 numerical variables, shows covariances and relations of the two variables\n\n\n3.How can we make a plot look more professional? - take it as minimal as possible, no “junk”, no Color, if no color is needed, scale comprehensible\n\n\nHow to guide the reader?\n\n\nhighlight the central aspect",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#criteria-for-good-graphs-and-visualization",
    "href": "visualization.html#criteria-for-good-graphs-and-visualization",
    "title": "2  Visualization",
    "section": "3.2 Criteria for good graphs and visualization",
    "text": "3.2 Criteria for good graphs and visualization\nGuidelines for routine plotting:\n\nproperly chosen format and design\nuse words, numbers and drawing together\ndisplay an accessible complexity of detail\navoid content-free decoration\nmaximize the “data-to-ink” ratio\nsimplify, remove everything that is not necessary\nno cherry picking in data, visuals must be chosen in relation to data, example: Age cohorts in Barcharts, longtidual changes in point charts\nreduce aesthetics to a minimal and use colour and so on only if it has a meaning.\nhumans ability to see contrast is stronger for monochrome images than for color\nusing color in data visualization introduces a number of other complications, because color contains the hue (Farbton) and a chrominance ot chroma (intesity or vividness of the color):\n\nhow bright an object looks depends partly on the brightness of objects near it.\ndistance of variables should be found also in a perceptually sense in the choice of colors, not only in a numerical one\n\n“preattentive pop-out”: Some objects in our visual field are easier to see than others → indicate with shapes, color & position.\nMost people see the Poisson-generated pattern (a random generated pattern) as having more structure, or less ‘randomness’, than the Matérn (an equally distributed), whereas the reverse is true.\nhumans are always looking for structure, the tendency of infer relationships, “gestalt rules”:\n\nSimilarity: Things that look alike seem to be related.\nConnection: Things that are visually tied to one another seem to be related.\nContinuity: Partially hidden objects are completed into familiar shapes.\nClosure: Incomplete shapes are perceived as complete.\nFigure and Ground: Visual elements are taken to be either in the foreground or the background.\nCommon Fate: Elements sharing a direction of movement are perceived as a unit.\n\nhumans can identify and estimate percentages of differences of two sizes for graphs on a different level, here the results of testing:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#channels-and-type-of-graph-in-overwiew",
    "href": "visualization.html#channels-and-type-of-graph-in-overwiew",
    "title": "2  Visualization",
    "section": "3.3 Channels and type of graph in overwiew",
    "text": "3.3 Channels and type of graph in overwiew",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#principles-of-design",
    "href": "visualization.html#principles-of-design",
    "title": "2  Visualization",
    "section": "3.4 Principles of Design",
    "text": "3.4 Principles of Design\n\nPracitcal advice\nReduce cognitive load: - Removing unnecessary clutter - More professional/aesthetically pleasant Contrast: - Eliminate unnecessary lines (all frames, use gray grid lines, etc) - ’t use a gray background - White space is your friend (allows for “breathing”) - Enlarge the labels - Use vector graphics (svg/pdf/eps) to avoid blurry figures –&gt; Edit them in AI or Inkscape Repetition: Be consistent in different figures Alignment: Make sure you align subplots/labels Proximity: When possible, label data directly (instead of using legends)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#guide-the-reader",
    "href": "visualization.html#guide-the-reader",
    "title": "2  Visualization",
    "section": "3.5 Guide the reader",
    "text": "3.5 Guide the reader\n\nWe read plots in a Z-shaped flow: top-left to top-right to bottom-left to bottom-right\n\n\nWith this elements:\n\nThe most useful pre-attentive attribute: - Increases contrast - Allows for consistency (same country with the same color)\nColor affect emotion and this is culture-dependent. Some responses are nearly universal - Warm colors –&gt; alive/alert - Blue colors –&gt; calming/focus\nColor for the colorblinding: https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40\nIn addition of highlighting, colours can be used to: - Represent categories (not more than 4 colors) - Represent values: - Only if necessary (i.e. you are using the x and y axis for more important variables) - Not accurate (show trends)\n\n\nleft, too much: you are lost\nright, your attention is guided to the important aspects\n\n\n\n\nQualitative: categorial data\nSequential: The minimum or maximum is important\nDiverging: The middle value is the important one, which comparison is drawn on",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#how-ggplot-function-works",
    "href": "visualization.html#how-ggplot-function-works",
    "title": "2  Visualization",
    "section": "3.6 how ggplot function works",
    "text": "3.6 how ggplot function works\n\nrequired library: library(tidyverse)\nIn R, grammar of graphics is implemented in ggplot(), a function in the ggplot2 package.\nElements of a graph:\n\nThe data: ggplot(data = gapminder)\nAesthetic mappings (position, shape, color, …) – map variables to influence visual channels: mapping = aes(x = gdp, y = pop)\nGeometric objects (points, lines, bars, …) – use those mappings: + geom_point()\nLabels (titles, caption, axes labels): + labs(x = “GDP”, y=“Population”)\n\nggplot() is the function to plot aes or astehtic mappings is the logical connection bewtween your data and the plot element geom defines the type of plot like\n\ngeom_point\ngeom_bar\ngeom_boxplot\nin this function additional elements could be added like scales, labels and so on\n\nggplot function is additive, you add layer by layer, e. g.:\np &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp))\np + geom_point() +\n    geom_smooth(method = \"gam\") +\n    scale_x_log10(labels = scales::dollar)\nOverview ggplot aesthetics: https://ggplot2.tidyverse.org/reference/index.html#section-aesthetics\nOverview ggplot geometrics: https://ggplot2.tidyverse.org/reference/index.html#section-geoms",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html",
    "href": "model-accuracy.html",
    "title": "3  Model accuracy and fit",
    "section": "",
    "text": "3.1 Readings\nISLR:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#readings",
    "href": "model-accuracy.html#readings",
    "title": "3  Model accuracy and fit",
    "section": "",
    "text": "Chapter 2.2: Assessing Model Accuracy\nChapter 5.1: Resampling Methods: Cross-Validation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#how-do-we-estimate-f",
    "href": "model-accuracy.html#how-do-we-estimate-f",
    "title": "3  Model accuracy and fit",
    "section": "3.2 How do we estimate f?",
    "text": "3.2 How do we estimate f?\nto estimate training data with statistical learning methods: - parametric: make some assumption about the functional form of f, e .g. \\(Price = \\beta_0 + \\beta_1 * Tweets\\)\n - for every increase in one tweet, the bitcoin price will increase by 3,543 - methods: linear regression, with quadratic term, with higher-order-polynomials\n\nnon-parametric: not make an explicit assumptions about the functional form of f, e.g. price of bitcoin is the average of the 3 closest points in our data set\n\n\n\nmethods: non-parametric regression (LOESS, \\(y_i\\) from a “local” regression wihthin a window of its nearest neighbors) or K-NN regression ($y_i predicted from the vale of the closest neighbors)\n\n→ so different models to estimate f, which is the best? How to check if the model does a good job? - which predictors is the best? in more complexe models, vertain parameters have to be “tuned”. Which value for these tuned parameters is the best?\n\n3.2.1 What affects our ability to estimate f?\n\nirreducible error\nvariance of Y\nsample size\nmodel & task complexity\n\nExample for different \\(Var(\\epsilon)\\), “noise”\nThe bigger the “noise” the worse the estimate\n\n\nuse a more restrictive model\nbigger sample size\ninclude other variables\nuse a method, where some variables are hold fixed\n\nExample for sample size\nThe bigger the sample size, the better the estimate",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#model-performance",
    "href": "model-accuracy.html#model-performance",
    "title": "3  Model accuracy and fit",
    "section": "3.3 Model performance",
    "text": "3.3 Model performance\nExample:\n\nthe left is underfitting the data, the right one is more close to the observations\nin practice it is good to know that up to a certain number of tweets, the bitcoin price increase slows down\n\n\nHow can you formalize this, compute the model performance?\n\n3.3.1 Measuring the Quality of Fit\nEvaluate the performance of a statistical learning method on a given data set in measuring how well its predictions actually match the observed data.\nFor Regression: mean squared error (MSE)\nThe Mean squared error (MSE) represents the error of the estimator or predictive model created based on the given set of observations in the sample. Intuitively, the MSE is used to measure the quality of the model based on the predictions made on the entire training dataset vis-a-vis the true label/output value. In other words, it can be used to represent the cost associated with the predictions or the loss incurred in the predictions. And, the squared loss (difference between true & predicted value) is advantageous because they exaggerate the difference between the true value and the predicted value.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2,\n\\] If the MSE is smaller, the predicted responses are very close to the true and vice versa.\nWith the training MSE, computed out of the training data (\\(\\hat{f}(x_i)\\) \\(\\approx\\) \\(y_i\\)) we are interested in the accuracy of the predictions with the test set (\\(\\hat{f}(x_0)\\) \\(\\approx\\) \\(y_0\\)) ) → we want to choose the method that gives us the lowest test MSE, not the lowest training MSE, so we compute the average squared prediction error for the test observations:\n\\[\nAve(y_0-\\hat{f}(x_0))^2\n\\] In some settings, test set is available → sample is split in train and test set What, when no test set available? The learning method is chosen that minimizes the training MSE.\nDegrees of freedom: A quantity, that summarizes the flexibility of a curve → the more restricted an estimate is, e. g. an estimate computed with regression, the less degrees of freedom the curve has.\nProblem of overfitting with taking the training MSE for test MSE\nAs model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.\n→ traing MSE is smaller than test MSE. Especially when a less flexible model would have yielded a smaller test MSE.\n\n\n3.3.1.1 Other measures of the model accury besides the MSE\n\nRoot mean squared error (RMSE): \\[\n\\sqrt{MSE}\n\\]\n\nMean absolute error (MAE): \\[\nMAE= \\frac{1}{n} \\sum_{i=1}^{n} |y - \\hat{y})^2\n\\]\n\nMedian absolute error (mAE): \\[\nmAE = median |y - \\hat{y})^2\n\\]\nr-Squared: Proportion of variance explained (R2): R2=correlation(y,y^)2\n\n\\[\nR^2 = correlation (y, \\hat{y})^2\n\\] \\[\nR^2 = \\frac{\\sum (\\hat{y}_i - \\overline{y})^2} {\\sum y_i - \\overline{y})^2}\n\\] \\[\nR^2 = 1- \\frac{MSE}{Var(y)}\n\\] R-Squared is the ratio of the sum of squares regression (SSR) and the sum of squares total (SST). Sum of Squares Regression (SSR) represents the total variation of all the predicted values found on the regression line or plane from the mean value of all the values of response variables. The sum of squares total (SST) represents the total variation of actual values from the mean value of all the values of response variables. R-squared value is used to measure the goodness of fit or best-fit line. The greater the value of R-Squared, the better is the regression model as most of the variation of actual values from the mean value get explained by the regression model. However, we need to take caution while relying on R-squared to assess the performance of the regression model. This is where the adjusted R-squared concept comes into the picture. This would be discussed in one of the later posts. R-Squared is also termed as the coefficient of determination. For the training dataset, the value of R-squared is bounded between 0 and 1, but it can become negative for the test dataset if the SSE is greater than SST. Greater the value of R-squared would also mean a smaller value of MSE. If the value of R-Squared becomes 1 (ideal world scenario), the model fits the data perfectly with a corresponding MSE = 0. As the value of R-squared increases and become close to 1, the value of MSE becomes close to 0.\nR-Squared is also termed the standardized version of MSE. R-squared represents the fraction of variance of the actual value of the response variable captured by the regression model rather than the MSE which captures the residual error.\n\nIn this class focus on MSE, with that we can compute our decision and intuition:\n\nExample: Which one is better?\n\nThe left method is overfitting the data and fit to the noise, difficult to interprete.\nWe want to find a model, that is in the middle, that is “just right”:\n\nSo we should not care too much about the MSE, because we already know this observations, we want to understand new data, predict what will happen and generalize.\n\n\n\n3.3.2 Bias-Variance Trade Off\nexpected test MSE can be decomposed into the sum of three fundamental quantities: variance of \\(\\hat{f}(x_0)\\) , squared bias of \\(\\hat{f}(x_0)\\) and variance of the error term \\(\\epsilon\\).\nThe expected test MSE at \\(x_0\\) is:\n\\[\nE(y_o -\\hat{f}(x_0))^2 =  Var(\\hat{f}(x_0)) + Bias([\\hat{f}(x_0))]^2 + Var(\\epsilon)\n\\]\n\\[\nE(MSE) = E(\\frac{1}{2} \\sum_{i=1}^{n} (y - \\hat{y})^2) \\\\\n= E(\\frac{1}{2} \\sum_{i=1}^{n} ( outcome_i - predicted_i)^2) \\\\\n= Bias^2(model) + Variance(model) Variance(\\epsilon)\n\\]\nThe expected test MSE refers to Ave, that we would obtain if repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). Overall expected test MSE can be computed by averaging \\(E(y_o -\\hat{f}(x_0))^2\\) over all possible values of \\(x_0\\) in the test set. → for getting a small test MSE, we need a method that simultaneously achieves low variance and low bias (because both is squared it can not be negative). Furthermore expected test MSE can never be lower than \\(Var(\\epsilon)\\), the irreducible error.\nVariance: refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. → more flexible models have higher variance, because it is more volatile. It adapts more to specific to the observations. Variance refers the sensitiviy of our model to small fluctuations in the training dataset. Since the training data are used to fit the statistical learning method, different training data sets will result in a different \\(\\hat{f}\\) . High variance often comes from overfitting.\nBias: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. In reality not only X influences Y, there are many other variables and linear relationships are unlikely.\nVariance\\((\\epsilon)\\): Irreducible error\nConsequently, the U-shape is result of 2 competing properties: Bias and Variance. If you choose a more flexible method, the Variance will increase but the bias will decrease and vice versa for a more restrictive model. So you need the ideal degree of flexibility, that your test MSE stays small. → that is the bias-variance trade off\nMSE is influenced by both bias and variance: Model with high bias: Model that is not able to capture the complexity of the phenomena Model with high variance: Model that easily overfits accidental patterns\nin general more flexible models will fit the training data more closely, but they have the problem of overfitting → in the test set you can see the flexible model is printing the noise to and so have at a certain degree higher MSE with increasing flexibility.\n\nComplexity: Possible definitions of complexity:\nAmount of information in data absorbed into model; Amount of compression performed on data by model; Number of effective parameters, relative to effective degrees of freedom in data. For example: More predictors, more complexity; Higher-order polynomial, more complexity\n\nModel complexity vs. interpretability:\n\n\n\n3.3.3 The classification setting\nIn this setting, \\(y_i\\) is not metric and the regression logic is not applicable. Most common approach for quantifying the accuracy of estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations:\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n} I(y_i \\neq\\hat{y_i}).\n\\]\n\\(\\hat{y_i}\\) is class label for the ith observation using \\(\\hat{f}\\). \\(I(y_i \\neq\\hat{y_i})\\) is indicator variable that equals 1 if \\(y_i \\neq\\hat{y_i}\\) and 0 if \\(y_i = \\hat{y_i}\\). If \\(I(y_i \\neq\\hat{y_i} = 0)\\) than the ith observation was classified correctly, otherwise it is misclassified.\nWe are interested in the test error rate: \\[\nAveI(y_0 \\neq\\hat{y_0})\n\\] → the smaller the test error, the better the classifier.\nThe Bayes Classifier\nIt is possible to show that the test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values:\n\\[\nPr(Y = j | X= x_0)\n\\]\nconditional probability, is the probability that \\(Y=j\\) given the observed predictor vector \\(x_0\\).\nIf there are only two categories (binary) the Bayes classifier predict one class if \\(Pr(Y = j | X= x_0) &gt; 0.5\\) and for class two otherwise.\nExample with simulated data, like we have the test set available:\n\nPurple line: here is the probability exactly 50 % → Bayes decision boundary, it is the boundary, from which both observations are assigned to the groups.\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate, the classifier will always choose the class for which the probability is the largest, so:\n\\[\n1- E( max_j Pr(y=j |X))\n\\] for all possible X.\nK-Nearest Neighbors\nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.\nK-Nearest Neighbors (KNN) classifier estimate the conditional distribution of Y given X and then classify a given observation to the class with highest estimated probability.\nGiven a positive integer K and a test observation \\(x_0\\), the KNN classifier first identifies the K points in the training data that are closest to \\(x_0\\), represented by \\(N_0\\).Then it estimates the conditional probability for class j as the fraction of points in \\(N_0\\), whose response values equal j:\n\\[\nPr(Y=j | X= x_0) = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i=j)\n\\] Finally, test observation \\(x_0\\) to the class with the largest probability like the Bayes classifier.\n\nThe choice of K has a drastic effect on the KNN classifier obtained. Figure 2.16 displays two KNN fits to the simulated data from Figure 2.13, using K = 1 and K = 100. When K = 1, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.\n\nJust as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. → As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#how-to-estimate-the-generalization-error-emse-reliably-training-and-test-set",
    "href": "model-accuracy.html#how-to-estimate-the-generalization-error-emse-reliably-training-and-test-set",
    "title": "3  Model accuracy and fit",
    "section": "3.4 How to estimate the generalization error E(MSE) reliably? Training and test set",
    "text": "3.4 How to estimate the generalization error E(MSE) reliably? Training and test set\nWhy do we want to predict on unseen data? Conceptually: We often want to understand/predict a general phenomena, not just the observations we already have Pragmatically: It allows to understand if our model is overfitting the data. Idea = you can’t overfit data if you don’t see that data.\nTry the function with a new data set. Best practice: Use new data from a later time point. In practice it is really difficult to get data from the future, if you would like predict something, so, we hace to use new observations from the intended prediction situation. Often you only have one dataset, which we split in 2.\nDatasets:\n\nTraining: train the model\nobservations used to fit \\(\\hat{f}(x)\\)\nValidation: compare between models, tune models, selscz features\nnew observations from the same source as training data (used several times to select model complexity)\ntest: assess E(MSE) of the final model\nNew observations from the intended prediction situation (to evaluate E(MSE) for your final model) –&gt; Only used once in our final model!\n\n\nSteps:\n\nyou will have overfitting only using the training data set\nusing the training data set to train different models\nusing the validation set to select the best model\nusing the test data set to evluate the accuracy of the model (you can not use the validation set, because it is biased. With the validation data set you have selected the best model, so if you would test it with that, too, it is biased)\ncomputed MSE of the test data set to come to the true MSE, the E(MSE) to show, if the model is reliably\n\n\n\nusing the MSE of our models in a new data set, we can see which model is really the best and now we can see that the left one is not the best one, because of overfitting it is not that good fitting to new data → to volatile to the obervations and has been fitted to the noise\n\nWhen comparing the models, you can see, which model is the best: (in real world we do not have the actual distribution of observations, we estimate them)\n - we can see that the quadratic regression is the best, the KNN regression is overfitting the data.\nExample in practice, if we have one dataset:\n\nShuffle the observations\nDivide into training (85%) and testing (15%)\nTraining –&gt; Divide into training (70%) and validation (15%)\nTune models, evaluating models using MSE on the validation set\nSelect the best model\nUse the training + validation set to retrain the final model\nEvaluate the model using the test data\n\nProblem of bias, because you compare samples, in which the actual worse model fits better with the other samples\n\nThe validation estimate of the test error can be highly variable\nOnly a subset of the observations are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set.\nif performance on validation and performance on test set, start over again\n\n\n\n3.4.1 How to split the datasets\nTraining: Bulk of observations (~50-80%)\nValidation and testing: Smaller subsets (~10-20%) –&gt; Should be representative in order to estimate E(MSE) accurately.\ne.g. without cross-validation\n\nTraining: 50-70%\nValidation: 15-25%\nTest: 15-25%\n\ne.g. with cross-validation\n\nTraining: 70-80% + 5-10 fold cross-validation to separate into training/validation\nTest: 20-30%\n\n\n\n3.4.2 considerations with the test dataest und cross validation\nThe idea is that the \\(MSE_{test}\\) is a good estimate of the \\(E(MSE)\\) (prediction / Bayes error) → This is only true if the test data is similar to the prediction data!\n\nsometimes a wrong model is better than a true model, on average better → selecting a simpler model can be better, if you want to show relationships, because world is too complex\nthese factors together determine what works best:\nhow close the function form of \\(\\hat{f}(x)\\) is to the true \\(f(x)\\).\nthe amount of irreducible variance \\((\\omega^2)\\)\nthe sample size (n)\nthe complexity of model (p/df or equivalent)\n\n\n\n3.4.3 Two alternatives of model selection:\n\nComparing statistical methods (e.g. linear regression vs knn regression)\nComparing models with different predictors included (e.g. linear regression - including predictors [X1, X2] vs [X1, X2, X3] )\nComparing two models with different hyperparameters (e.g. KNN regression using - the closest 3 vs 10 neighbors)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#resampling-methods-cross-validation",
    "href": "model-accuracy.html#resampling-methods-cross-validation",
    "title": "3  Model accuracy and fit",
    "section": "3.5 Resampling Methods: Cross-Validation",
    "text": "3.5 Resampling Methods: Cross-Validation\n\nMethods for estimating test error rates and thereby choosing the optimal level of flexibility for a given statistical learning method.\ninvolve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\ncross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility\nevaluating model´s perfomance known as model assessment, process of selcting the proper level of flexibility is known as model selection.\nbasic mechanism: a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations\n\nThe following explanations are first made for regression and afterwards for classification\n\n3.5.1 The Validation Set Approach\nRandomly dividing the available set of observations into a model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. → resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.\nThis is not only done once, it is done multiple times and tested than.\nProblem: The results are not that clear, how much polynominals would produce the lowest MSE. In the Figure each training data produces another degree of polynominal → only thing that can be stated for sure is that a linear fit is not adequate. → very variable → too few observations, overestimate the test error rate\n\n\n\n3.5.2 Leave-One-Out Cross Validation (LOOCV)\nOnly a single observation is used for validation. The statistical learning method is fit on the \\(n - 1\\) training observations, and a prediction \\(\\hat{y_1}\\) is made for the excluded observation, using its value \\(x_1\\).\nBecause \\((x_1, y_1)\\) was not used in the fitting process, \\(MSE_1 = (y_1-\\hat{y_1})^2\\) is approximately unbiased estimate for the test error. → because only one observation is highly variable as a check up, same procedure by selecting \\((x_2, y_2)\\) and so on. Tge LOOOCV estimate for the ttest MSE is the average of these \\(n\\) test error estimates:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i=1}^{n} MSE_i\n\\]\nadvantages:\n\nfar less biased, because only one observation is included in each run → not to overestimate the test error rate\nrunning LOOCV multiple times always yields the same results → no randomness in the splits\n\n\nLOOCV has the potential to be expensive to implement, since the model has to be fit \\(n\\) times → shortcut:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{y_i -\\hat{y_i}} {1-h_i})^2\n\\]\n\n3.5.2.1 high leverage points\nLeverage is the influence, that one point has on the regression line, if the point is left out. How huge the leverage of a point is, depends on how far away the observation from other observations is. This is not depend on how much an observation can explain the residual sum of sqares (RSS). It can be, that a point is high in leverage but only explain a little part of RSS and vice versa. Points with a high leverage called high-leverage points. Data points at the edge are more likely to have high leverage than the points in the center.\n\\(h_i\\) is the leverage (In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations. High-leverage points, if any, are outliers with respect to the independent variables):\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2} {\\sum_{i´=1}^{n} (xi´-\\overline{x})^2}\n\\]\nLike ordinary MSE, exept the ith residual is devided by \\(1-h_i\\). Leverage lies between 1/n and 1. Selects the amount that an observation influences its own fit. Hold not in general, in which case the model has to be refit n times.\n\n\n\n3.5.3 k-Fold Cross-Validation\nDivided sets of observations into k groups /folds of approximately equal size First fold is validation set, method is fit on the remaing k-1 folds \\(MSE_1\\) is then computed on the held-out fold → procedure is repeated -times, each time, with a different group of observations is validation set. k-fold CV estimate:\n\\[\nCV_(k) = \\frac{1}{k} \\sum_{i=1}^{k} MSE_i\n\\] Consequently, LOOCV as a special case of the k-fold CV in which k is set to equal n. In practice, k-folds CV often performs using k=5 or k=10. Advantage: LOOCV requires fitting the statistical learning method n times. This has the potential to be computationally expensive → then only 5 or 10 times the learning procedure is fitted, more feasible.\n\n\nonly split in training and test data set\n‘Cross validation’ often used to replace single dev set approach;\nInstead of dividing one time the training dataset (into train/dev), do it many times.\nPerform the train/dev split several times, and average the result.\nUsually K = 5 or K = 10.\nWhen K = N, ‘leave-one-out’;\n\n In all three plots, the two cross-validation estimates are very similar.\nModel assessment: When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest.\nModel selection: But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility—that is, the flexibility level corresponding to the smallest test MSE.\n\n3.5.3.1 Bias-Variance Trade-Off for k-Fold Cross-Validations\nk-fold CV has computational advantage to LOOCV and often gives more accurate estimates of test error rate → because of bias-variance trade-off:\n\nLOOCV has less bias, because almost all observations are used in the training sets every time. k-fold CV in comparison more biased, because each training set exclude more observations.\nLOOCV has higher variance than k-fold CV. In LOOCV averaging the outputs of n fitted models, each of which is trained on an almost identical set of obesrvations → correlation between them is high. Have higher variance, so the test error estimate tends to have higher variance. k-Fold CV with k &lt; n less correlation,overlap bewtween training sets is smaller.\n\nTo summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#in-r",
    "href": "model-accuracy.html#in-r",
    "title": "3  Model accuracy and fit",
    "section": "3.6 in R",
    "text": "3.6 in R\nTraining, Validation, Test:\nlibrary(caret)\n# define the training partition \ntrain_index &lt;- createDataPartition(Boston$medv, p = .7, \n                       list = FALSE, \n                       times = 1)\n\n# split the data using the training partition to obtain training data\nboston_train &lt;- Boston[train_index,]\n\n# remainder of the split is the validation and test data (still) combined \nboston_val_and_test &lt;- Boston[-train_index,]\n\n# split the remaining 30% of the data in a validation and test set\nval_index &lt;- createDataPartition(boston_val_and_test$medv, p = .6, \n                       list = FALSE, \n                       times = 1)\n\nboston_valid &lt;- boston_val_and_test[val_index,]\nboston_test  &lt;- boston_val_and_test[-val_index,]\n\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~70%)\n# validation (~20%)\n# test (~10%)\n\n# Note that creating the partitions using the `y` argument (letting the function know what your dependent variable will be in the analysis), makes sure that when your dependent variable is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.\n\n\n\n#Then train the model 1 only with the train data\nmodel_1 &lt;- lm(medv ~ lstat, data = boston_train)\nsummary(model_1)\n\n\n# Train model 2 only with train data \nmodel_2 &lt;- lm(medv ~ lstat + age + tax, data = boston_train)\nsummary(model_2)\n\n#MSE for trained models only show, how well the model perform on the trained data set! For evaluating which model you should choose, validation data set\n\n  # MSE function for evaluation of accuracy \nmse &lt;- function(y_true, y_pred) {\n  mean((y_true - y_pred)^2)\n}\n\n  # Calculate the MSE for validation\nmodel_1_mse_valid &lt;- mse(y_true = boston_valid$medv, \n                 y_pred = predict(object = model_1, newdata = boston_valid))\nmodel_2_mse_valid &lt;- mse(y_true = boston_valid$medv, \n                 y_pred = predict(model_2, newdata = boston_valid))\n\n# Choose your model (in this case model 2) based on the lower value of validation MSE, because you want the better out-of-sample prediction\n\n# estimate accuracy of your selected model\n\n  #first: train your model again using this time train and validation data\nmodel_2b &lt;- lm(medv ~ lstat + age + tax, data = bind_rows(boston_train, boston_valid))\nsummary(model_2b)\n\n  #second: predict on the test data:\nmodel_2_mse_test &lt;- mse(y_true = boston_test$medv, \n                y_pred = predict(model_2b, newdata = boston_test))\n  #inspect the MSE \nmodel_2_mse_test\n\n  #compute the R(MSE)\nAnother quantity that we calculate is the Root Mean Squared Error (RMSE). It is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis.\nKey point: The RMSE is thus the distance, on average, of a data point from the fitted line, measured along a vertical line.\n# The estimate for the expected amount of error when predicting the median value of a not previously seen town in Boston when  using this model is:\n\nsqrt(model_2_mse_test)\ncross-validation\n# Just for reference, here is the mse() function once more\nmse &lt;- function(y_true, y_pred) mean((y_true - y_pred)^2)\n\ncv_lm &lt;- function(formula, dataset, k) {\n  # We can do some error checking before starting the function\n  stopifnot(is_formula(formula))     # formula must be a formula\n  stopifnot(is.data.frame(dataset))   # dataset must be data frame\n  stopifnot(is.integer(as.integer(k))) # k must be convertible to int\n  \n  # first, add a selection column to the dataset as before\n  n_samples  &lt;- nrow(dataset)\n  select_vec &lt;- rep(1:k, length.out = n_samples)\n  data_split &lt;- dataset %&gt;% mutate(folds = sample(select_vec))\n  \n  # initialise an output vector of k mse values, which we \n  # will fill by using a _for loop_ going over each fold\n  mses &lt;- rep(0, k)\n  \n  # start the for loop\n  for (i in 1:k) {\n   # split the data in train and validation set\n   data_train &lt;- data_split %&gt;% filter(folds != i)\n   data_valid &lt;- data_split %&gt;% filter(folds == i)\n   \n   # calculate the model on this data\n   model_i &lt;- lm(formula = formula, data = data_train)\n   \n   # Extract the y column name from the formula\n   y_column_name &lt;- as.character(formula)[2]\n   \n   # calculate the mean square error and assign it to mses\n   mses[i] &lt;- mse(y_true = data_valid[[y_column_name]],\n             y_pred = predict(model_i, newdata = data_valid))\n  }\n  \n  # now we have a vector of k mse values. All we need is to\n  # return the mean mse!\n  mean(mses)\n}\n\n# use the formula to perfom a cross-validation for the model\ncv_lm(formula = medv ~ lstat + age + tax, dataset = Boston, k = 9)\n  \n# the output is the test MSE \n\n3.6.1 Cross-Validation on Classification Problems\nLOOCV error rate and analogously to that the k-fold CV and validation set error rates:\n\\[\nCV_(n) = \\frac{1}{n} \\sum_{i_1}^{n} Err_i\n\\] \\[\nErr_i = I(y_i \\neq \\hat{y_i})\n\\]\nlogistic regression has not enough flexibility often, therefore an extension is needed → using a polynomial functions of the predictors, e. g. an quadratic logistic regression model with 2 degrees of freedom:\n\\[\nlog( \\frac{p}{1-p}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X1^2 + \\beta_3 X_2 + \\beta_4X_2^2\n\\]\n\nIn practice, for real data, the Bayes decision boundary and the test error rates are unknown → cross-validation\n\n→ the 10-fold CS error rate provides a pretty good approximation to the test error rate\n→ 10-fold CV error indicates the best value for K, training error rate declies as the method becomes more flexible, so cannot be used to select the optimal value for K.\n\n\n3.6.2 Practice\nIn this lab, you will learn how to plot a linear regression with confidence and prediction intervals, and various tools to assess model fit: calculating the MSE, making train-test splits, and writing a function for cross validation. You can download the student zip including all needed files for practical 3 here.\nWe will use the Boston dataset, which is in the MASS package that comes with R. In addition, we will make use of the caret package in Part 2 to divide the Boston dataset into a training, test, and validation set.\n\nlibrary(ISLR)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(scales)\n\n\nInspect the Boston dataset using the View() function\n\n\nview(Boston)\n?Boston\n\nThe Boston dataset contains the housing values and other information about Boston suburbs. We will use the dataset to predict housing value (the variable medv, here the outcome/dependent variable) by socio-economic status (the variable lstat, here the predictor / independent variable).\nmedv → Y - median value of owner occupied homes in $1000 steps - metric variable lstat → X - lower status of popultation - in percent, metric variable\nsample size → 506 rows\nLet’s explore socio-economic status and housing value in the dataset using visualization.\n\nCreate a scatter plot from the Boston dataset with lstat mapped to the x position and medv mapped to the y position. Store the plot in an object called p_scatter.\n\n\np_scatter &lt;- ggplot(Boston, aes(x=lstat, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter\n\n\n\n\n\n\n\n\n\np_scatter2 &lt;- ggplot(Boston, aes(x=lstat, y=medv*1000))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  scale_y_continuous(labels=scales::label_dollar())+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter2\n\n\n\n\n\n\n\n\n\n\n3.6.3 Plotting linear regression including a prediction line\nWe’ll start with making and visualizing the linear model. As you know, a linear model is fitted in R using the function lm(), which then returns a lm object. We are going to walk through the construction of a plot with a fit line. During the part done within the lab, we will add prediction and confidence intervals from an lm object to this plot.\nFirst, we will create the linear model. This model will be used to predict outcomes for the current data set, and - further along in this lab - to create new data.\n\nCreate a linear model object called lm_ses using the formula medv ~ lstat and the Boston dataset.\n\n\nlm_ses &lt;- lm(medv ~ lstat, data= Boston)\n\nYou have now trained a regression model with medv (housing value) as the outcome/dependent variable and lstat (socio-economic status) as the predictor / independent variable.\nRemember that a regression estimates \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope) in the following equation:\n\\[\\boldsymbol{y} = \\beta_0 + \\beta_1\\cdot \\boldsymbol{x}_1 + \\boldsymbol{\\epsilon}\\]\n\np_scatter3 &lt;- ggplot(Boston, aes(x=lstat, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  geom_smooth(method=lm, se=TRUE, color=\"slategray2\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nUse the function coef() to extract the intercept and slope from the lm_ses object. Interpret the slope coefficient.\n\n\ncoef(lm_ses)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\n\nThere is a negative correlation between the median value of owner occupied houses and the percentage of population with lower status.\nThe intercept is really high. So with no individuals with lower status lives near by the house, the housing price is 34 000$. If the percentage of people with lower status increases by one, the median value of the owner-occupied homes decreases bei 1000 $.\n\n\nUse summary() to get a summary of the lm_ses object. What do you see? You can use the help file ?summary.lm.\n\n\nsummary(lm_ses)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can now see the more information about the linear regression.\n\nthe weighted residuals, the usual residuals rescaled by the square root of the weights specified in the call to lm\ndegrees of freedom, a 3-vector (p, n-p, p*), the first being the number of non-aliased coefficients, the last being the total number of coefficients.\nthe coefficients a p x 4 matrix with columns for the estimated coefficient, its standard error, t-statistic and corresponding (two-sided) p-value. Aliased coefficients are omitted.\nf statistic (for models including non-intercept terms) a 3-vector with the value of the F-statistic with its numerator and denominator degrees of freedom.\nr.squared \\(R^2\\), the ‘fraction of variance explained by the model’,\n\n\\[\nR^2 = 1 - \\frac{ \\sum{R_i^2}}{ \\sum{(y_i - \\hat{y})^2}}\n\\] where $ is the mean of $y_i}f there is an intercept and zero otherwise.\nWe now have a model object lm_ses that represents the formula\n\\[\\text{medv}_i = 34.55 - 0.95 * \\text{lstat}_i + \\epsilon_i\\]\nWith this object, we can predict a new medv value by inputting its lstat value. The predict() method enables us to do this for the lstat values in the original dataset.\n\nSave the predicted y values to a variable called y_pred\n\n\ny_pred &lt;- predict(lm_ses, newdata=Boston)\nhead(y_pred)\n\n       1        2        3        4        5        6 \n29.82260 25.87039 30.72514 31.76070 29.49008 29.60408 \n\n\n\nCreate a scatter plot with y_pred mapped to the x position and the true y value (Boston$medv) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?\n\n\ndata &lt;- data.frame( medv  = Boston$medv, \n              y_pred = y_pred)\npred_scatter &lt;- ggplot(data, aes(x=y_pred, y=medv))+\n  geom_point(alpha=0.5)+\n  labs(title= \"Prediction of median value of house\", x= \"prediction\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\npred_scatter\n\n\n\n\n\n\n\n\nIf the prediction would be perfect, we had a linear regression line, because the real and the predicted value are not the same, although we have used the same data set.\n\n\n3.6.4 Plotting linear regression with confindence or prediction intervals\nWe will continue with the Boston dataset, the created model lm_ses that predicts medv (housing value) by lstat (socio-economic status), and the predicted housing values stored in y_pred.\nIn addition to predicting housing values for values of lstat observed in the Boston dataset, we also can generate predictions from new values using the newdat argument in the predict() method. For that, we need to prepare a data frame with new values for the original predictors.\nOne method of number generation, is through using the function seq(), this function from base R generates a sequence of number using a standardized method. Typically length of the requested sequence divided by the range between from to to. For more information call ?seq.\n\nUse the seq() function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with (data.frame() or tibble()) as its column name lstat. Name the data frame pred_dat.\n\n\npred_dat &lt;- tibble(lstat=seq(0,40, length.out=1000))\n\n\n\nUse the newly created data frame, from Question 8, as the newdata argument to a predict() call for lm_ses. Store it in a variable named y_pred_new.\n\n\n\ny_pred_new &lt;- predict(lm_ses, newdata=pred_dat)\n\nNow, we’ll continue with the plotting part by adding a prediction line to the plot we previously constructed.\n\n\nAdd the vector y_pred_new to the pred_dat data frame with the name medv.\n\n\n\npred_dat &lt;- pred_dat %&gt;% mutate(medv=y_pred_new)\n\n\nAdd a geom_line() to p_scatter from Question 2, with pred_dat as the data argument. What does this line represent?\n\n\np_scatter &lt;- ggplot()+\n  geom_point(data=Boston, mapping =aes(x=lstat, y=medv, alpha=0.5))+\n  geom_line(data= pred_dat,mapping= aes(x=lstat, y=medv))+\n   labs(title= \"Housing Values in Boston and the socio-economic status\", x= \"percentage of lower status\", y=\"median value of house\")+\n  theme_minimal()+\n  theme(plot.title=element_text(size=12))\np_scatter\n\n\n\n\n\n\n\n\nThis line represents predicted values of medv for the values of lstat.\n\nThe interval argument can be used to generate confidence or prediction intervals. Create a new object called y_pred_95 using predict() (again with the pred_dat data) with the interval argument set to “confidence”. What is in this object?\n\n\n?predict\ny_pred_95 &lt;- predict(lm_ses, newdata =  pred_dat, interval = \"confidence\")\n\n\nUsing the data from Question 11, and the sequence created in Question 8; create a data frame with 4 columns: medv, lstat, lower, and upper.\n\n\ndata &lt;- cbind(pred_dat, y_pred_95)\ndata &lt;- data %&gt;% select(lstat, medv, lwr, upr)\nhead(data)\n\n       lstat     medv      lwr      upr\n1 0.00000000 34.55384 33.44846 35.65922\n2 0.04004004 34.51580 33.41307 35.61853\n3 0.08008008 34.47776 33.37768 35.57784\n4 0.12012012 34.43972 33.34229 35.53715\n5 0.16016016 34.40168 33.30690 35.49646\n6 0.20020020 34.36364 33.27150 35.45578\n\n\n\nAdd a geom_ribbon() to the plot with the data frame you just made. The ribbon geom requires three aesthetics: x (lstat, already mapped), ymin (lower), and ymax (upper). Add the ribbon below the geom_line() and the geom_points() of before to make sure those remain visible. Give it a nice colour and clean up the plot, too!\n\n\nBoston %&gt;% \n  ggplot(aes(x = lstat, y = medv)) + \n  geom_ribbon(aes(ymin = lwr, ymax = upr), data = data, fill = \"#00008b44\") +\n  geom_point(colour = \"#883321\") + \n  geom_line(data = pred_dat, colour = \"#00008b\", size = 1) +\n  theme_minimal() + \n  labs(x   = \"Proportion of low SES households\",\n     y   = \"Median house value\",\n     title = \"Boston house prices\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nExplain in your own words what the ribbon represents.\n\n\n# The ribbon represents the 95% confidence interval of the fit line.\n# The uncertainty in the estimates of the coefficients are taken into\n# account with this ribbon. \n\n# You can think of it as:\n# upon repeated sampling of data from the same population, at least 95% of\n# the ribbons will contain the true fit line.\n\n\nDo the same thing, but now with the prediction interval instead of the confidence interval.\n\n\n# pred with pred interval\ny_pred_95 &lt;- predict(lm_ses, newdata = pred_dat, interval = \"prediction\")\n\n\n# create the df\ngg_pred &lt;- tibble(\n  lstat = pred_dat$lstat,\n  medv  = y_pred_95[, 1],\n  l95  = y_pred_95[, 2],\n  u95  = y_pred_95[, 3]\n)\n\n# Create the plot\nBoston %&gt;% \n  ggplot(aes(x = lstat, y = medv)) + \n  geom_ribbon(aes(ymin = l95, ymax = u95), data = gg_pred, fill = \"#00008b44\") +\n  geom_point(colour = \"#883321\") + \n  geom_line(data = pred_dat, colour = \"#00008b\", size = 1) +\n  theme_minimal() + \n  labs(x    = \"Proportion of low SES households\",\n     y    = \"Median house value\",\n     title = \"Boston house prices\")\n\n\n\n\n\n\n\n\nWhile the confidence interval indiciates the uncertainty surrounding the average of y over the sample, the prediction interval quantify the uncertainty for a particular observation. A prediction interval is a type of confidence interval (CI) used with predictions in regression analysis; it is a range of values that predicts the value of a new observation, based on your existing model. Similarly, the prediction interval tells you where a value will fall in the future, given enough samples, a certain percentage of the time. A 95% prediction interval of 100 to 110 hours for the mean life of a battery tells you that future batteries produced will fall into that range 95% of the time. There is a 5% chance that a battery will not fall into this interval. the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty\n\n\n3.6.5 Model fit using the mean square error\nNext, we will write a function to assess the model fit using the mean square error: the square of how much our predictions on average differ from the observed values.\n\nWrite a function called mse() that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.\n\nStart like so:\n\nmse &lt;- function(y_true, y_pred) {\n  mean((y_true - y_pred)^2)\n}\n\nWikipedia may help for the formula.\n\nMake sure your mse() function works correctly by running the following code.\n\nmse(1:10, 10:1)\nIn the code, we state that our observed values correspond to \\(1, 2, ..., 9, 10\\), while our predicted values correspond to \\(10, 9, ..., 2, 1\\). This is graphed below, where the blue dots correspond to the observed values, and the yellow dots correspond to the predicted values. Using your function, you have now calculated the mean squared length of the dashed lines depicted in the graph below. If your function works correctly, the value returned should equal 33.\nVisualiation of this:\n\n\n\n\n\n\n\n\n\n\nCalculate the mean square error of the lm_ses model. Use the medv column as y_true and use the predict() method to generate y_pred.\n\n\nmse(Boston$medv, predict(lm_ses))\n\n[1] 38.48297\n\n\nIt is not the same values, it is squared, you must use the squared root to get the right unit and not the squared one.\n\nsqrt(mse(Boston$medv, predict(lm_ses)))\n\n[1] 6.203464\n\n\nYou have calculated the mean squared length of the dashed lines in the plot below. As the MSE is computed using the data that was used to fit the model, we actually obtained the training MSE. Below we continue with splitting our data in a training, test and validation set such that we can calculate the out-of sample prediction error during model building using the validation set, and estimate the true out-of-sample MSE using the test set.\n\n\n\n\n\n\n\n\n\nNote that you can also easily obtain how much the predictions on average differ from the observed values in the original scale of the outcome variable. To obtain this, you take the root of the mean square error. This is called the Root Mean Square Error, abbreviated as RMSE.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#obtaining-train-validation-test-splits",
    "href": "model-accuracy.html#obtaining-train-validation-test-splits",
    "title": "3  Model accuracy and fit",
    "section": "3.7 Obtaining train-validation-test splits",
    "text": "3.7 Obtaining train-validation-test splits\nNext, we will use the caret package and the function createDataPartition() to obtain a training, test, and validation set from the Boston dataset. For more information on this package, see the caret website. The training set will be used to fit our model, the validation set will be used to calculate the out-of sample prediction error during model building, and the test set will be used to estimate the true out-of-sample MSE.\n\nUse the code given below to obtain training, test, and validation set from the Boston dataset.\n\n\nlibrary(caret)\n# define the training partition \ntrain_index &lt;- createDataPartition(Boston$medv, p = .7, \n                       list = FALSE, \n                       times = 1)\n\n# split the data using the training partition to obtain training data\nboston_train &lt;- Boston[train_index,]\n\n# remainder of the split is the validation and test data (still) combined \nboston_val_and_test &lt;- Boston[-train_index,]\n\n# split the remaining 30% of the data in a validation and test set\nval_index &lt;- createDataPartition(boston_val_and_test$medv, p = .6, \n                       list = FALSE, \n                       times = 1)\n\nboston_valid &lt;- boston_val_and_test[val_index,]\nboston_test  &lt;- boston_val_and_test[-val_index,]\n\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~70%)\n# validation (~20%)\n# test (~10%)\n\nNote that creating the partitions using the y argument (letting the function know what your dependent variable will be in the analysis), makes sure that when your dependent variable is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.\nWe will set aside the boston_test dataset for now.\n\nTrain a linear regression model called model_1 using the training dataset. Use the formula medv ~ lstat like in the first lm() exercise. Use summary() to check that this object is as you expect.\n\n\nmodel_1 &lt;- lm(medv ~ lstat, data = boston_train)\nsummary(model_1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = boston_train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.840 -3.583 -1.197  2.238 24.699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.01137    0.62224   54.66   &lt;2e-16 ***\nlstat       -0.91398    0.04222  -21.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.633 on 354 degrees of freedom\nMultiple R-squared:  0.5697,    Adjusted R-squared:  0.5685 \nF-statistic: 468.6 on 1 and 354 DF,  p-value: &lt; 2.2e-16\n\n\n\nCalculate the MSE with this object. Save this value as model_1_mse_train.\n\nSys.setenv(LANG = “en”)\n\nmodel_1_mse_train &lt;- mse(boston_train$medv, predict(model_1, boston_train))\nmodel_1_mse_train\n\n[1] 31.55615\n\n\n\nNow calculate the MSE on the validation set and assign it to variable model_1_mse_valid. Hint: use the newdata argument in predict().\n\n\nmodel_1_mse_valid &lt;- mse(boston_valid$medv, predict(model_1, boston_valid))\nmodel_1_mse_valid\n\n[1] 57.79488\n\n\nThis is the estimated out-of-sample mean squared error.\n\nCreate a second model model_2 for the train data which includes age and tax as predictors. Calculate the train and validation MSE.\n\n\nmodel_2 &lt;- lm(medv ~ lstat + age+ tax, data= boston_train)\nmodel_2_mse_train &lt;- mse(y_true= boston_train$medv, y_pred= predict(model_2, boston_train))\nmodel_2_mse_train\n\n[1] 30.70336\n\n\n\nmodel_2_mse_valid &lt;- mse(y_true =boston_valid$medv, y_pred=predict(model_2, boston_valid))\nmodel_2_mse_valid\n\n[1] 54.90731\n\n\nIf you are interested in out-of-sample prediction, the answer may depend on the random sampling of the rows in the dataset splitting: everyone has a different split. However, it is likely that model_2 has both lower training and validation MSE. In choosing the best model, you should base your answer on the validation MSE. Using the out of sample mean square error, we have made a model decision (which parameters to include, only lstat, or using age and tax in addition to lstat to predict housing value). Now we have selected a final model.\n\nCompare model 1 and model 2 in terms of their training and validation MSE. Which would you choose and why?\n\nModel 1 is better, model 2 is overfitting data. Model 1 is better in the validation set and so less biased to the sample split.\nI would choose model\n\nmodel_2b &lt;- lm(medv ~ lstat + age + tax, data = bind_rows(boston_train, boston_valid))\nsummary(model_2b)\n\n\nCall:\nlm(formula = medv ~ lstat + age + tax, data = bind_rows(boston_train, \n    boston_valid))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.709  -3.744  -1.221   1.848  24.361 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.013271   0.848376  40.092  &lt; 2e-16 ***\nlstat       -0.982788   0.053724 -18.293  &lt; 2e-16 ***\nage          0.048576   0.013544   3.587 0.000372 ***\ntax         -0.005804   0.002077  -2.795 0.005422 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.991 on 444 degrees of freedom\nMultiple R-squared:  0.5666,    Adjusted R-squared:  0.5637 \nF-statistic: 193.5 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\n\nIn choosing the best model, you should base your answer on the validation MSE. Using the out of sample mean square error, we have made a model decision (which parameters to include, only lstat, or using age and tax in addition to lstat to predict housing value). Now we have selected a final model.\n\nFor your final model, retrain the model one more time using both the training and the validation set. Then, calculate the test MSE based on the (retrained) final model. What does this number tell you?\n\n\nmodel_2_mse_test &lt;- mse(y_true = boston_test$medv, \n                y_pred = predict(model_2b, newdata = boston_test))\nmodel_2_mse_test\n\n[1] 47.7833\n\n# The estimate for the expected amount of error when predicting \n# the median value of a not previously seen town in Boston when \n# using this model is:\n\nsqrt(model_2_mse_test)\n\n[1] 6.912547\n\n\nAs you will see during the remainder of the course, usually we set apart the test set at the beginning and on the remaining data perform the train-validation split multiple times. Performing the train-validation split multiple times is what we for example do in cross validation (see below). The validation sets are used for making model decisions, such as selecting predictors or tuning model parameters, so building the model. As the validation set is used to base model decisions on, we can not use this set to obtain a true out-of-sample MSE. That’s where the test set comes in, it can be used to obtain the MSE of the final model that we choose when all model decisions have been made. As all model decisions have been made, we can use all data except for the test set to retrain our model one last time using as much data as possible to estimate the parameters for the final model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#optional-cross-validation-advanced",
    "href": "model-accuracy.html#optional-cross-validation-advanced",
    "title": "3  Model accuracy and fit",
    "section": "3.8 Optional: cross-validation (advanced)",
    "text": "3.8 Optional: cross-validation (advanced)\nThis is an advanced exercise. Some components we have seen before in this lab, but some things will be completely new. Try to complete it by yourself, but don’t worry if you get stuck. If you don’t know about for loops in R, read up on those before you start the exercise, for example by reading the Basics: For Loops tab on the course website.\nUse help in this order:\n\nR help files\nInternet search & stack exchange\nYour peers\nThe answer, which shows one solution\n\nYou may also just read the answer when they have been made available and try to understand what happens in each step.\n\nCreate a function that performs k-fold cross-validation for linear models.\n\nInputs:\n\nformula: a formula just as in the lm() function\ndataset: a data frame\nk: the number of folds for cross validation\nany other arguments you need necessary\n\nOutputs:\n\nMean square error averaged over folds\n\n\n# Just for reference, here is the mse() function once more\nmse &lt;- function(y_true, y_pred) mean((y_true - y_pred)^2)\n\ncv_lm &lt;- function(formula, dataset, k) {\n  # We can do some error checking before starting the function\n  stopifnot(is_formula(formula))     # formula must be a formula\n  stopifnot(is.data.frame(dataset))   # dataset must be data frame\n  stopifnot(is.integer(as.integer(k))) # k must be convertible to int\n  \n  # first, add a selection column to the dataset as before\n  n_samples  &lt;- nrow(dataset)\n  select_vec &lt;- rep(1:k, length.out = n_samples)\n  data_split &lt;- dataset %&gt;% mutate(folds = sample(select_vec))\n  \n  # initialise an output vector of k mse values, which we \n  # will fill by using a _for loop_ going over each fold\n  mses &lt;- rep(0, k)\n  \n  # start the for loop\n  for (i in 1:k) {\n   # split the data in train and validation set\n   data_train &lt;- data_split %&gt;% filter(folds != i)\n   data_valid &lt;- data_split %&gt;% filter(folds == i)\n   \n   # calculate the model on this data\n   model_i &lt;- lm(formula = formula, data = data_train)\n   \n   # Extract the y column name from the formula\n   y_column_name &lt;- as.character(formula)[2]\n   \n   # calculate the mean square error and assign it to mses\n   mses[i] &lt;- mse(y_true = data_valid[[y_column_name]],\n             y_pred = predict(model_i, newdata = data_valid))\n  }\n   \n  # now we have a vector of k mse values. All we need is to\n  # return the mean mse!\n  mean(mses)\n}\n\n\nUse your function to perform 9-fold cross validation with a linear model with as its formula medv ~ lstat + age + tax. Compare it to a model with as formula medv ~ lstat + I(lstat^2) + age + tax.\n\n\ncv_lm(formula = medv ~ lstat + age + tax, dataset = Boston, k = 9)\n\n[1] 37.69752\n\n\n\ncv_lm(formula = medv ~ lstat + I(lstat^2) + age + tax, dataset = Boston, k = 9)\n\n[1] 28.24359",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "model-accuracy.html#conclusions",
    "href": "model-accuracy.html#conclusions",
    "title": "3  Model accuracy and fit",
    "section": "3.9 Conclusions",
    "text": "3.9 Conclusions\nWe want to learn a model of our data\n\nWe don’t want to underfit (will have high MSE –&gt; Because of high bias)\nWe don’t want to overfit (will have high MSE –&gt; Because of high variance) More complex models tend to have higher variance and lower bias\n\nWe need to choose the correct complexity estimating E(MSE) in the validation dataset\n\nCompare between models, tune the hyperparameters of the model or select features\nOr even better, cross-validation\n\nWe can estimate E(MSE) using the test dataset\n\nAllows us to understand how the results of our model generalize to unseen data\nDone only for the best model (sometimes best k models)\nGetting good test data is difficult, unsolved problem Keep in mind that complex models are often less interpretable",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model accuracy and fit</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "4  Linear Regression",
    "section": "",
    "text": "4.1 Readings\nISLR:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#readings",
    "href": "linear-regression.html#readings",
    "title": "4  Linear Regression",
    "section": "",
    "text": "Chapter 3.1 - 3.4: Linear Regression (optional)\nChapter 6.1 through 6.2: Linear Model Selection and Regularization: Subset Selection & Shrinkage Methods",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#simple-linear-regression",
    "href": "linear-regression.html#simple-linear-regression",
    "title": "4  Linear Regression",
    "section": "4.2 Simple linear regression",
    "text": "4.2 Simple linear regression\nPredicting a quantitative response Y on the basis on a single predictor variable X. We want to estimate the predicted variable of the basis of X, computing the parameter \\(\\beta\\).\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta_1}x + \\epsilon\n\\]\nFor estimating the parameters \\(\\hat{\\beta_0}\\) (intercept) and \\(\\hat{\\beta_1}\\) (parameter for x) most used method is least squares criterion. Residuals are computed for each ith observed response value minus the ith predicted value by the linear model for sample of size n.\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\nThese residuals are summed up for all i observations to the residual sum of squares (RSS):\n\\[\nRSS = e_1^2 + e_2^2 + \\dots + e_n^2\n\\]\nSo in sum the formula for RSS: \\[\nRSS = \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n\\]\nThe least squares approach choose the estimates (\\(\\beta\\)) to minimize the RSS:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sum_{i=1}^{n} (x_i - \\overline{x})}\n\\] \\[\n\\hat{\\beta_0}= \\overline{y} - \\hat{\\beta_1}\\overline{x}\n\\] Where \\(\\overline{x}\\) and \\(\\overline{y}\\) re the sample means.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#multiple-linear-regressions",
    "href": "linear-regression.html#multiple-linear-regressions",
    "title": "4  Linear Regression",
    "section": "4.3 Multiple linear regressions",
    "text": "4.3 Multiple linear regressions\nextended formula of simple regression:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\dots +\\hat{\\beta_p}x_p + \\epsilon\n\\]\nwhere Xj represents the jth predictor and βj quantifies the association between that variable and the response. We interpret βj as the average effect on Y of a one unit increase in Xj , holding all other predictors fixed.\nSame least squared method for estimating the parameters are used.\nInterpreted like this: When we have increase on the x axis by one, there will be an increase on the y axis by coefficent of x1 holding the other variables constant.\nImportant questions:\n\nIs at least one of the predictors X1,X2, . . . ,Xp useful in predicting the response?\nDo all the predictors help to explain Y , or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#additive-assumption",
    "href": "linear-regression.html#additive-assumption",
    "title": "4  Linear Regression",
    "section": "4.4 Additive Assumption",
    "text": "4.4 Additive Assumption\nTwo of the most important assumptions state that the relationship between the predictors and response are additive and linear. The additivity assumption means that the association between a predictor Xj and the response Y does not depend on the values of the other predictors. The linearity assumption states that the change in the response Y associated with a one-unit change in Xj is constant, regardless of the value of Xj . Additive assumption refers to an interaction effect between the variables. One way of extending this model is to include a third predictor, called an interaction term, which is constructed by computing the product of X1 and X2:\n\\[\nY=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 + \\epsilon\n\\]\nThe hierarchical principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant. In other words, if the interaction between X1 and X2 seems important, then we should include both X1 and X2 in the model even if their coefficient estimates have large p-values",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#polynomial-regression-for-non-linear-relationships",
    "href": "linear-regression.html#polynomial-regression-for-non-linear-relationships",
    "title": "4  Linear Regression",
    "section": "4.5 polynomial regression for non-linear relationships",
    "text": "4.5 polynomial regression for non-linear relationships\nsimple approach for incorporating non-linear associations in a linear model is to include transformed versions of the predictors. Example is a quadric shape:\n\\[\nY = \\beta_0 + \\beta_1x_1 + \\beta_2x_2^2 + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#accurary-of-linear-regressions",
    "href": "linear-regression.html#accurary-of-linear-regressions",
    "title": "4  Linear Regression",
    "section": "4.6 Accurary of linear regressions",
    "text": "4.6 Accurary of linear regressions\nHow we can verify, that our parameters fits to the real one? Because samples are always biased, how can we be sure, that our function is accurate?\n\nTherefore, we compute the standard error, which is the standard deviation of the sample distribution.\ntells us the average amount that an estimate differs from the actual value\nshrinks, the bigger n is\nThe bigger the SE, the bigger the insecurity about the parameters\n\nStandard deviation is squared root of variance\n\\[\n\\sigma = \\sqrt{\\frac{1}{N} \\sum_{1}^{n} (x_i - \\overline{x}_i)^2}\n\\]\nVariance: \\[\nVar(X) = \\sigma^2\n\\]\n\\[\nVar(X) = \\frac{1}{N} \\sum_{1}^{n} (x_i - \\overline{x}_i)^2\n\\] For estimating standard errors of the parameters to check how near they are to the true values: \\[\nSE(\\hat{\\beta_0}^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\overline{x}^2} {\\sum_{i=1}^{n} (x_i - \\overline{x})^2}]\n\\] \\[\nSE(\\hat{\\beta_1}^2 = \\frac{\\sigma^2} {\\sum_{i=1}^{n} (x_i - \\overline{x})^2}\n\\] In general \\(\\sigma^2\\) is not known, therefore, we compute the residual standard error: \\[\nRSE = \\sqrt{\\frac{RSS}{n-2}}\n\\]\nOur goal is to find parameters that minimize the MSE, in doing this: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y - \\hat{y})^2\n\\] \n\n4.6.1 cost function for multiple and linear regressions\nHow to find the optimal θ (the ones that minimize MSE)?\n\nOption 1: Using algebra\nOption 2: Using gradient descent –&gt; optimization technique used in many machine learning method\n\nWe are computing the MSE (see above) with different parameter settings. If the MSE (or the local parameter is \\(L_\\theta\\)) is 0, the model fit perfect the data. For each value of the parameter \\(\\beta\\) or in this case called \\(\\theta\\) we get another MSE. As a result we have a curve for simple regression that looks like this:\n\nThe curve is bell-shaped. In the following, we can see how the bell-shaped curve looks like in a graph and how the gradient of the regression (\\(h_{\\theta}(x)\\)) and the function of the parameters are connected with each other:\n\n\n\nStandard errors useful for computing confidence intervals. 95 % confidence interval means,that within this interval is a 95 % probability, that this range will contain the true unknown value of the parameter.\nStandard errors for hypothesis tests with null hypothesis: \\(H_0: \\beta_1 = 0\\) → model reduced to \\(Y = \\beta_0 + \\epsilon\\) → computing a t-statistic:\n\\[\nt= \\frac{\\hat{\\beta_1} - 0} { SE(\\hat{\\beta_1})}\n\\]\nmeasures the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0.No relationship, t- distribution with n-2 degrees of freedom. The t-distribution has a bell shape and for values of n greater than approximately 30 it is quite similar to the standard normal distribution.Consequently, it is a simple matter to compute the probability of observing any number equal to |t| or larger in absolute value, assuming β1 = 0. We call this probability the p-value. Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response.\nIf the standard deviation is known, we use z statistic insteat of t-statistic.\n\n\n4.6.2 3 more ways accessing accuracy of the model:\n\nResidual Standard Error (RSE) an absoulte measure of the lack of fit of the model, so the number has to be seen in context of the values on Y (computed seen above)\n\\(R^2\\) an alternative measure, takes the form of the proportion of variance explained and and so it always takes on a value between 0 and 1, and is independent of the scale of Y . Computed like this:\n\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1- \\frac{RSS}{TSS}\n\\]\nTotal sum of squares: \\[\nTSS = \\sum (y_i- \\overline{y_i})^2\n\\]\nTSS measures the total variance in the response Y , and can be squares thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS −RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and R2 measures the proportion of variability in Y that can be explained using X.\nInterpretation more easily, because values always lay between 0 and 1.\n\nF-Statistic only for multiple regression, null-hypothesis testing is not possible, we need to ask whether all of the regression coefficients are zero.\n\nTherefore, F-statistic: \\[\nF= \\frac{(TSS-RSS)/p)}{RSS/(n-p-1)}\n\\] oberhalb des Bruchs: \\(H_0\\) is true unterhalb des Bruchs: linear model assumptions would be correct. Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1. On the other hand, if Ha is true, then E{(TSS − RSS)/p} &gt; σ2, so we expect F to be greater than 1. How large does the F-statistic need to be before we can reject H0 and conclude that there is a relationship? It turns out that the answer depends on the values of n and p. When n is large, an F-statistic that is just a little larger than 1 might still provide evidence against H0. In contrast, a larger F-statistic is needed to reject H0 if n is small. When H0 is true and the errors ϵi have a normal distribution, the F-statistic follows an F-distribution.6 For any given value of n and p, any statistical software package can be used to compute the p-value associated with the F-statistic using this distribution. Based on this p-value, we can determine whether or not to reject H0. The approach of using an F-statistic to test for any association between the predictors and the response works when p is relatively small, and certainly small compared to n. However, sometimes we have a very large number of variables. If p &gt; n then there are more coefficients βj to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the Fstatistic cannot be used, and neither can most of the other concepts that we have seen so far in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#potential-problems-of-linear-regression-models",
    "href": "linear-regression.html#potential-problems-of-linear-regression-models",
    "title": "4  Linear Regression",
    "section": "4.7 Potential Problems of linear regression models",
    "text": "4.7 Potential Problems of linear regression models\n\nNon-linearity of the response-predictor relationships.\nCorrelation of error terms. An important assumption of the linear regression model is that the error terms, ϵ1, ϵ2, . . . , ϵn, are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that ϵi is positive provides little or no information about the sign of ϵi+1. The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be.\nNon-constant variance of error terms. Another important assumption of the linear regression model is that the error terms have a constant variance, Var(ϵi) = σ2. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in heterothe residual plot.\nOutliers. Residual plots can be used to identify outliers. In this example, the outlier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual ei by its estimated standard studentized error. Observations whose studentized residuals are greater than 3 in abso- residual lute value are possible outliers.\nHigh-leverage points. We just saw that outliers are observations for which the response yi is unusual given the predictor xi. In contrast, observations with high leverage high have an unusual value for. In order to quantify an observation’s leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage.\nCollinearity. Collinearity refers to the situation in which two or more predictor variables are closely related to one another.To avoid such a situation, it is desirable to identify and address potentialcollinearity problems while fitting the model. A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multi- collinearity is to compute the variance inflation factor (VIF).The smallest possible value for VIF is 1,which indicates the complete absence of collinearity. Typically in practicethere is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#why-we-need-test-data",
    "href": "linear-regression.html#why-we-need-test-data",
    "title": "4  Linear Regression",
    "section": "4.8 Why we need test data?",
    "text": "4.8 Why we need test data?\nWhy is the MSE from the validation data set so high?\n\nWe have so many models,like in this case over 50,000 models, we are overfitting the validation data.\nWe cannot use the same data set for compare the models and test one model\ncross-validation does not have this problem",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#which-variables-shall-i-include-in-my-model",
    "href": "linear-regression.html#which-variables-shall-i-include-in-my-model",
    "title": "4  Linear Regression",
    "section": "4.9 Which variables shall I include in my model?",
    "text": "4.9 Which variables shall I include in my model?\nSelect only certain variables helps not overfitting the data and the influence of the variables is more easy to interprete.\nA very flexible model (one with many coefficients) is like a kid in candyshop with a platinum credit card: It goes around buying all the coefficients it wants and never stops.\nIdea: Tell the model not to go overboard with the complexity. We set up the correct complexity as the one that minimizes MSE in the validation data.\n\nPrediction Accuracy:\n\nProvided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If n &gt; p that is, if n, the number of observations, is much larger than p, the number of variables then the least squares estimates tend to also have low variance, and hence will perform well on test observations. However, if n is not much larger than p, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. And if p &gt; n, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all. By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.\n\nModel interpretability: It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables that is, by setting the corresponding coefficient estimates to zero we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing feature selection or variable selection that is, for excluding irrelevant variables from a multiple regression model.\n\nApproaches, that have better prediction accuracy and model interpretability than least squares method:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#subset-selection",
    "href": "linear-regression.html#subset-selection",
    "title": "4  Linear Regression",
    "section": "4.10 Subset Selection",
    "text": "4.10 Subset Selection\n\nidentifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables\n\nRestrict the number of predictors (kid is allowed only to choose 3 candies)\nPick the best p predictors\nHow, with the MSE training data set, not the validation data set, because: we do not waste our validation set, because we need this to evaluate the best model in the next step, training data set has the same complexity\nHow to do constrain the complexity of the model? There are different methods:\nBest subset selection\n\nfit a separate least squares regression for each possible combination of the p predictors\nwhich model is the best? The one with the smallest RSS or largest \\(R^2\\).\n\n - starts with a null model for all predictors and then compares all models with each other - in the next step the best model is chosen - then the model need to be validated with crossvalidation or the validation data set - In the case of logistic regression, instead of ordering models by RSS in Step 2 we instead use the deviance, a measure deviance that plays the role of RSS for a broader class of models. The deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit - not handy for very large p because if p = 10, we would have 1000 possible models and for p =200 over a million\nThat is because often this is done:\nForward stepwise selection\n\nbest alternative to best subset selection\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nat each step the variable that gives the greatest additional improvement to the fit is added to the model is added\nif p= 20 only 211 models have to be computed\n\n\n\nfirst step, the null model\nsecond: consider all models at the moment in comparison to a model where an additional predictor is added\nchoose the best model among these models\nselect the best model and using cross validation or the validation data set\nThough forward stepwise tends to do well in practice, it is not guaranteed to find the best possible model out of all 2p models containing subsets of the p predictors. For instance, suppose that in a given data set with p = 3 predictors, the best possible one-variable model contains X1, and the best possible two-variable model instead contains X2 and X3. Then forward stepwise selection will fail to select the best possible two-variable model, because M1 will contain X1, so M2 must also contain X1 together with one additional variable\n\nBackward stepwise selection\n\nsimilar to forward selection, only the other way around\nn needs to be larger than p, so a full model can be fitted.\n\n\nAdvantages and Disadvantages\n\nall models are compared with each other\nnot feasible, lot of computations\nforward selection (start with an empty model and add one by one)\nfaster\nnot sure, that you find the best model, because not all models are compared and only the one is chosen, which was better than the one before, so the interactions between possible predictors are not integrated → stack local minima\nbackward selection (full model and remove one by one)\nfaster\nstack local minima\nif you have more predictors than observations we cannot start with the full model, so if p &gt; n we cannot use it\n\nBackward and forward is not that good in finding the truth than the best subset solution → it is especially a problem if you want to infer, because in this case truth is more important than in predictions\nNow we have for all the models computed the MSE in one of three ways we use the validation set to identify, which model is outside this comparisons the best. So not biased by the comparisons, we compute the MSE with the validation set again.\nFor each selection same procedure:\nFor each level of complexity (number of predictors): Fit x models of equal complexity –&gt; Keep the best using e.g. MSE or R2 Estimate E(MSE) for models of different complexity using cross-validation –&gt; Select best model Estimate E(MSE) of the best model using test data\nCp, AIC, BIC, and Adjusted R2\nIf we do not have data for validation, - that would be the preferable way- we can estimate tht test error my making an adjustment to the training error to account for the bias due to overfitting.\nCp as unbiased estimate of the test MSE\n\\[\nC_p = \\frac{1}{n}(RSS + 2d\\sigma^2)\n\\]\n\\(\\sigma^2\\) is a estimate of the variance of the error \\(\\epsilon\\). Typically ˆσ2 is estimated using the full model containing all predictors. As a consequence, the Cp statistic tends to take on a small value for models with a low test error, so when determining which of a set of models is best, we choose the model with the lowest Cp value.\nAIC Akaike information criterion for a large class of models fit by maximum likelihood\n\\[\nAIC = \\frac{1}{n}(RSS + 2d\\sigma^2)\n\\] Hence for least squares models, Cp and AIC are proportional to each other.\nBIC Bayesian information criterion (BIC) \\[\nBIC = \\frac{1}{n}(RSS + log(n)d\\sigma^2)\n\\] from Bayesian point of view. Like Cp, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value. BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than C\nAdjusted R \\[\nAdbjusted R^2 = = 1- \\frac{RSS/(n-d-1)}{TSS/(n-d-1)}\n\\] Since RSS always decreases as more variables are added to the model, the R2 always increases as more variables are added.a large value of adjusted R2 indicates a model with a small test error. The intuition behind the adjusted R2 is that once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS.\nwhy validation and crossvalidation is better:\nThis procedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\).\nFor more precision, we can use the one-standard-error rule. We first calculate standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model that is, the model with the smallest number of predictors.\n\n4.10.1 in R\nBest Subset\n#generate all formuals for a data set and the predicted outcome\n # Input : \n # p   : number of variables \n # x_vars : character vector of x vars\n # y_var : character of y var\n\ngenerate_formulas &lt;- function(p, x_vars, y_var) {\n # Input checking\n if (p %% 1 != 0)      stop(\"Input an integer n\")\n if (p &gt; length(x_vars))  stop(\"p should be smaller than number of vars\")\n if (!is.character(x_vars)) stop(\"x_vars should be a character vector\")\n if (!is.character(y_var)) stop(\"y_vars should be character type\")\n \n # combn generates all combinations, apply turns them into formula strings\n apply(combn(x_vars, p), 2, function(vars) {\n  paste0(y_var, \" ~ \", paste(vars, collapse = \" + \"))\n })\n}\n\n #all formulas with e.g. 1 and 2 predictors\nformulas_1 &lt;- generate_formulas(p = 1, x_vars = x_vars, y_var = \"Salary\")\nformulas_2 &lt;- generate_formulas(p = 2, x_vars = x_vars, y_var = \"Salary\")\n\n# Initialise a vector we will fill with MSE values\nmses_1 &lt;- rep(0, length(formulas_1))\nmses_2 &lt;- rep(0, length(formulas_2))\n\n# loop over all the formulas\nfor (i in 1:length(formulas_1)) {\n mses_1[i] &lt;- lm_mse(as.formula(formulas_1[i]), baseball_train, baseball_valid)\n}\n\nfor (i in 1:length(formulas_2)) {\n mses_2[i] &lt;- lm_mse(as.formula(formulas_2[i]), baseball_train, baseball_valid)\n}\n\n\n# Compare mses, output is the MSE\nmin(mses_1)\nmin(mses_2)\n\n# extract the best formula out of it, in this case, the model with 2 p wins, output is the forumla\n\nformulas_2[which.min(mses_2)]\n\n# estimate then the model and calculate the mse\nlm_best &lt;- lm(Salary ~ Walks + CAtBat, baseball_train)\nmse &lt;- function(y_true, y_pred) mean((y_true - y_pred)^2)\nmse(baseball_test$Salary, predict(lm_best, newdata = baseball_test))\nBackward\n# start with a full model\nfull_model &lt;- lm(MntWines ~ ., data = train_data)\n# step backward\nstep(full_model, direction = \"backward\")\nForward\nlibrary(leaps)\nregfit_fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = \"forward\")\nsummary(regfit_fwd)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#shrinkage-regularization",
    "href": "linear-regression.html#shrinkage-regularization",
    "title": "4  Linear Regression",
    "section": "4.11 Shrinkage, Regularization",
    "text": "4.11 Shrinkage, Regularization\nThis approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.\nRestrict the number, which model is accurate (the kid is given only 3 dollar) Constrain the sum of squared cofficients (L2) or absolute sum of coefficients (L1) to be below s How: Adapt the loss function (e.g. MSE) to penalize including variables\nWe want to fit the training data (estimate the weights of the coefficients) Make the model behave ‘regularly’ by penalizing the purchase of ‘too many’ coefficients Extremely efficient way to approximately solve the best subset problem: Variable selection + regression in one step Often yields very good results If you are interested in prediction and not inference (i.e. if identifying the relevant features is not a primary goal of the analysis), regularization will usually be better\n\nShrinking penalty means, when the parameters/ coefficients are close to zero, it is shrinking them down to zero.\n\n4.11.1 Ridge regression\nlimits the size of the coefficients by adding an L1 penalty equal to the absolute value of the magnitude of coefficients, none of the coefficients are set to zero\nRidge regression’s advantage over least squares is rooted in the bias-variance trade-off. As \\(\\lambda\\) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias.\n\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates the \\(\\hat{\\beta}^R\\) are the values that minimize where \\(\\lambda \\ge 0\\) is a tuning parameter, to be determined separately. The tuning parameter λ serves to control the relative impact of these two terms on the regression coefficient estimates. When λ = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as λ→∞, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\nIf we increase lambda, we have a higher penality and in conclusion, we have smaller coefficients and a simpler model and in conclusion have less variance and more bias.Hence, ridge regression works best in situations where the least squares estimates have high variance. Ridge regression also has substantial computational advantages over best subset selection, which requires searching through 2p models. As we discussed previously, even for moderate values of p, such a search can be computationally unfeasible. In contrast, for any fixed value of λ, ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly. In fact, one can show that the computations required to solve, simultaneously for all values of λ, are almost identical to those for fitting a model using least squares\nPenalization as shrinkage to zero:\n\nIf λ = 0 we have the least squared fit, if λ is sufficiently large, we have a null model, where each parameter is near to 0 (Ridge) or set to (Lasso) zero.\nHere we can see the connection of increasing lambda and how the coefficients, in this case Income, Limit, Rating and Student behave, if lambda increases: The left hand panel shows the connection between an increasing lambda and the value of of the standardized coefficient. If λ = 0 we have the least squared and the coefficients are very high. If λ increases, the ridge coefficient estimate shrinks to zero.\n\nOn the right hand panel the connection between the standardized coefficient is shown in relation to another value: It is a measure of the estimated parameter with least squares in connection with the penalty. The x-axis ranges from 1 (where λ = 0) to 0 () \\(\\lambda = \\infty\\). The x-axis shows the amount that the ridge regression coefficent estimates have been shrunken towards zero → a small value indicates that they have been shrunken very close to zero,\n\n\n4.11.2 How to select λ\nBoth methods need the tuning parameter λ or the value of constraint s.\n\nOption\n\n\nDivide the data into train/val/test\nCreate models using different λ, fit them using the train data.\nEstimate E(MSE) in the validation data and select the best model.\nEstimate prediction error for the best model in the test datast.\n\n\nOption (better):\n\n\nDivide the data into train/test\nUse cross-validation, for each k split of train –&gt; train/val:\nCreate models using different λ.\nEstimate E(MSE) in the validation dataset\nSelect the model with the minimum average MSE.\nEstimate prediction error in the test data\n\n\n\n4.11.3 Lasso regression\ntry to minimize the sum of the coefficients and the MSE, R^2, a lot of coefficients are removed → huge advantage, because ridge regression include all p predictors in the final model. Although the penalty \\(\\lambda * \\sum_{j&gt;0} \\theta_i^2\\) of Ridge shrink all coefficients towards zero, it will not set any of them exactly to zero (unless \\(\\lambda = \\infty\\)).\n→ Ridge including all predictors not a problem for model accuracy but for interpretability. Increasing the value of λ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.\nBccause of that: Lasso was invented, similar to ridge. Only withouth the square. As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the ℓ1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.\n→ Lasso in advantage to Ridge: also performs variable selection, Lasso yields sparse models, only a subset of variables. Seen in the figure above, Lasso will have less predictors with a lower lambda.\nWhy has Lasso a variable selection?\n\n\nLeast square solution \\(\\hat{\\beta}\\).\nBlue areas constrains for left Lasso and right panel Ridge\nred elipses, contours of the RSS, all points in one conture circle have the same RSS value\nwhen we perform lasso and ridge, we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget how large s (the variance, that determine S) can be\nlasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region\nSince ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero.\n\nLasso or Ridge? Which has the better predictions?\nneither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.\nIn R:\n#Lasso fit &lt;- glmnet(x, y, alpha = 1, lambda = 1.5)\n#Ridge fit &lt;- glmnet(x, y, alpha = 0, lambda = 1.5)\n\n\n4.11.4 Standardization of predictors\nis really important, set them to the same size makes a huge difference if for example temperature is measured with Fahrenheit or Celsius.\nThe parameters has to be scale equivalent. For instance, consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of 1,000. Now due to the sum of squared coefficients term in the regression formulation, such a change in scale will not simply cause the ridge regression coefficient estimate for income to change by a factor of 1,000. In other words, \\(X_i \\hat{\\beta_i\\lambda}\\) will depend not only on the value of λ, but also on the scaling of the jth predictor.\nConsequently, all of the standardized predictors will have a standard deviation of one. As a result the final fit will not depend on the scale on which the predictors are measured.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#in-r-1",
    "href": "linear-regression.html#in-r-1",
    "title": "4  Linear Regression",
    "section": "4.12 in R",
    "text": "4.12 in R\nlibrary(glmnet)\n# We need to input a predictor matrix x and a response (outcome) variable y, as well as a family = \"gaussian\" . Generate the input matrix:\n\nx_train &lt;- model.matrix(Salary ~ ., data = baseball_train)\n\nresult &lt;- glmnet(x   = x_train[, -1],     # X matrix without intercept\n         y   = baseball_train$Salary, # Salary as response\n         family = \"gaussian\",       # Normally distributed errors\n         alpha = 1,           # LASSO penalty, if its set to 0 ridge regression\n         lambda = 15)           # Penalty value\n\n# extract coefficients out of the beta element\nrownames(coef(result))[which(coef(result) != 0)]\n\n# tuninng lamda without assigning a lambda will give you an object that contains sets of coefficients for different values of lambda\n\n#determine the lambda value using k-fold cross validation\nx_cv &lt;- model.matrix(Salary ~ ., bind_rows(baseball_train, baseball_valid))[, -1]\nresult_cv &lt;- cv.glmnet(x = x_cv, y = c(baseball_train$Salary, baseball_valid$Salary), nfolds = 15) # setting the k to 15\nbest_lambda &lt;- result_cv$lambda.min\n\n #the output best lambda gives you the value of the tuning parameter\nIn this practical, you will learn how to handle many variables with regression by using variable selection techniques, shrinkage techniques, and how to tune hyper-parameters for these techniques. This practical has been derived from chapter 6 of ISLR. In addition, you will need for loops (see also the Basics: For Loops tab on the course website under week 3), data manipulation techniques from Dplyr, and the caret package (see lab week 3) to create a training, validation and test split for the used dataset\nAnother package we are going to use is glmnet. For this, you will probably need to install.packages(\"glmnet\") before running the library() functions.\n\n#install.packages(\"glmnet\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ISLR)\nlibrary(stats)\nlibrary(glmnet)\nlibrary(tidyverse)\nlibrary(caret)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#best-subset-selection",
    "href": "linear-regression.html#best-subset-selection",
    "title": "4  Linear Regression",
    "section": "4.13 Best subset selection",
    "text": "4.13 Best subset selection\nOur goal is to to predict Salary from the Hitters dataset from the ISLR package. In this at home section, we will do the pre-work for best-subset selection. During the lab, we will continue with the actual best subset selection. First, we will prepare a dataframe baseball from the Hitters dataset where you remove the baseball players for which the Salary is missing. Use the following code:\n\nbaseball &lt;- Hitters %&gt;% filter(!is.na(Salary))\n\nWe can check how many baseball players are left using:\n\nnrow(baseball)\n\n[1] 263\n\n\n\n\nCreate baseball_train (50%), baseball_valid (30%), and baseball_test (20%) datasets using the createDataPartition() function of the caret package.\n\n\n\n# define the training partition \ntrain_index &lt;- createDataPartition(baseball$Salary, p = .5, \n                 list = FALSE, \n                 times = 1)\n\n# split the data using the training partition to obtain training data\nbaseball_train &lt;- baseball[train_index,]\n\n## remainder of the split is the validation and test data (still) combined \nbaseball_val_test &lt;- baseball[-train_index,]\n\n# split the remaining 50% of the data in a validation and test set\nval_index &lt;- createDataPartition(baseball_val_test$Salary, p = .6, \n                 list = FALSE, \n                 times = 1)\n\nbaseball_valid &lt;- baseball_val_test[val_index,]\nbaseball_test &lt;- baseball_val_test[-val_index,]\n\n# Outcome of this section is that the data (100%) is split into:\n# training (~50%)\n# validation (~30%)\n# test (~20%)\n\n\n\nUsing your knowledge of ggplot from lab 2, plot the salary information of the train, validate and test groups using geom_histogram() or geom_density()\n\n\n\nhist &lt;- ggplot()+\n geom_density(data= baseball_train, aes(x=Salary, color= \"p\"), size=1, alpha=0.2)+ \n geom_density(data= baseball_valid, aes(x=Salary, color=\"o\"), size=1)+\n geom_density(data= baseball_test, aes(x=Salary, color=\"g\"), size=1)+\n scale_color_manual(name= \"color\", values=c(\"p\" = \"purple\", \"o\" = \"orange\", \"g\" = \"green\"), labels=c(\"test\", \"valid\", \"train\"))+\n theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nhist\n\n\n\n\n\n\n\n\nWe will use the following function which we called lm_mse() to obtain the mse on the validation dataset for predictions from a linear model:\n\nlm_mse &lt;- function(formula, train_data, valid_data) {\n y_name &lt;- as.character(formula)[2]\n y_true &lt;- valid_data[[y_name]]\n \n lm_fit &lt;- lm(formula, train_data)\n y_pred &lt;- predict(lm_fit, newdata = valid_data)\n \n mean((y_true - y_pred)^2)\n}\n\nNote that the input consists of (1) a formula, (2) a training dataset, and (3) a test dataset.\n\nTry out the function with the formula Salary ~ Hits + Runs, using baseball_train and baseball_valid.\n\n\nlm_mse(Salary ~Hits + Runs, baseball_train, baseball_valid)\n\n[1] 192681.2\n\n\nWe have pre-programmed a function for you to generate a character vector for all formulas with a set number of p variables. You can load the function into your environment by sourcing the .R file it is written in:\n\ngenerate_formulas &lt;- function(p, x_vars, y_var) {\n  # Input checking\n  if (p %% 1 != 0)           stop(\"Input an integer n\")\n  if (p &gt; length(x_vars))    stop(\"p should be smaller than number of vars\")\n  if (!is.character(x_vars)) stop(\"x_vars should be a character vector\")\n  if (!is.character(y_var))  stop(\"y_vars should be character type\")\n  \n  # combn generates all combinations, apply turns them into formula strings\n  apply(combn(x_vars, p), 2, function(vars) {\n    paste0(y_var, \" ~ \", paste(vars, collapse = \" + \"))\n  })\n}\n\nYou can use it like so:\n\ngenerate_formulas(p = 2, x_vars = c(\"x1\", \"x2\", \"x3\", \"x4\"), y_var = \"y\")\n\n[1] \"y ~ x1 + x2\" \"y ~ x1 + x3\" \"y ~ x1 + x4\" \"y ~ x2 + x3\" \"y ~ x2 + x4\"\n[6] \"y ~ x3 + x4\"\n\n\n\nCreate a character vector of all predictor variables from the Hitters dataset. colnames() may be of help. Note that Salary is not a predictor variable.\n\n\npred_vec &lt;- colnames(Hitters)\npred_vec &lt;- pred_vec[!pred_vec %in% \"Salary\"] %&gt;% as.character()\npred_vec \n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"NewLeague\"\n\n\n\nUsing the function generate_formulas() (which is inlcuded in your project folder for lab week 4), generate all formulas with as outcome Salary and 3 predictors from the Hitters data. Assign this to a variable called formulas. There should be 969 elements in this vector.\n\n\nformulas &lt;- generate_formulas(p = 3, x_vars = pred_vec, y_var = \"Salary\")\n\nAt home, you created the baseball dataframe which contains a subset Hitters dataset of the ISLR package: only the rows of the Hitters dataset for which the information on Salary is complete. In addtion, you divided the baseball dataset into a train, validation and test split, and generated all formulas with as outcome Salary and 3 predictors from the Hitters data. Now, within the lab, we are going to find the best set of 3 predictors in the Hitters dataset.\n\nUse a for loop to find the best set of 3 predictors in the Hitters dataset based on MSE. Use the baseball_train and baseball_valid datasets.\n\nWhen creating the for loop, use the function as.formula() from the stats package to loop over all the equations contained in formulas. as.formula() transforms the characters of the input to a formula, so we can actually use it as a formula in our code.\nTo select the best formula with the best MSE, use the function which.min(), which presents the lowest value from the list provided.\n\n#create a vector , we will fill fill with MSE values\nmses &lt;- rep(0,969)\n\n# loop over all the formulas\n #we want to loop the lm for i model in one of 969 formuals\n # mses[i] is the reference to the vector we store the calculations for i in\n # lm_mse is our function\n # formulas[i] refers to one variation of the formukla\n # then the data set added \nfor(i in 1:969){\n mses[i] &lt;- lm_mse(as.formula(formulas[i]), baseball_train, baseball_valid)\n}\n\n# extract the minimal:\nbest_3_preds &lt;- formulas[which.min(mses)]\nbest_3_preds\n\n[1] \"Salary ~ Hits + CRBI + Division\"\n\n\n\nDo the same for 1, 2 and 4 predictors. Now select the best model from the models with the best set of 1, 2, 3, or 4 predictors in terms of its out-of-sample MSE\n\n\nformula1 &lt;- generate_formulas(p = 1, x_vars = pred_vec, y_var = \"Salary\")\nformula2 &lt;- generate_formulas(p = 2, x_vars = pred_vec, y_var = \"Salary\")\nformula4 &lt;- generate_formulas(p = 4, x_vars = pred_vec, y_var = \"Salary\")\n\nmses1 &lt;- rep(0,19)\nfor(i in 1:19){\n mses1[i] &lt;- lm_mse(as.formula(formula1[i]), baseball_train, baseball_valid)\n}\nbest_1_preds &lt;- formulas[which.min(mses1)]\n\nmses2 &lt;- rep(0,171)\nfor(i in 1:171){\n mses2[i] &lt;- lm_mse(as.formula(formula2[i]), baseball_train, baseball_valid)\n}\nbest_2_preds &lt;- formulas[which.min(mses2)]\n\nmses4 &lt;- rep(0,3876)\nfor(i in 1:3876){\n mses4[i] &lt;- lm_mse(as.formula(formula4[i]), baseball_train, baseball_valid)\n}\nbest_4_preds &lt;- formulas[which.min(mses4)]\n\nmin(mses)\nmin(mses1)\nmin(mses2)\nmin(mses4)\n\n[1] 121754.9\n[1] 163444.9\n[1] 131244.8\n[1] 115821.2\n\n\n\n\nCalculate the test MSE for the model with the best number of predictors.\n\n\n\n\nUsing the model with the best number of predictors, create a plot comparing predicted values (mapped to x position) versus observed values (mapped to y position) of baseball_test.\n\n\nThrough enumerating all possibilities, we have selected the best subset of at most 4 non-interacting predictors for the prediction of baseball salaries. This method works well for few predictors, but the computational cost of enumeration increases quickly to the point where it is not feasible to enumerate all combinations of variables:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#regularization-with-glmnet",
    "href": "linear-regression.html#regularization-with-glmnet",
    "title": "4  Linear Regression",
    "section": "4.14 Regularization with glmnet",
    "text": "4.14 Regularization with glmnet\nglmnet is a package that implements efficient (quick!) algorithms for LASSO and ridge regression, among other things.\n\nSkim through the help file of glmnet. We are going to perform a linear regression with normal (gaussian) error terms. What format should our data be in?\n\n\n?glmnet\n\nAgain, we will try to predict baseball salary, this time using all the available variables and using the LASSO penalty to perform subset selection. For this, we first need to generate an input matrix.\n\nFirst generate the input matrix using (a variation on) the following code. Remember that the “.” in a formula means “all available variables”. Make sure to check that this x_train looks like what you would expect.\n\n\nx_train &lt;- model.matrix(Salary ~ ., data = baseball_train) \nhead(x_train)\n\n                 (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits\n-Alan Ashby                1   315   81     7   24  38    39    14   3449   835\n-Alvin Davis               1   479  130    18   66  72    76     3   1624   457\n-Andre Dawson              1   496  141    20   65  78    37    11   5628  1575\n-Al Newman                 1   185   37     1   23   8    21     2    214    42\n-Argenis Salazar           1   298   73     0   24  24     7     3    509   108\n-Andres Thomas             1   323   81     6   26  32     8     2    341    86\n                 CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists\n-Alan Ashby          69   321  414    375       1         1     632      43\n-Alvin Davis         63   224  266    263       0         1     880      82\n-Andre Dawson       225   828  838    354       1         0     200      11\n-Al Newman            1    30    9     24       1         0      76     127\n-Argenis Salazar      0    41   37     12       0         1     121     283\n-Andres Thomas        6    32   34      8       1         1     143     290\n                 Errors NewLeagueN\n-Alan Ashby          10          1\n-Alvin Davis         14          0\n-Andre Dawson         3          1\n-Al Newman            7          0\n-Argenis Salazar      9          0\n-Andres Thomas       19          1\n\n\nIntercept is one.\nThe model.matrix() function takes a dataset and a formula and outputs the predictor matrix where the categorical variables have been correctly transformed into dummy variables, and it adds an intercept. It is used internally by the lm() function as well!\n\nUsing glmnet(), perform a LASSO regression with the generated x_train as the predictor matrix and Salary as the response variable. Set the lambda parameter of the penalty to 15. NB: Remove the intercept column from the x_matrix – glmnet adds an intercept internally.\n\n\nresult &lt;- glmnet(x   = x_train[, -1],     # X matrix without intercept\n         y   = baseball_train$Salary, # Salary as response\n         family = \"gaussian\",       # Normally distributed errors\n         alpha = 1,           # LASSO penalty\n         lambda = 15)           # Penalty value\n\n\nThe coefficients for the variables are in the beta element of the list generated by the glmnet() function. Which variables have been selected? You may use the coef() function.\n\nExtract all coefficients, without the coefficients that set to zeor.\n\nrownames(coef(result))[which(coef(result) !=0)]\n\n [1] \"(Intercept)\" \"Hits\"        \"Walks\"       \"CHmRun\"      \"CRuns\"      \n [6] \"CRBI\"        \"DivisionW\"   \"PutOuts\"     \"Assists\"     \"NewLeagueN\" \n\n\n\nCreate a predicted versus observed plot for the model you generated with the baseball_valid data. Use the predict() function for this! What is the MSE on the validation set?\n\n\nx_valid &lt;- model.matrix(Salary ~ ., data = baseball_valid)[,-1]\ny_pred &lt;- as.numeric(predict(result, newx = x_valid))\n?Hitters\ntibble(Predicted = y_pred, Observed = baseball_valid$Salary) %&gt;% \n ggplot(aes(x = Predicted, y = Observed)) +\n geom_point() + \n geom_abline(slope = 1, intercept = 0, lty = 2) +\n theme_minimal() +\n labs(title = \"Predicted versus observed salary\")\n\n\n\n\n\n\n\n\n\nmse &lt;- function(y_true, y_pred) {\n mean((y_true - y_pred)^2)\n}\nmse(baseball_valid$Salary, y_pred)\n\n[1] 127722.9",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#tuning-lambda",
    "href": "linear-regression.html#tuning-lambda",
    "title": "4  Linear Regression",
    "section": "4.15 Tuning lambda",
    "text": "4.15 Tuning lambda\nLike many methods of analysis, regularized regression has a tuning parameter. In the previous section, we’ve set this parameter to 15. The lambda parameter changes the strength of the shrinkage in glmnet(). Changing the tuning parameter will change the predictions, and thus the MSE. In this section, we will select the tuning parameter based on out-of-sample MSE.\n\n\nFit a LASSO regression model on the same data as before, but now do not enter a specific lambda value. What is different about the object that is generated? Hint: use the coef() and plot() methods on the resulting object.\n\n\n\nresult_nolambda &lt;- glmnet(x = x_train[, -1], y = baseball_train$Salary, \n             family = \"gaussian\", alpha = 1)\n\n# This object contains sets of coefficients for different values of lambda,\n# i.e., different models ranging from an intercept-only model (very high \n# lambda) to almost no shrinkage (very low lambda).\n\nplot(result_nolambda)\n\n\n\n\n\n\n\n\n\n\nTo help you interpret the obtained plot, Google and explain the qualitative relationship between L1 norm (the maximum allowed sum of coefs) and lambda.\n\n\nFor deciding which value of lambda to choose, we could work similarly to what we have don in the best subset selection section before. However, the glmnet package includes another method for this task: cross validation.\n\nUse the cv.glmnet function to determine the lambda value for which the out-of-sample MSE is lowest using 15-fold cross validation. As your dataset, you may use the training and validation sets bound together with bind_rows(). What is the best lambda value?\n\nNote You can remove the first column of the model.matrix object, which contains the intercept, for use in cv.glmnet. In addition, To obtain the best lambda value, you can call the output value lambda.min from the object in which you stored the results of calling cv.glmnet.\n\nx_cv &lt;- model.matrix(Salary ~ ., bind_rows(baseball_train, baseball_valid))[, -1]\nresult_cv &lt;- cv.glmnet(x = x_cv, y = c(baseball_train$Salary, baseball_valid$Salary), nfolds = 15)\nbest_lambda &lt;- result_cv$lambda.min\nbest_lambda\n\n[1] 1.744613\n\n\n\nTry out the plot() method on this object. What do you see? What does this tell you about the bias-variance tradeoff?\n\n\nplot(result_cv)\n\n\n\n\n\n\n\n\nthe MSE is high with very small values of lambda (no shrinkage) and with very large values of lambda (intercept-only model). introducing a bit of bias lowers the variance relatively strongly (fewer variables in the model) and therefore the MSE is reduced.\nIt should be noted, that for all these previous exercises they can also be completed using the Ridge Method which is not covered in much depth during this practical session. To learn more about this method please refer back Section 6.2 in the An Introduction to Statistical Learning Textbook.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#comparing-methods-advanced",
    "href": "linear-regression.html#comparing-methods-advanced",
    "title": "4  Linear Regression",
    "section": "4.16 Comparing methods (advanced)",
    "text": "4.16 Comparing methods (advanced)\nThis last exercise is optional. You can also opt to view the answer when made available and try to understand what is happening in the code.\n\nCreate a bar plot comparing the test set (baseball_test) MSE of (a) linear regression with all variables, (b) the best subset selection regression model we created, (c) LASSO with lambda set to 50, and (d) LASSO with cross-validated lambda. As training dataset, use the rows in both the baseball_train and baseball_valid\n\n\n# create this new training dataset and the test dataset\ntrain_data &lt;- bind_rows(baseball_train, baseball_valid)\nx_test &lt;- model.matrix(Salary ~ ., data = baseball_test)[, -1]\n\n# generate predictions from the models\ny_pred_ols &lt;- predict(lm(Salary ~ ., data = train_data), newdata = baseball_test)\ny_pred_sub &lt;- predict(lm(Salary ~ Runs + CHits + Division + PutOuts, data = train_data),\n           newdata = baseball_test)\n# these two use x_cv from the previous exercises\ny_pred_las &lt;- as.numeric(predict(glmnet(x_cv, train_data$Salary, lambda = 50), newx = x_test))\ny_pred_cv &lt;- as.numeric(predict(result_cv, newx = x_test, s = best_lambda))\n\n# Calculate MSEs\nmses &lt;- c(\n mse(baseball_test$Salary, y_pred_ols),\n mse(baseball_test$Salary, y_pred_sub),\n mse(baseball_test$Salary, y_pred_las),\n mse(baseball_test$Salary, y_pred_cv)\n)\n\n# Create a plot\ntibble(Method = as_factor(c(\"lm\", \"subset\", \"lasso\", \"cv_las\")), MSE = mses) %&gt;% \n ggplot(aes(x = Method, y = MSE, fill = Method)) +\n geom_bar(stat = \"identity\", col = \"black\") +\n theme_minimal() +\n theme(legend.position = \"none\") +\n labs(title = \"Comparison of test set MSE for different prediction methods\") +\n scale_fill_viridis_d() # different colour scale",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#conclusions",
    "href": "linear-regression.html#conclusions",
    "title": "4  Linear Regression",
    "section": "4.17 Conclusions",
    "text": "4.17 Conclusions\n\nBy using feature selection or regularization, we can obtain better prediction accuracy and model interpretability\nFeature selection includes best subset, forward and backward selection\nBest subset selection performs best, but it comes at a prize Regularization includes LASSO and Ridge\nLASSO shrinks unimportant parameters to truly zero, while Ridge shrinks them to small values",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "5  classification",
    "section": "",
    "text": "5.1 Readings\nISLR:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#readings",
    "href": "classification.html#readings",
    "title": "5  classification",
    "section": "",
    "text": "Chapter 4: Classification",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#classification",
    "href": "classification.html#classification",
    "title": "5  classification",
    "section": "5.2 Classification",
    "text": "5.2 Classification\nSupervised Learning two methods:\n\nregression (numerical or metric, linear correlations)\nclassification, predict to which category an observation belongs (qualitative outcome)\nmany supervised learning problems concern categorical outcome - cancer, weather, banking data (default on payment of debt), images (is a cat on the photo or not?), news articles (in rubrics like sport, politics)\n\n\n5.2.1 Types of classification\n\n\nmost of the methods based on binary classification, because no order, if you transform qualitative categories in numbers\nso in most cases multi-class classification must be transformed into binary classification → easiest way: one-vs-all-model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#which-model",
    "href": "classification.html#which-model",
    "title": "5  classification",
    "section": "5.3 Which model?",
    "text": "5.3 Which model?\nParametric or non-parametric classifiers\n\nParametric model:\n\nWhen we have a data set, we made assumption about the data set\nwe select from our assumption relevant predictors\nlogistic regression (this lecture)\n\nNon-parametric model:\n\nas many predictors as we want\nless assumption about the data\nrequires more data\nKNN (this lecture) or decision trees (lecture 8)\n\nGenerative & Discriminative models\nGenerative:\n\nRepresent both the data and the labels\nfocus of probability: has a person cancer? yes or no? How probable is yes or no for each observation?\nOften, makes use of conditional independence and priors\nModels of data may apply to future prediction problems\nExamples: Naïve Bayes classifier, Bayesian network\n\nDiscriminative:\n\nLearn to directly predict the labels from the data\nOften, assume a simple boundary (e.g., linear), not want to estimate probability for each observation, are looking for the relationship, the correlation\nOften easier to predict a label from the data than to model the data\nExamples: Logistic regression, SVM, decision tree\n\nGenerative classifiers try to model the data. Discriminative classifiers try to predict the label.\n\n5.3.1 Why use another model than logistic regression?\n\nwhen substantial separation between two classes, the parameter estimates for logistic regression quite unstable\ndistribution of predictors approxiametely normal and small sample size\nmore than two response categories\nthen: use the bayes classifier! Three classifiers, that use different estimates to approximate the Bayes classifier (see ISLR p. 142 -158)\nlinear discriminate analysis\nquadratic discirminant analysis\nnaive Bayes",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#classification-algorithms",
    "href": "classification.html#classification-algorithms",
    "title": "5  classification",
    "section": "5.4 Classification algorithms",
    "text": "5.4 Classification algorithms\nThere is a lot! Here the most popular:\n\nK-nearest neighbors\nLogistic regression\nNaive Bayes\nNeural networks (deep learning)\nSupport vector machine\nDecision tree\nRandom forest\n\nWhich algorithm to choose: Generalization\nHow well does a learned model generalize from the data it was trained on to a new test set?\n\nNo free Lunch Theorem: Variance-Bias Trade Off → there is no optimal solution of analyzing data !",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#k-nearest-neighbors",
    "href": "classification.html#k-nearest-neighbors",
    "title": "5  classification",
    "section": "5.5 K-nearest neighbors",
    "text": "5.5 K-nearest neighbors\n\nOne of the most simple (supervised) machine learning methods\nBased on feature similarity: how similar is a data point to its neighbor(s) and classifies the data point into the class it is most similar to.\nk determines, how many neighbors are included to assign a value to the missing observation, take the average of the neighbors\ndifferent similarities are there for methodolical determination of values, we as Data Scientists decide, which method we use to deterine similarity\nthis is a non-parameteric model: does not make assumptions about the data set\nthis is lazy algorithm: memorizes the training data set itself instead of learning a function from it, we don´t learn any functions, we don´t get any parameters as a result\nCan be used for both classification and regression (but more commonly used for classification)\nAlthough a very simple approach, KNN can often produce classifiers that are surprisingly good!\n\nThe model:\nGiven the memorized training data, and a new data point (test observation):\n\nIdentify the K closests points in the training data to the new data point \\(X_0\\).\nK is a hyper parameter, cannot be computed like the normal parameters out of the data → has to be set by the Scientist but: - Hyper parameter tuning: Assign different values to K and compute models with that\nellbow method for KNN: Try different values and choose the value with the best model performance\nThis set of ‘nearest neighbors’ we call Estimate the probability that the new data point belongs to category by \\[\n    PR(Y=j | X=x_0) = \\frac{1}{K} \\sum_{i \\epsilon N_0}I (y_i = j)\n    \\]\n(so, the fraction of points in whose response equal j) - Majority vote: classify the test observation to the categroy with the largest probability\n\n\nTuning Parameter K\n\nResults obtained with KNN highly depend on chosen value for K , the number of neighbors used\nSmall K (e.g.,K = 1): low bias but very high variance, ‘overly flexible decision boundary’ (see next slides)\nLarge K: low-variance but high-bias, ‘decision boundary’ that is close to linear\nif K is too large, e.g. equal to sample size, we compute the same value for each observation, we want to predict The optimal value for K needs to be determined using a (cross-)validation approach\n\n \n\ndecision boundary is the line, that separate the observations and assign them to a class\nin the first, k=1 is the best solution, because the obersvations are very clear separated\nfor less well seperated observations, an higher k is better\nleft once are high variance (overfitting), right once are high biased (underfitting)\nk=5 must not be the best solution, we have to check other values\none option: do clustering before!\n\nin R\n#split the data set int train, validation, test\n## create class predictions for different ks using the train data to learn the model and the test data to evaluate the parameters\nlibrary(class)\nknn_5_pred &lt;- knn(\n  train = default_train %&gt;% select(-default),\n  test  = default_valid  %&gt;% select(-default),\n  cl    = as_factor(default_train$default),\n  k     = 5\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#logistic-regression",
    "href": "classification.html#logistic-regression",
    "title": "5  classification",
    "section": "5.6 Logistic regression",
    "text": "5.6 Logistic regression\n\nUsed to predict the probability that belongs to one of two categories (i.e., a binary outcome), for example:\n\nsmoking/ non smoking\npass/ fail an exam\n\nCan be extended to model &gt; 2 outcome categories: multinomial logistic regression (not treated in this course)\nOther option to model &gt; 2 outcome categories: Neural networks, naive Bayes, linear discriminant analysis (not treated in this course, but treated in ISLR)\n\n\n\\[\nWant 0 \\le H_\\theta (x) \\le 1\n\\]\n\\[\nH_\\theta (x) = \\theta^T x\n\\]\n\\[\nH_\\theta (x) = g(\\theta^T x)\n\\]\nsigmoid function /logistic function:\n\\[\ng(z) = \\frac{1}{1 + e^-z}\n\\]\n\n\n\n5.6.1 Why can linear regression not be used on this type of data?\n\nLinear regression would predict impossible outcomes ( Pr (x) &lt; 0 and &gt; 1)\nAssumption of normally distributed residuals heavily violated\nTo avoid this problem, we use a ‘trick’: we use a logistic `link function (logit)`\nAdvantage: all predicted probabilities are above 0 and below 1\nso for the example below:\n\nDefault is category, the Y outcome\nbalance is X\ne is Eulersche Zahl and in R with exp written\n\n\n\\[\nPr (Default = yes| balance) = \\frac{e^{\\beta_0 + \\beta1 balance}}{ 1 + exp^{\\beta_0 + \\beta1 balance}}\n\\]\n\nleft: normal linear regression with Default = yes (negative values!, not possible!)\nright: logistic regression, all probabilities are positive!\n\n\n\nto fit this model. maximum likelihood → always S-shaped form\n\n\n\n5.6.2 odds\n\ncan take a value between 0 and \\(\\infty\\) . Values very near to 0 or to \\(\\infty\\) indicate very low or very high probability. This is the formula of the quantity:\n\n\\[\n\\frac{Pr(Y=1)}{Pr(Y=0)} = \\frac{pi}{1-pi} = e^{\\beta_0 + \\beta_1X_1 + \\dots}\n\\]\n\nused instead of probability → Es geht von der Idee der Chancen, der odds aus. Eine Möglichkeit, Wahrscheinlichkeiten anzugeben. Wie hoch ist die Chance, dass ein Ereignis eintritt gegenüber einem anderen?\n\nBeispiel: Münzwurf, 1:1, 1 Chance ist Kopf, eine andere Chance ist Zahl\nBeispiel 2: Würfeln einer 6, 1:5, eine Zahl 6 ist möglich, andere 5 Zahlen auch möglich\n\nHence, when using logistic regression, we are modelling the log of the odds. Odds are a way of quantifying the probability of an event E.\n\nodds for an event E are: \\[\nodds(E) = \\frac{Pr(E)}{Pr(E^c)}  = \\frac{Pr(E)}{1-Pr(E)}\n\\]\nThe odds of getting head in a coin toss is:\n\n\n\\[\nodds(heads) = \\frac{Pr(heads)}{Pr(tails)} = \\frac{Pr(heads)}{1- Pr(heads)} = odds(heads) = \\frac{0.5}{1-0.5} = 1\n\\]\n\nAnother example: game Lingo 44 balls, 26 are blue, 6 red and 2 are green\n\nchoosing blue:\n\n\n\\[\nodds(blue) =\\frac{36} {6} = \\frac{46/44} {8/44} = 4.5\n\\]\n\nHence, odds of 1 indicate an equal likelihood of event occurring or not occurring. Odds &lt; 1 indicate a lower likelihood of the event, odds &gt;1 indicate higher likelihood\n\n\\[\nln(odds) = \\beta_0 + \\beta_1X_1 + \\dots\n\\]\nSo the linear part of the function models the log of the odds. With the log we have a model where X is linear.\n\n\n5.6.3 How to estimate the coefficients?\n\nmaximum likelihood method:\n\nfor binaries, e. g. 1 = smoking, 0 = not smoking\napproach: estimate \\(\\beta_0\\) and \\(\\beta_1\\) such that the probability \\(\\hat{p}(x_i)\\) of an individual with \\(Y=1\\) corresponds as closely as possible to the observed \\(Y=1\\) for an individual.\n\nlogistic function:\n\n\\[\nl(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i´: y_i´= 0} (1- p(xi´))\n\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are chosen to maximize the likelihood function.\n\n\n\n5.6.4 Interpretation regression coefficients\n\nin linear regression one unit change in X is one unit change in Y, in logistic regression instead: with increase in X by one unit change the log odds by \\(\\beta_1\\) → multiplies the odds by \\(e^{\\beta_1}\\) .\nwith standard error measure the accuracy of the coefficients.\nz-statistic: a large (absolute) value in it indicates evidence against the null hypothesis\np-value: significant or not?\nestimated intercept plays not a role!\nbecause relationship between \\(p(X)\\) and \\(X\\) are not a straight line, a unit change in \\(X\\) does not change \\(p(X)\\) and \\(\\beta_1\\) does not correspond to the change in \\(X\\). But we direction is is corresponding, negative or positive relationship!\nqualitatively: positive or negative effect of the predictor on the log of the odds (logit)\nquantitatively: effect on the pdds is \\(exp(\\beta)\\)\neffect statistically significant?\n\nMaking Predictions:\n\nmaking predictions by filling hin the equatation\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1X} +\\hat{\\beta_pX_p}}} {1 + e^{\\hat{\\beta_0} + \\hat{\\beta_1X}+\\hat{\\beta_pX_p}}}\n\\]\nExample: We have the following coefficients for a multiple logistic regression:\n\n How can we compare different classes?\n\n30 year old female first class\n\n\n(exp(3.76 - (0.039*30))) /  (1 + exp(3.76 + ((-0.039)*30)))\n\n[1] 0.9302152\n\n\n\n45 old male from 3rd class?\n\n\n(exp(3.76 - (2.521*1) - (2.631*1) - (0.039*45))) / (1-exp(3.76 - (2.521*1) - (2.631*1)-(0.039*45)))\n\n[1] 0.0449112\n\n\nWhile a 30 year old female in first class has a probability of 93% survival rate, a 45 old male from 3rd class only have a 4 % rate.\n\n\n5.6.5 Multinominal Logistic Function\n\nexpand logistic regression to more than 2 classes\nsingle class is chosen as base line (not really necessary, because estimates change, but key model outputs and log odds stay the same)\nso instead the softmax coding!\n\n\\[\nlog(\\frac{Pr(Y=k| X=x)}{Pr(Y=K| X=x)} ) = \\beta_{k0} + \\beta_{k1x1} + \\beta_{kpxp}\n\\]\nlog odds softmax coding function, which treat all K equally without baseline:\n\\[\nPr(Y = k | X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1x1} +\\beta_{kpxp}}} { \\sum_{l=1}^K  e^{\\beta_{k0}+ \\beta_{k1x1} +\\beta_{kpxp}}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "classification.html#in-r",
    "href": "classification.html#in-r",
    "title": "5  classification",
    "section": "5.7 in R",
    "text": "5.7 in R\n## make a logistic regression using training data\nlibrary(glm)\nlr_mod &lt;- glm(default ~ ., family = binomial, data = default_train)\n\n#get the coefficients\ncoefs &lt;- coef(lr_mod)\n\n5.7.1 Evaluating classifiers\nWhen applying classifiers, we have new options to evaluate how well a classifier is doing besides model fit:\n\nConfusion matrix (used to obtain most measures below)\nSensitivity (‘Recall’)\nSpecificity\nPositive predictive value (‘Precision’)\nNegative predictive value\nAccuracy (and error rate)\nROC and area under the curve\nFor even more: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n\n\n\n5.7.2 Most important: Confusion Matrix\np_ped &lt;- predict(log_mod_titanic, type = \"response\")\nwith(titanic, table(p_ped &gt; 0.5, Survived))\n\n### Survived\n### 0 1\n### FALSE 372 91\n### TRUE 71 222\n\nor in this way:\nconf_2NN &lt;- table(predicted = knn_2_pred, true = default_valid$default)\nconf_2NN\n  ###          true\n  ### predicted   No  Yes\n  ###       No  1885   46\n  ###       Yes   48   20\n\nNow we are looking at the confusion matrix of the complete data set.Even better would be to split the data, fit the model on the training data only and compute the confusion matrix on the validation set only.\nWe set a threshold for survival of 0.5. That is, for everyone with a predicted probability &gt; 0.5, we assume he/she survived, and a predicted probability of \\(\\le\\) 0.5 we assume he/she did not survive.\nIn case of a binary outcome (e.g., survival yes or no), we either correctly classify, or make two kind of mistakes:\nLabel someone as survivor who has survived (TP)\nLabel someone who died and who has died (TN)\nLabel a survivor as someone who died False negative (FN)\nLabel someone who died as a survivor False positive (FP)\n\n\n\n5.7.3 Measures\n\nmost important: Error rate and accuracy!\n\n\n\nError rate (ERR) is calculated as the number of all incorrect predictions divided by the total number of the dataset. The best error rate is 0.0, whereas the worst is 1.0.\n\\[\nERR = \\frac{FP + FN}{P + N}\n\\]\nAccuracy (ACC) is calculated as the number of all correct predictions divided by the total number of the dataset. The best accuracy is 1.0, whereas the worst is 0.0. It can also be calculated by 1 – ERR \\[\nACC = \\frac{TP + TN}{P + N}\n\\] Error costs of positives and negatives are usually different. For instance, one wants to avoid false negatives more than false positives or vice versa. Other basic measures, such as sensitivity and specificity, are more informative than accuracy and error rate in such cases.\nSensitivity (SN) is calculated as the number of correct positive predictions divided by the total number of positives. It is also called recall (REC) or true positive rate (TPR). The best sensitivity is 1.0, whereas the worst is 0.0.\n\\[\nSN = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n\\] Specificity (SP) is calculated as the number of correct negative predictions divided by the total number of negatives. It is also called true negative rate (TNR). The best specificity is 1.0, whereas the worst is 0.0. \\[\nSP = \\frac{TN}{TN + FP} = \\frac{TN}{N}\n\\] Precision (PREC) is calculated as the number of correct positive predictions divided by the total number of positive predictions. It is also called positive predictive value (PPV). The best precision is 1.0, whereas the worst is 0.0.\n\\[\nPREC = \\frac{TP}{TP + FP}\n\\]\nFalse positive rate (FPR) is calculated as the number of incorrect positive predictions divided by the total number of negatives. The best false positive rate is 0.0 whereas the worst is 1.0. It can also be calculated as 1 – specificity.\n\\[\nFPR = \\frac{FP}{TN + FP} = 1 -SP\n\\]\nThreshold\n\nMoving around the threshold affects the sensitivity and specificity!\nMoving the threshold especially makes sense when the predicted categories are unbalanced. For example, many more non survivors compared to survivors in the data set.\n\n\n\n5.7.4 ROC Curve\n\nThe Receiver Operating Characteristics (ROC) plot is a popular measure for evaluating classifier performance.\nROC curve is a popular graphic for simultaneously displaying the true and false positive rate for all possible thresholds\nThe ROC plot is a model-wide evaluation measure that is based on two basic evaluation measures – specificity and sensitivity. Specificity is a performance measure of the whole negative part of a dataset, whereas sensitivity is a performance measure of the whole positive part.\nThe overall performance of a classifier, summarized over all possible thresholds, is given by the area under the curve (AUC)\n\n\nA classifier with the random performance level always shows a straight line from the origin (0.0, 0.0) to the top right corner (1.0, 1.0). Two areas separated by this ROC curve indicates a simple estimation of the performance level. ROC curves in the area with the top left corner (0.0, 1.0) indicate good performance levels, whereas ROC curves in the other area with the bottom right corner (1.0, 0.0) indicate poor performance levels.\n\nA classifier with the perfect performance level shows a combination of two straight lines – from the origin (0.0, 0.0) to the top left corner (0.0, 1.0) and further to the top right corner (1.0, 1.0). It is important to notice that classifiers with meaningful performance levels usually lie in the area between the random ROC curve (baseline) and the perfect ROC curve.\nComparison of multiple classifiers is usually straight-forward especially when no curves cross each other. Curves close to the perfect ROC curve have a better performance level than the ones closes to the baseline.\n\nAnother advantage of using the ROC plot is a single measure called the AUC (area under the ROC curve) score. As the name indicates, it is an area under the curve calculated in the ROC space. One of the easy ways to calculate the AUC score is using the trapezoidal rule, which is adding up all trapezoids under the curve. Although the theoretical range of AUC score is between 0 and 1, the actual scores of meaningful classifiers are greater than 0.5, which is the AUC score of a random classifier.\n\n###Assignment\nDue to ascension day, this lab will not be a guided lab, but a lab at home. Note that the content covered in this lab is (just like all other labs) exam material. In this lab at home, two different classification methods will be covered: K-nearest neighbours and logistic regression. Please send your knitted .html file with the completed lab to your lab teacher before next lecture (Tuesday May 31st 9AM) to be marked as ‘attended’ for this lab. Solutions to this lab will be posted on Tuesday May 31st on the course website, and will shortly be discussed during lab 6.\nOne of the packages we are going to use is class. For this, you will probably need to install.packages(\"class\") before running the library() functions. In addition, you will again need the caret package to create a training and a validation split for the used dataset (note: to keep this at home lab compact, we will only use a training and validation split, and omit the test dataset to evaluate model fit). You can download the student zip including all needed files for practical 5 here.\n\nlibrary(MASS)\nlibrary(class)\nlibrary(caret)\nlibrary(ISLR)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(ggthemes)\n\nThis practical will be mainly based around the default dataset which contains credit card loan data for 10 000 people. With the goal being to classify credit card cases as yes or no based on whether they will default on their loan.\n\ndata(\"Default\")\nforce(Default)\nstr(Default)\n\n4 Variables, part of the ISLR book!\nResearch subject: Credit card loan data Sample: 10.000 customers of a bank Predictor: - student - default - balance - income\npredicted outcome: credit card case yes / no\n\nCreate a scatterplot of the Default dataset, where balance is mapped to the x position, income is mapped to the y position, and default is mapped to the colour. Can you see any interesting patterns already?\n\n\nPlot1&lt;-ggplot(Default, aes(x= balance, y= income, color= default))+\n  geom_point()+\n  labs( x= \"Balance\",\n        y= \"Income\",\n        color = \"Default\",\n        title= \"Balance and Income of customers grouped by default\")+\n  scale_color_viridis(discrete= TRUE)+\n  theme_minimal()\nPlot1\n\n\n\n\n\n\n\n\nFirst of all, no correlation between Balance and Income is obersable. That is really counterintuitive, because you would expect, with higher balance and higher income and vice versa. On explanation could be, that people that earn a lot of money are good in finance and invest their money in the stock market or in real estate and consequently do not have a higher balance. In my opinion, balance is the y variable and income the x variable, so I had mapped them the other way around. Because why should Balance influence the income? Do you get interest rates on your balances? What you can see further, is that customers with higher balance has often a default, whereas people with lower balance has less often a default.\n\nAdd facet_grid(cols = vars(student)) to the plot. What do you see?\n\n\nggplot(Default, aes(x= balance, y= income, color= default))+\n  geom_point()+\n  labs( x= \"Balance\",\n        y= \"Income\",\n        color = \"Default\",\n        title= \"Balance and Income of customers\",\n        subtitle= \"grouped by default and student\")+\n  scale_color_viridis(discrete= TRUE)+\n  theme_minimal()+\n  facet_grid(cols= vars(student))\n\n\n\n\n\n\n\n\nWhat you can see now, is that students have by far less income then not students. Here, no one has an income above 40.000, whereas in the not student group the only a few earn less than 10.000 and 40.000 ?$ seems like to be the centre span. Further, studens tend to have a higher balance, than non-students. Having a default look very similar distributed in both groups. In both groups, with higher balance more customers have a default.\n\nFor use in the KNN algorithm, transform “student” into a dummy variable using ifelse() (0 = not a student, 1 = student). Then, randomly split the Default dataset into a training set default_train (80%) and a validation set default_valid (20%) using the createDataPartition() function of the caret package.\n\nIf you haven’t used the function ifelse() before, please feel free to review it in Chapter 5 Control Flow (particular section 5.2.2) in Hadley Wickham’s Book Advanced R, this provides a concise overview of choice functions (if()) and vectorised if (ifelse()).\n\n# dummy variable\nlevels(Default$student)\n\n[1] \"No\"  \"Yes\"\n\nDefault$student &lt;- ifelse(Default$student == \"Yes\", 1,0)\n\n\n# define the training partition  ~ 80 percent\ntrain_index &lt;- createDataPartition(Default$default, p = .8, \n                                  list = FALSE, \n                                  times = 1)\n\n# split the data using the training partition to obtain training data   ~ 80 percent\ndefault_train &lt;- Default[train_index,]\n\n# define the valid set   ~ 20 percent\ndefault_valid &lt;- Default[-train_index,]\n\n\n5.7.4.1 K-Nearest Neighbours\nNow that we have explored the dataset, we can start on the task of classification. We can imagine a credit card company wanting to predict whether a customer will default on the loan so they can take steps to prevent this from happening.\nThe first method we will be using is k-nearest neighbours (KNN). It classifies datapoints based on a majority vote of the k points closest to it. In R, the class package contains a knn() function to perform knn.\n\nCreate class predictions for the test set using the knn() function. Use student, balance, and income (but no basis functions of those variables) in the default_train dataset. Set k to 5. Store the predictions in a variable called knn_5_pred.\n\nRemember: make sure to review the knn() function through the help panel on the GUI or through typing “?knn” into the console. For further guidance on the knn() function, please see Section 4.6.5 in An introduction to Statistical Learning\n\n?knn\n\n#Select training set except the predicted\nxtrain &lt;- default_train [, -1 ]\n\n#Select valid set except the predicted variables\nyvalid &lt;- default_valid[, -1]\n\n# vector containing the class labels for the training observations\nYtrain &lt;- default_train$default\n\n# predict with knn\nknn_5_pred &lt;- class::knn(train =xtrain, test = yvalid, cl = Ytrain, k= 5)\ntable(knn_5_pred)\n\nknn_5_pred\n  No  Yes \n1971   28 \n\n\n\nCreate two scatter plots with income and balance as in the first plot you made. One with the true class (default) mapped to the colour aesthetic, and one with the predicted class (knn_5_pred) mapped to the colour aesthetic. Hint: Add the predicted class knn_5_pred to the default_valid dataset before starting your ggplot() call of the second plot. What do you see?\n\n\n# Plot 1:\nPlot1\n\n\n\n\n\n\n\n\nWhy use I the default valid data set in the second plot? I do not understand, why I must plot with the validation set now. Should I use the validation set as data in the former graph, too?+\n→ default plot, because then we have the true class of default!\nSo here the former graph with only the validation data:\n\nPlot1v &lt;-ggplot(default_valid, aes(x= balance, y= income, color= default))+\n  geom_point()+\n  labs( x= \"Balance\",\n        y= \"Income\",\n        color = \"predicted Default\",\n        title= \"Balance and Income of customers grouped by default\")+\n  scale_color_viridis(discrete= TRUE)+\n  theme_minimal()\nPlot1v\n\n\n\n\n\n\n\n\n\n# combine data\ndefault_valid2 &lt;- cbind(default_valid, knn_5_pred)\n\n# plot predicted values\nPlot2&lt;-ggplot(default_valid2, aes(x= balance, y= income, color= knn_5_pred))+\n  geom_point()+\n  labs( x= \"Balance\",\n        y= \"Income\",\n        color = \"predicted Default\",\n        title= \"Balance and Income of customers grouped by predicted default\")+\n  scale_color_viridis(discrete= TRUE)+\n  theme_minimal()\nPlot2\n\n\n\n\n\n\n\n\nIf we compare the two distributions of the default groups in the validation data set, we see in the predicted default plot a more narrow distribution. In the second one, for the predicted subjects the balance is higher than as we observed. So with this method we have less variance, but much more bias. I would conclude, with this knn we could underfit the data.\n\nthere are quite some misclassifications: many “No” predictions\nwith “Yes” true class and vice versa.\n\n\nRepeat the same steps, but now with a knn_2_pred vector generated from a 2-nearest neighbours algorithm. Are there any differences?\n\n\nknn_2_pred &lt;- class::knn(train =xtrain, test = yvalid, cl = Ytrain, k= 2)\ntable(knn_2_pred)\n\nknn_2_pred\n  No  Yes \n1931   68 \n\n\nDuring this we have manually tested two different values for K, this although useful in exploring your data. To know the optimal value for K, you should use cross validation.\n\n# combine data\ndefault_valid2 &lt;- cbind(default_valid2, knn_2_pred)\n\n# plot predicted values\nPlot3&lt;-ggplot(default_valid2, aes(x= balance, y= income, color= knn_2_pred))+\n  geom_point()+\n  labs( x= \"Balance\",\n        y= \"Income\",\n        color = \"predicted Default\",\n        title= \"Balance and Income of customers grouped by predicted default\")+\n  scale_color_viridis(discrete= TRUE)+\n  theme_minimal()\nPlot3\n\n\n\n\n\n\n\n\nIn this case, we overfit the data, we have a lot more variance, but less bias. Neither 2 or 5 seems to be the best knn.\n\ncompared to the 5-nn model, more people get classified as “Yes”\nStill, the method is not perfect\n\n\n\n5.7.4.2 Assessing classification\nThe confusion matrix is an insightful summary of the plots we have made and the correct and incorrect classifications therein. A confusion matrix can be made in R with the table() function by entering two factors:\n\nconf_2NN &lt;- table(predicted = knn_2_pred, true = default_valid$default)\nview(conf_2NN)\n\nTo learn more these, please see Section 4.4.3 in An Introduction to Statistical Learning, where it discusses Confusion Matrices in the context of another classification method Linear Discriminant Analysis (LDA).\n\nWhat would this confusion matrix look like if the classification were perfect?\n\nIf the confusion matrix would be perfect, we have no false positives and no false negatives. Why we are using again only the validation set? In the ISLR book the true default status of the training data set is used.\n\nconf_true &lt;- table(true_valid = default_valid$default, true_valid = default_valid$default)\nconf_true \n\n          true_valid\ntrue_valid   No  Yes\n       No  1933    0\n       Yes    0   66\n\n\n\nMake a confusion matrix for the 5-nn model and compare it to that of the 2-nn model. What do you conclude?\n\n\nconf_5NN &lt;- table(predicted = knn_5_pred, true = default_valid$default)\nview(conf_5NN)\n\nIn the confusion matrix for the 2 KNN model there is a prediction for 49 customers to default, but they actually not did (false positives) and for 49 customers is the case the other way around (false negative). In the 5 Knn model the false positive is by far less, only 14 customers are predicted to would default but actually did not. The false negative rate is in the second model slightly higher.\nNow the bank manager has to asked the following question: Is a slightly more negative rate more important for credit card loans or is a much higher false positive rate much more higher? I do not know, how much it costs a bank, giving credit cards to people, that default.\n\nComparing performance becomes easier when obtaining more specific measures. Calculate the specificity, sensitivity, accuracy and the precision.\n\nAssuming that model 5 is better, we test:\n\n#specifity of both, true negative / all negative\nspec2 &lt;- 1884 / (1884 + 49) \nspec5 &lt;- 1919 / (1919 + 14)\nspec5 &gt; spec2\n\n[1] TRUE\n\n\n\n# sensivity of both, true positve / all positive\nsn2 &lt;- 17 / (17+ 49)\nsn5 &lt;- 14 / (14 +52)\nsn5 &gt; sn2\n\n[1] FALSE\n\n\n\n# accuracy TP + TN / P + N\nacc2 &lt;- (1884 + 17) / (1884 + 49 + 49 +17)\nacc5 &lt;- (1919 + 14) / (1919 + 14+ 52+14)\nacc5 &gt; acc2\n\n[1] TRUE\n\n\n\n# Precision TP / TP + FP\nprec2 &lt;- 17 / (17 + 49)\nprec5 &lt;- 14/ (14 + 14)\nprec5 &gt; prec2\n\n[1] TRUE\n\n\n\nThe 5NN model has better specificity, but worse sensitivity. However, the overall accuracy of the 5NN model is (slightly) better. When we look at the precision, the 5NN model performs a lot better compared to the 2NN model.\n\n\n\n\n5.7.5 Logistic regression\nKNN directly predicts the class of a new observation using a majority vote of the existing observations closest to it. In contrast to this, logistic regression predicts the log-odds of belonging to category 1. These log-odds can then be transformed to probabilities by performing an inverse logit transform:\n\\(p = \\frac{1}{1 + e^{-\\alpha}}\\)\nwhere \\(\\alpha\\); indicates log-odds for being in class 1 and \\(p\\) is the probability.\nTherefore, logistic regression is a probabilistic classifier as opposed to a direct classifier such as KNN: indirectly, it outputs a probability which can then be used in conjunction with a cutoff (usually 0.5) to classify new observations.\nLogistic regression in R happens with the glm() function, which stands for generalized linear model. Here we have to indicate that the residuals are modeled not as a Gaussian (normal distribution), but as a binomial distribution.\n\nUse glm() with argument family = binomial to fit a logistic regression model lr_mod to the default_train data.\n\n\nlr_mod &lt;- glm(default ~ student + balance + income, data= default_train, family= binomial)\npred_log &lt;- predict.glm(lr_mod, newdata = default_train, type= \"response\")\n\nNow we have generated a model, we can use the predict() method to output the estimated probabilities for each point in the training dataset. By default predict outputs the log-odds, but we can transform it back using the inverse logit function of before or setting the argument type = \"response\" within the predict function.\nIs it right, that I used the default_train data as `new data’ argument ?\n\nVisualise the predicted probabilities versus observed class for the training dataset in lr_mod. You can choose for yourself which type of visualisation you would like to make. Write down your interpretations along with your plot.\n\n\nlogdata &lt;- cbind(default_train, pred_log)\n\n\nggplot(logdata, aes(x=income, y= balance, color= default))+\n   geom_point()+\n  labs(title = \"observed and predicted default by balance\",\n       x= \"income\",\n       y=\"balance\")+\n    scale_color_viridis(discrete = TRUE)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(logdata, aes(x=income, y= balance, color= pred_log))+\n   geom_point()+\n  labs(title = \"observed and predicted default by balance\",\n       x= \"income\",\n       y=\"balance\")+\n    scale_color_viridis()+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe balance for the predicted default is way more higher than in the observations. In the predicted distribution no one with a default has a probable balance under 1500 $. While in the observed data in the validation set we can see a lot of observations that are lower than a 1500$. This is a sign for underfitting the data and for a too linear model.\nAnother advantage of logistic regression is that we get coefficients we can interpret.\n\nLook at the coefficients of the lr_mod model and interpret the coefficient for balance. What would the probability of default be for a person who is not a student, has an income of 40000, and a balance of 3000 dollars at the end of each month? Is this what you expect based on the plots we’ve made before?\n\n\noptions(scipen=999, digits=7)\nsummary(lr_mod)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = binomial, \n    data = default_train)\n\nCoefficients:\n                 Estimate    Std. Error z value             Pr(&gt;|z|)    \n(Intercept) -10.634426784   0.536539293 -19.820 &lt; 0.0000000000000002 ***\nstudent      -0.771838466   0.265548022  -2.907              0.00365 ** \nbalance       0.005728527   0.000257862  22.215 &lt; 0.0000000000000002 ***\nincome       -0.000001169   0.000009179  -0.127              0.89868    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1252.3  on 7997  degrees of freedom\nAIC: 1260.3\n\nNumber of Fisher Scoring iterations: 8\n\n\nWhat would the probability of default be for a person who is not a student, has an income of 40000, and a balance of 3000 dollars at the end of each month? Is this what you expect based on the plots we’ve made before?__\n\n(exp(-10.63 - (0.000001*40000) + (0.00573*3000))) /\n(1 + exp(-10.63 - (0.000001*40000) + (0.00573*3000)))\n\n[1] 0.9985285\n\n\nYes, I would expect this I look to the graphs before.\nLet’s visualise the effect balance has on the predicted default probability.\n\nprobability of .998 of defaulting. This is in line with the plots of before\nbecause this new data point would be all the way on the right.\n\n\nCreate a data frame called balance_df with 3 columns and 500 rows: student always 0, balance ranging from 0 to 3000, and income always the mean income in the default_train dataset.\n\n\nstudent &lt;- rep.int(0, 500)\nbalance &lt;- seq(0, 3000, length.out = 500)\nmean_income &lt;-mean(default_train$income)\nincome &lt;- rep(mean_income, 500)\n\nbalance_df &lt;- data.frame(student, balance, income)\nview(balance_df)\n\n\nUse this dataset as the newdata in a predict() call using lr_mod to output the predicted probabilities for different values of balance. Then create a plot with the balance_df$balance variable mapped to x and the predicted probabilities mapped to y. Is this in line with what you expect?\n\n\n# predict \npred_balance &lt;- predict.glm(lr_mod, newdata = balance_df, type= \"response\")\n\nbalance_df &lt;- cbind(balance_df, pred_balance)\n\n\n#plot\nplot_balance &lt;- ggplot(balance_df, aes(x=balance, y= pred_balance))+\n   geom_point()+\n  labs(title = \"predicition with balance data set\",\n       x= \"balance\",\n       y=\"predicition\")+\n    scale_color_viridis(discrete = TRUE)+\n  theme_minimal()\n\nplot_balance \n\n\n\n\n\n\n\n\nIt is what would be expected. If all the other predictors hold constant and we have only one equally distributed predictor, we have an S-shaped curve. That is because we have a categorical outcome variable. At one threshold, the probability of default changes.\n\nCreate a confusion matrix just as the one for the KNN models by using a cutoff predicted probability of 0.5. Does logistic regression perform better?\n\n\n pred_prob &lt;- predict(lr_mod, newdata = default_valid, type = \"response\")\npred_lr   &lt;- factor(pred_prob &gt; .5, labels = c(\"No\", \"Yes\"))\n\nconf_logreg &lt;- table(predicted = pred_lr, true = default_valid$default)\nconf_logreg\n\n         true\npredicted   No  Yes\n      No  1923   47\n      Yes   10   19\n\n\n\nlogistic regression performs better in every way than knn. This depends on\nyour random split so your mileage may vary\n\n\nCalculate the specificity, sensitivity, accuracy and the precision for the logistic regression using the above confusion matrix. Again, compare the logistic regression to KNN.\n\n\nspec_logreg &lt;- conf_logreg[1,1] / (conf_logreg[1,1] + conf_logreg[2,1])\nspec_logreg\n\n[1] 0.9948267\n\n\n\nsens_logreg &lt;- conf_logreg[2,2] / (conf_logreg[2,2] + conf_logreg[1,2])\nsens_logreg\n\n[1] 0.2878788\n\n\n\nacc_logreg &lt;- (conf_logreg[1,1] + conf_logreg[2,2]) / sum(conf_logreg)\nacc_logreg\n\n[1] 0.9714857\n\n\n\nprec_logreg &lt;- conf_logreg[2,2] / (conf_logreg[2,2] + conf_logreg[2,1])\nprec_logreg\n\n[1] 0.6551724\n\n\nNow we can very clearly see that logisitc regression performs a lot better compared to KNN, especially the increase in precision is impressive!\n\n5.7.5.1 Final exercise\nNow let’s do another - slightly less guided - round of KNN and/or logistic regression on a new dataset in order to predict the outcome for a specific case. We will use the Titanic dataset also discussed in the lecture. The data can be found in the /data folder of your project. Before creating a model, explore the data, for example by using summary().\n\nlibrary(readr)\ntitanic &lt;- read.csv(\"data/Titanic.csv\")\ntitanic &lt;- as_tibble(titanic)\nstr(titanic)\n\ntibble [1,313 × 5] (S3: tbl_df/tbl/data.frame)\n $ Name    : chr [1:1313] \"Allen, Miss Elisabeth Walton\" \"Allison, Miss Helen Loraine\" \"Allison, Mr Hudson Joshua Creighton\" \"Allison, Mrs Hudson JC (Bessie Waldo Daniels)\" ...\n $ PClass  : chr [1:1313] \"1st\" \"1st\" \"1st\" \"1st\" ...\n $ Age     : num [1:1313] 29 2 30 25 0.92 47 63 39 58 71 ...\n $ Sex     : chr [1:1313] \"female\" \"female\" \"male\" \"female\" ...\n $ Survived: int [1:1313] 1 0 0 0 1 1 1 0 1 0 ...\n\n\n\nCreate a model (using knn or logistic regression) to predict whether a 14 year old boy from the 3rd class would have survived the Titanic disaster.\n\n\n# define the training partition  ~ 80 percent\ntrain_index &lt;- createDataPartition(titanic$Survived, p = .8, \n                                  list = FALSE, \n                                  times = 1)\n\n# split the data using the training partition to obtain training data   ~ 80 percent\nsurvived_train &lt;- titanic[train_index,]\n\n# define the valid set   ~ 20 percent\nsurvived_valid &lt;- titanic[-train_index,]\n\n\nlr_tit &lt;- glm(Survived ~ PClass + Age + Sex, data= survived_train, family= binomial)\nsummary(lr_tit)\n\n\nCall:\nglm(formula = Survived ~ PClass + Age + Sex, family = binomial, \n    data = survived_train)\n\nCoefficients:\n             Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)  3.848086   0.451679   8.520 &lt; 0.0000000000000002 ***\nPClass2nd   -1.478496   0.300063  -4.927          0.000000834 ***\nPClass3rd   -2.578366   0.313073  -8.236 &lt; 0.0000000000000002 ***\nAge         -0.036480   0.008555  -4.264          0.000020048 ***\nSexmale     -2.815392   0.233197 -12.073 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 811.17  on 598  degrees of freedom\nResidual deviance: 527.37  on 594  degrees of freedom\n  (452 observations deleted due to missingness)\nAIC: 537.37\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npredict(lr_tit, \n        newdata = tibble(\n          PClass = c( \"3rd\",    \"2nd\"),\n          Age    = c(    14,       14), \n          Sex    = c(\"male\", \"female\")\n        ), \n        type = \"response\"\n)\n\n        1         2 \n0.1134086 0.8651658 \n\n\n\nWould the passenger have survived if they were a 14 year old girl in 2nd class?\n\n\nSo our hypothetical passenger does not have a large survival probability:\nour model would classify the boy as not surviving. The girl would likely survive however. This is due to the women and children getting preferred access to the lifeboats. Also 3rd class was way below deck.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>classification</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html",
    "href": "beyond-linearity.html",
    "title": "6  Beyond linearity",
    "section": "",
    "text": "6.1 Readings\nISLR\nChapter 7: Beyond linearity",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#why-extend-the-linear-model",
    "href": "beyond-linearity.html#why-extend-the-linear-model",
    "title": "6  Beyond linearity",
    "section": "6.2 Why extend the linear model?",
    "text": "6.2 Why extend the linear model?\n\nMore flexibility, while still being able to interpret the outcomes of the model.\nIn (even) more flexible models (such as Random Forests, will be discussed in two weeks) interpretability is limited\n\n\n\n\neasy to code, only one line!\nPolynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables, X, X2, and X3, as predictors. This approach provides a simple way to provide a nonlinear fit to data.\nStep functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\nRegression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit. Smoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\nLocal regression (LOESS) is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\nGeneralized additive models allow us to extend the methods above to deal with multiple predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#basic-functions-and-linear-regression",
    "href": "beyond-linearity.html#basic-functions-and-linear-regression",
    "title": "6  Beyond linearity",
    "section": "6.3 Basic functions and linear regression",
    "text": "6.3 Basic functions and linear regression\nGeneral idea:\n\nWe have a non-linear relationship between X and Y.\nWe replace X by \\(b_1(x), b_2(x)\\) etc., choosing the \\(b_j()\\) so the relationship between \\(y\\) and \\(b_j(x)\\) is linear\nwe fit our linear regression \\[\nf(x) = \\beta_0 b_1(x) + \\beta_1 b_2(x) + \\beta2 b_j(x)\n\\]\nwe can get the \\(\\beta\\) s estimate standard errors etc.\n\nThe basis function \\(b()\\) are fixed and known. Linear regression is a special case where \\(b_0(x) = 1\\) and \\(b_1(x) = x\\) because the intercept \\(\\beta_0\\) does not interact with \\(x\\) and we only have one \\(x\\) - Linear regression:\n\\[\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#polynomial-regression",
    "href": "beyond-linearity.html#polynomial-regression",
    "title": "6  Beyond linearity",
    "section": "6.4 polynomial regression",
    "text": "6.4 polynomial regression\n\\[\ny = \\beta_0 + \\beta_1x  +  \\beta_2x^2 + \\dots + \\beta_d^d  + \\epsilon\n\\]\n\\(b_0(x) =1\\) is the intercept\n\\(d\\) stands for the degree of freedom (quantity that summarizes the flexibility of a curve) - for polynomial regression usually not greater than 3 or 4, because then it would be too flexible or use cross-validation to choose . - for each extra degree in the polynomials, the linear prediction line is ‘allowed’ to make one extra bend.\n\n\n6.4.1 Interpretations of coefficients\n\nTypically, we are not so interested in the values of the regression coefficients \\(\\beta_0\\), but more in: The predicted outcome for each value of \\(x_i\\) (i.e., the fitted function values) How much increases when we increase \\(x\\) (i.e., the marginal effect)\n\nTherefore, marginal effect plot\n\nThe plot shows, how the rate of the curve changes with increasing x.\nin linear models, the marginal effect stays the same, because the gradient is always the same\nwith each step in fixed_acidity / in \\(x\\), the outcome will decrease by -0-06-\n\n\n\nin quadric regression:\nWith every extra point on fixed acidity, the pH is predicted to go down with 0.18 points.\nHowever, as fixed acidity increases, the negative effect levels off with 0.01 points per point on fixed \\(acidity^2\\)\nWith every extra point on fixed acidity, the decrease on pH depends on the level of fixed acidity\nThe marginal effect is the derivative (Ableitung) of f(x) (i.e. the slope of the curve at each point)\n\n\n\ncubic regression:\nWith a fixed acidity of 0, the predicted pH equals 5.50\nWith every extra point on fixed acidity, the pH is predicted to go down with 0.57 points\nThis negative effect levels off with 0.05 points per point on fixed \\(acidity^2\\)\nAs acidity increases, the ‘levelling off’ effect is negated with 0.001 points per point on fixed \\(acidity^3\\)\n\n\nExample: how to interpret?\n\n\n\nat 10 the second estimated parameter works, so for an increase in fixed_acidity in one, the estimate is 0.049\nfor 0.01 you have then 0.0049\n\n\n\n6.4.2 use also for logistic regression\n\\[\nPr(y_i = 1| X) = \\frac{e_{\\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\dots + \\beta_dx_i^d}} { 1+e_{\\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\dots + \\beta_dx_i^d}}\n\\]\n\nin R\npoly() function, to specify your polynomial inside a formula for a linear (or logistic) regression as such:\nHere three degree polynomial\nmod &lt;- lm(pH ~ poly(fixed_acidity, degree = 3), data = winequality_red)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#interpretation-of-coefficients",
    "href": "beyond-linearity.html#interpretation-of-coefficients",
    "title": "6  Beyond linearity",
    "section": "8.1 Interpretation of coefficients",
    "text": "8.1 Interpretation of coefficients\nFor example:\nst2_logb &lt;- lm(pH ~ I(fixed_acidity &gt; 6 & fixed_acidity &lt; 8) + I(fixed_acidity &gt;= 8), data = winequality\n\n\n6 and 8 are chosen as thresholds (In real practice, you do not choose 6 and 8 by cut the curve by certain steps, like every 3 points in X I compute another estimate)\nfor example, if I want to predict for X=12, I use this estimate! Because the threshold here is &gt;= 8. If I want to predict 7, I use the 0.2 estimate.\nInterpretation of the coefficients: - Intercept: Value of before the first knot - I(fixed_acidity &gt; 6 & fixed_acidity &lt; 8): Value of between the first and second knots (in relationship to the intercept) - I(fixed_acidity &gt;= 8): Value of after the second knot (in relationship to the intercept)\n\n\nin this model accumulated!\nif you have model like this, we have estimates that accumulate! So for X=12 we have to use -021 and -0.15\nEach function gets activated at a different break, adding to the previous.\n\n\n8.1.1 Advatages & Disadvantages\nAdvantages: - Simple - Popular in epedemiology and biostatistics - Useful when there are natural cutpoints that are of interest (e.g., students under and over 18) - Interpretation is easy: the coefficients show the predicted value - They are local: in step functions, regression coefficients only influence outcomes in a specific region, while with polynomials the regression coefficients influcence the predicted outcome for all ranges of\nDisadvantages: - They are not continuous\nin R\nNumber of breaks (or and where to cut is decided by the user.\nIn R: cut(predictor, c(breaks))\nWhen the location of the breaks is not specified, the breaks are spaced evenly\n#use cut\npw2_mod &lt;- lm(pH ~ cut(fixed_acidity, breaks = c(0,6,8,14)), data = winequality_red) \n\n#use I\nnew_mod &lt;- lm(pH ~ I(fixed_acidity &gt; 6) + I(fixed_acidity &gt; 8), data = winequality_red)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#dicontinuous-piecewise-polynomials",
    "href": "beyond-linearity.html#dicontinuous-piecewise-polynomials",
    "title": "6  Beyond linearity",
    "section": "8.2 Dicontinuous piecewise polynomials",
    "text": "8.2 Dicontinuous piecewise polynomials\n\nCombination of polynomial regression and step function:\nWe fit separate low-degree polynomials over different regions of X, for example:\n\n\n\nparameters \\((d+1)(K+1))\\) where K= numbers of knots / breaks k\nif you have one break, you have two number of polynomials\nusing more knots means more flexible piecewise polynomial functions, because we subset the data in \\(x_i &lt; \\xi\\) and in \\(x_i &gt;= \\xi\\)\nthe polinomials are indicated by the second number, so \\(\\beta_01\\) is the intercept of the first regression line, \\(\\beta_02\\) is the intercept for the second regression line. So we have two cubic regression lines with each 3 degrees of freedom that are merged into one.\nIf maximum degree is 1 = we fit linear functions\nIf maximum degree is 3 = cubic polynomials\n\n\n\n8.2.1 Advatages & Disadvantages\nAdvantage: more flexible than step functions\nDisadvantage: still discontinous\nSolution: Splines",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#splines",
    "href": "beyond-linearity.html#splines",
    "title": "6  Beyond linearity",
    "section": "8.3 Splines",
    "text": "8.3 Splines\nWe add constraints to the former piecewise polynomials: 1. Splines have a maximum amount of continuity: The \\(d- 1\\) derivatives are constrained to be continuous at the knot as well. For example, if we use a cubic function \\(( d= 3)\\), we force the \\(1\\) and \\(2\\) derivative to be continuous as well → very important! 2. This means that cubic splines are not only continuous, but also have the same slope (1 derivative) and curvature (2 derivative)\nThe following image shows, why the spline is the best solution:\n\n\npiecewise cubic is not continuous, we have two intercepts\nremoving this intercept, in fact we have a continuous curve, but it looks not very natural\nin rhe cubic spline, we have added the two constraints: at the threshold ‘age=50’ both derivates are continuous and the curve is also very smooth!\nin doing this, we restrict the number of degrees of freedom: Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, by reducing the complexity of the resulting piecewise polynomial fit. So in the top left plot, we are using eight degrees of freedom, but in the bottom left plot we imposed three constraints (continuity, continuity of the first derivative, and continuity of the second derivative)\nIn general, a cubic spline with K knots usesa total of 4 + K degrees of freedom.\nin the right lower panel we have a linear spline, which is also continuous at ‘age=50’\nThe general definition of a degree-d spline is that it is a piecewise degree-d polynomial, with continuity in derivatives up to degree d − 1 at each knot. Therefore, a linear spline is obtained by fitting a line in each region of the predictor space defined by the knots, requiring continuity at each knot.\n\nFore splines we use a truncated basis function.\n\n\nwe estimate two different regression lines, seperated by the threshold \\(\\xi\\)\nfirst linear regression use all observations, then add a second regression to it (like in the stepwise) at a certain threshold \\(\\xi\\). Problem now are the breaks!\ntherefore, we restrict the number of \\(d\\) in performing a least squares regression with an intercept and \\(d+K\\) predictors to have the form of \\(X, X^2, X^3\\), \\(h(X,\\xi_1),h(X,\\xi_2), \\dots ..., h(X,\\xi_3)\\), where \\(\\xi\\) are the knots\nso we have a formula\nfor a cubic spline we have then 4 degrees of freedom, because the intercept is also one: \\(b_0(x) = Intercept 1, b_1(x)= x, b_2(x)= x^2,  b_3(x)=x^3\\) where it is \\(b_{d+k}(x) = (x-\\xi_k)_+^d for k=1, \\dots, K\\)\n\n\\[\nf(x) = \\beta_0 + \\beta_1x_i + \\beta_2(x_i-\\xi_k)\n\\] Very common to use the cubic splines, because 1. & 2. derivate are smooth, continuous, same slope, same curvature (Krümmung, Wölbung) and going past cubic dies not get much scmoother-looking curves\nWhat happens at the treshold to make the spline continuous and smooth?\n\\[\n\\begin{align*}\nf(x) & = \\beta_0 + \\beta_1\\xi \\\\\nBefore  &  \\xi  (e.g. at \\xi - \\delta) \\\\\nf(x)  & = \\beta_0 + \\beta_1\\xi - \\beta_1\\delta \\\\\nAfter &  \\xi  (e.g. at \\xi + \\delta) \\\\\nf(x)  & = \\beta_0 + \\beta_1\\xi - (\\beta_1 + \\beta_2)\\delta \\\\\n\\end{align*}\n\\]\nTruncaded power basis function\n\n\n8.3.1 Advatages & Disadvantages\nPro: Continuous and smooth!\nCon: Prediction outside the range of the data can be wild\nin R\nlibrary(splines)\nbs3_l_mod &lt;- lm(pH ~ bs(fixed_acidity, knots = c(6,8)), data = winequality_red)\nbs3_l_log &lt;- glm(Survived ~ bs(Age, knots = c(18, 64)), data = titanic, family = 'binomial')\n(pred_plot(bs3_l_mod ) + ggtitle(\"cubic spline for wine data, 2 knots\") +\npred_plot2(bs3_l_log ) + ggtitle(\"cubic spline for Titanic data, 2 knots\"))\n\nMarginal effects:\nbs3_l_mod &lt;- lm(pH ~ bs(fixed_acidity, knots = median(fixed_acidity)), data = winequality_red)\n\n\n\n8.3.2 Constraining parameters further: natural splines\nUnfortunately, splines can have high variance at the outer range of the predictors—that is, when X takes on either a very small or very large value. Figure 7.4 shows a fit to the Wage data with three knots. We see that the confidence bands in the boundary region appear fairly wild. A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is spline smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries. In Figure 7.4, a natural cubic spline is also displayed as a red line. Note that the corresponding confidence intervals are narrower.\n\n\nextrapolates linearly boundary knots\nbeyond the range of the data, the function goes off linerarly\nThis adds extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline.\nParameters \\((d+1) + K-2\\) (two constrains in the boundaries) → removing degrees of freedom\n\nin R\nUsing ‘splines’ and ‘ns(x, …)’\nns3_l_mo &lt;- lm(pH ~ ns(fixed_acidity, knots = c(6, 8), data = winequality_red)\nns3_l_log &lt;- glm(Survived ~ ns(`Age`, knots = c(18, 64)), data = titanic, family = 'binomial')\n\n\nwe de not have the decrease at the end. Make it more linear, what is good because ends are often wild.\n\n→ to use, when the test data set gets out of the range of observations and are kind of outliers. → more simple models, less parameters!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#choosing-k-and-the-placements-of-knots",
    "href": "beyond-linearity.html#choosing-k-and-the-placements-of-knots",
    "title": "6  Beyond linearity",
    "section": "8.4 Choosing K and the placements of knots",
    "text": "8.4 Choosing K and the placements of knots\nVarious strategies:\n\nGiven the theoretical framework of the data at hand, there are logical, natural places for the knots\nGiven the data, place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. Very data dependent!\nMore common: place knots in a uniform fashion. That is:\nDecide on \\(K\\) (the number of knots) and place them at the quantiles of the observed \\(X\\). For example, if we choose 1 knot, we can place it at the median of \\(X\\)\nDeciding on \\(K\\) : use cross-validation\nin splines is more about making predictions, than to explain and estimate exact parameters, so more parameters do not harm! Add all!\n\n\n8.4.1 Smoothing splines\nEven more advanced method, for fitting splines without having to worry about knots. - Basic idea similar to Ridge regression: - add a roughness penalty: Minimize \\(MSE + \\lambda penalty\\) - Penalty is \\(\\int f''(r)^2\\): Sum of change in the curvature\n\nIn fitting a smooth curve to a set of data, what we really want to do is find some function, say g(x), that fits the observed data well: that is, we want RSS/ MSE to be small. However, there is a problem with this approach. If we don’t put any constraints on g(xi), then we can always make RSS zero simply by choosing g such that it interpolates all of the yi. Such a function would woefully overfit the data—it would be far too flexible. What we really want is a function g that makes RSS small, but that is also smooth. How might we ensure that g is smooth? There are a number of ways to do this. A natural approach is to find the function g that minimizes the MSE, where \\(\\lambda\\) is the nonnegative tuning parameter.\n\nHow many knots? One for each \\(x_i\\), controll smoothness through penalty\n\\(f(x)\\) ends up being a piecewise cubic polynomial and continuous first and second derivatives\nthe smaller \\(\\lambda\\) the more wiggly (wackelig, sich schlangend) the function, \\(\\lambda\\) → \\(\\infty\\) \\(g(x)\\) becomes linear\n\n\n8.4.1.1 Advantages\n\nNo need to define the number of knots\n\\(\\lambda\\) can be calculated easily due to some math magic without the need of cross-validation (i.e. faster)\nLess “effective degrees of freedom” (parameters)#\n\nin R\n# Cross validation to find best lambda\nss_mod1 &lt;- smooth.spline(winequality_red$fixed_acidity, winequality_red$pH, cv = TRUE)\n# Effective degrees of freedom\nss_mod1$df\n## [1] 10.61691\n#Fit linear model\nss_mod &lt;- gam(pH ~ s(fixed_acidity, df = 10.6), data = winequality_red)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#local-regression",
    "href": "beyond-linearity.html#local-regression",
    "title": "6  Beyond linearity",
    "section": "8.5 Local regression",
    "text": "8.5 Local regression\n\nmainly used to predict trends\nThe idea of local regression can be generalized in many different ways. In a setting with multiple features X1,X2, . . . ,Xp, one very useful generalization involves fitting a multiple linear regression model that is global some variables, but local in another, such as time. Such varying coefficient models are a useful way of adapting a model to the most recently gathered data.\nLocal regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 using only the nearby training observations.\nIn this figure the blue line represents the function \\(f(x)\\) from which the data were generated, and the light orange line corresponds to the local regression estimate \\(\\hat{f}(x)\\).\n\n\nNote that in Step 3 of Algorithm 7.1, the weights Ki0 will differ for each value of x0. In other words, in order to obtain the local regression fit at a new point, we need to fit a new weighted least squares regression model by minimizing (7.14) for a new set of weights. Local regression is sometimes referred to as a memory-based procedure, because like nearest-neighbors, we need all the training data each time we wish to compute a prediction. In order to perform local regression, there are a number of choices to be made, such as how to define the weighting function K, and whether to fit a linear, constant, or quadratic regression in Step 3. (Equation 7.14 corresponds to a linear regression.) While all of these choices make some difference, the most important choice is the span s, which is the proportion of points used to compute the local regression at x0, as defined in Step 1 above. The span plays a role like that of the tuning parameter λ in smoothing splines: it controls the flexibility of the non-linear fit. The smaller the value of s, the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations. → crossvalidation to choose s or direct specification.\nin R\nuse the funcion loess",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#generalized-additive-models-gam",
    "href": "beyond-linearity.html#generalized-additive-models-gam",
    "title": "6  Beyond linearity",
    "section": "8.6 Generalized additive models (GAM)",
    "text": "8.6 Generalized additive models (GAM)\nGeneralized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. Just like linear models, GAMs can be applied with both quantitative and qualitative responses.\nUse & Pros:\n\nWhen using the above methods on a multiple regression (i.e., more than 1 predictor) problem, we can extend them using GAM (generalized additive models)\nAllows for flexible nonlinearities in several variables\nAddition of smooth functions\nquite easy to use\nalso possible to mic methods over different predictors within the same model, e.g. a model that is global on some variables, - local in other (e.g. time)\n\nCons:\nThe main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form Xj × Xk. In addition we can add low-dimensional interaction functions of the form fjk(Xj,Xk) into the model; such terms can be fit using two-dimensional smoothers such as local regression, or two-dimensional splines (not covered here)\nin R\nlibrary(gam)\n\n#Example 1: more than one natural spline\nR: lm(wage ∼ ns(year, df = 5) + ns(age, df = 5) + education)\n\n# Example 2: different methods\nm(wage ∼ ns(year, df = 5) + lo(age, span = 0.5) + education)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "beyond-linearity.html#in-r",
    "href": "beyond-linearity.html#in-r",
    "title": "6  Beyond linearity",
    "section": "8.7 in R",
    "text": "8.7 in R\nIn this lab, we will learn about nonlinear extensions to regression using basis functions and how to create, visualise, and interpret them. Parts of it are adapted from the labs in ISLR chapter 7. You can download the student zip including all needed files for lab 6 here.\nOne of the packages we are going to use is splines. For this, you will probably need to install.packages(\"splines\") before running the library() functions, however depending on when you installed R, it may be a base function within your library.\nAdditionally this lab will use lots of functions from the tidyverse package dplyr. Therefore, if you are struggling with there usage, please review the guidance in the dplyr basics tab.\n\nlibrary(MASS)\nlibrary(splines)\nlibrary(ISLR)\nlibrary(tidyverse)\nlibrary(gridExtra)\n\nMedian housing prices in Boston do not have a linear relation with the proportion of low SES households. Today we are going to focus exclusively on prediction.\n\nBoston %&gt;% \n  ggplot(aes(x = lstat, y = medv)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFirst, we need a way of visualising the predictions. The below function pred_plot() does this for us: the function pred_plot() takes as input an lm object, which outputs the above plot but with a prediction line generated from the model object using the predict() method.\n\nIdentify what each line of code does within the function pred_plot(). Next, using # annotate what each line does within the function.\n\nhint: You can use the function print() to output and investigate what lines within the function are doing.\n\npred_plot &lt;- function(model) { #defines the function, needs model as an argument\n  x_pred &lt;- seq(min(Boston$lstat), max(Boston$lstat), length.out = 500) # set the x value for pred from the lowest lstat value to the highest. We now get a vector with 500 values from the lowest lstat to the highest. The numbers are generated randomly, so we have simulated data.\n  y_pred &lt;- predict(model, newdata = tibble(lstat = x_pred)) #we predict the numeric outcome for y based on model we have defined and use for the prediction the randomly generated data x_pred\n  \n  Boston %&gt;% #we set the data that we want to use\n    ggplot(aes(x = lstat, y = medv)) + #we define the asthetics and map them\n    geom_point(alpha= 0.7, color= \"steelblue4\") + #we want a scatterplot, so we define the associated geom\n    geom_line(data = tibble(lstat = x_pred, medv = y_pred), size = 1, col = \"purple\") + #we add a line for the predicted y on the predicted x data\n    theme_minimal() # we define a nice theme\n}\n\n\nCreate a linear regression object called lin_mod which models medv as a function of lstat. Check if the prediction plot works by running pred_plot(lin_mod). Do you see anything out of the ordinary with the predictions?\n\n\nlin_mod &lt;- lm(medv ~ lstat, data= Boston)\n\n\npred_plot(lin_mod)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWe can see that the predictions do not perform well for the low SES households and the very high SES households. Further, the line seems to be too high in the middle, it seems like it is pushed up because of the many observations with low lstat and high medv.\n→ linear regression underfits the data, is too simple to grasp. Too less variance explained\n\n8.7.1 Polynomial regression\nThe first extension to linear regression is polynomial regression, with basis functions \\(\\beta_{j}(x_{i}^{j})\\) (ISLR, p. 270).\n\nCreate another linear model pn3_mod, where you add the second and third-degree polynomial terms I(lstat^2) and I(lstat^3) to the formula. Create a pred_plot() with this model.\n\n\npn3_mod &lt;- lm(medv ~ poly(lstat,3), data= Boston) #in this case linear combination of the variables lstat and the polynoms\ncoef(pn3_mod)\n\n    (Intercept) poly(lstat, 3)1 poly(lstat, 3)2 poly(lstat, 3)3 \n       22.53281      -152.45955        64.22724       -27.05110 \n\n\nor the other way with the wrapper function I()\n\npn3_mod2 &lt;- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data= Boston)\ncoef(pn3_mod2)\n\n (Intercept)        lstat   I(lstat^2)   I(lstat^3) \n48.649625342 -3.865592779  0.148738477 -0.002003868 \n\n\nThe function poly() can automatically generate a matrix which contains columns with polynomial basis function outputs.\n\nplot_raw &lt;- pred_plot(pn3_mod2)\nplot_raw\n\n\n\n\n\n\n\n\n\nPlay around with the poly() function. What output does it generate with the arguments degree = 3 and raw = TRUE?\n\n\npn3_mod.t &lt;- lm(medv ~ poly(lstat,3, raw= TRUE), data= Boston) #in this case linear\ncoef(pn3_mod.t) #in this case we obtain the variables directly, so equal to the selve written function\n\n                (Intercept) poly(lstat, 3, raw = TRUE)1 \n               48.649625342                -3.865592779 \npoly(lstat, 3, raw = TRUE)2 poly(lstat, 3, raw = TRUE)3 \n                0.148738477                -0.002003868 \n\n\nPlay aroung with poly\n\npoly(1:5, degree= 3, raw=TRUE)\n\n     1  2   3\n[1,] 1  1   1\n[2,] 2  4   8\n[3,] 3  9  27\n[4,] 4 16  64\n[5,] 5 25 125\nattr(,\"degree\")\n[1] 1 2 3\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nPoly compute the x, x^2 and x^3 for all numbers from 5.\n\nUse the poly() function directly in the model formula to create a 3rd-degree polynomial regression predicting medv using lstat. Compare the prediction plot to the previous prediction plot you made. What happens if you change the poly() function to raw = FALSE?\n\n\nplot_notraw &lt;-pred_plot(pn3_mod)\ngrid.arrange(plot_raw, plot_notraw, ncol=2)\n\n\n\n\n\n\n\n\nLooks the same for me.\n\n\n8.7.2 Piecewise constant (Step function)\nAnother basis function we can use is a step function. For example, we can split the lstat variable into two groups based on its median and take the average of these groups to predict medv.\n\nCreate a model called pw2_mod with one predictor: I(lstat &lt;= median(lstat)). Create a pred_plot with this model. Use the coefficients in coef(pw2_mod) to find out what the predicted value for a low-lstat neighbourhood is.\n\n\npn2_mod &lt;- lm(medv ~ I(lstat &lt;= median (lstat)), data= Boston)\npred_plot(pn2_mod)\n\n\n\n\n\n\n\n\n\ncoef(pn2_mod)\n\n                  (Intercept) I(lstat &lt;= median(lstat))TRUE \n                     16.67747                      11.71067 \n\n\nThe predicted value for a low-stat neighborhood is .\n\nUse the cut() function in the formula to generate a piecewise regression model called pw5_mod that contains 5 equally spaced sections. Again, plot the result using pred_plot.\n\n\npw5_mod &lt;- lm(medv ~ cut(lstat, breaks = 5), data= Boston)\npred_plot(pw5_mod)\n\n\n\n\n\n\n\n\nNote that the sections generated by cut() are equally spaced in terms of lstat, but they do not have equal amounts of data. In fact, the last section has only 9 data points to work with:\n\ntable(cut(Boston$lstat, 5))\n\n\n(1.69,8.98] (8.98,16.2] (16.2,23.5] (23.5,30.7]   (30.7,38] \n        183         183          94          37           9 \n\n\n\nOptional: Create a piecewise regression model pwq_mod where the sections are not equally spaced, but have equal amounts of training data. Hint: use the quantile() function.\n\n\nbreaks &lt;- quantile(Boston$lstat, probs= c(0.2, 0.4, 0.6, 0.8, 1))\n\nbreaks &lt;- c(-Inf, breaks, Inf)\nbreaks\n\n        20%   40%   60%   80%  100%       \n -Inf  6.29  9.53 13.33 18.06 37.97   Inf \n\npwq_mod &lt;- lm(medv ~  cut(lstat, breaks), data= Boston)\npred_plot(pwq_mod)\n\n\n\n\n\n\n\n\n\ncoef(pwq_mod)\n\n                  (Intercept) cut(lstat, breaks)(6.29,9.53] \n                    34.089216                     -8.324859 \ncut(lstat, breaks)(9.53,13.3] cut(lstat, breaks)(13.3,18.1] \n                   -12.771394                    -16.066443 \n  cut(lstat, breaks)(18.1,38] \n                   -20.733770 \n\n\nHow to interpret?\nFor the area where the amount of people having a low status is 20 % the estimate is 34.089. For the lowest 40 % it is 34.089 + (-8.32), and for 34.089 + (- 12.77) and so on.\n\n\n8.7.3 Splines\nUsing splines, we can combine polyniomials with step functions (as is done in piecewise polynomical regression, see ISLR and the lecture) AND take out the discontinuities.\nThe bs() function from the splines package does all the work for us.\n\nCreate a cubic spline model bs1_mod with a knot at the median using the bs() function.\n\nhint: If you are not sure how to use the function bs(), check out the first example at the bottom of the help file by using ?bs.\n\n?bs\nbs1_mod &lt;- lm(medv ~ bs(lstat, knots=median (lstat)), data=Boston)\n\n\nCreate a prediction plot from the bs1_mod object using the plot_pred() function.\n\n\nbs1 &lt;- pred_plot(bs1_mod)\n\nNote that this line fits very well, but at the right end of the plot, the curve slopes up. Theoretically, this is unexpected – always pay attention to which predictions you are making and whether that behaviour is in line with your expectations.\nThe last extension we will look at is the natural spline. This works in the same way as the cubic spline, with the additional constraint that the function is required to be linear at the boundaries. The ns() function from the splines package is for generating the basis representation for a natural spline.\n\nCreate a natural cubic spline model (ns3_mod) with 3 degrees of freedom using the ns() function. Plot it, and compare it to the bs1_mod.\n\n\n?ns\nns3_mod &lt;- lm(medv ~ ns(lstat, knots = median (lstat)), data=Boston)\nns1 &lt;- pred_plot(ns3_mod)\n\n\nlibrary(gridExtra)\ngrid.arrange(bs1, ns1, ncol= 2)\n\n\n\n\n\n\n\n\n\nPlot lin_mod, pn3_mod, pw5_mod, bs1_mod, and ns3_mod and give them nice titles by adding + ggtitle(\"My title\") to the plot. You may use the function plot_grid() from the package cowplot to put your plots in a grid.\n\n\n#install.packages(\"cowplot\")\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlin_plot &lt;- pred_plot(lin_mod)\nlin_plot &lt;- lin_plot + ggtitle(\"Linear Model\")\nlin_plot\n\n\n\n\n\n\n\npn3_plot &lt;- pred_plot(pn3_mod) + ggtitle(\"third-degree polynomial\")\npw5_plot &lt;- pred_plot(pw5_mod) + ggtitle(\"piecewise regression\") + theme(title = element_text(size=11))\nbs1 &lt;- bs1 + ggtitle(\"splines\")\nns1 &lt;- ns1 + ggtitle(\"natural splines\")\n?plot_grid\nplot_grid(lin_plot,pn3_plot, pw5_plot,bs1, ns1, nrow=2)\n\n\n\n\n\n\n\n\n``` ## Conclusions\n\nIf we are interested in accurate predictions: Splines are preferred over polynomial\nThey are more flexible than polynomial and local (variance is often lower)\nBut it comes at a cost (interpretability)\nCubic splines are often all you need\nAll the methods we have seen so far are additive",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond linearity</span>"
    ]
  },
  {
    "objectID": "tree-based.html",
    "href": "tree-based.html",
    "title": "7  Tree-based Methods",
    "section": "",
    "text": "7.1 Readings\nISLR:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#readings",
    "href": "tree-based.html#readings",
    "title": "7  Tree-based Methods",
    "section": "",
    "text": "Chapter 8: Tree-based models",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#introduction",
    "href": "tree-based.html#introduction",
    "title": "7  Tree-based Methods",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nLecturer: Ayoub Bagheri\nBasic vocabulary\n\ncan applied to both, regression and classification\npredictor space is stratified or segmented into a number of simple regions → in order to make a prediction for a given observation, we typically use the mean of the mode response value for the training observations in the region to which it belongs\nroot of the tree / root node is the starting point, it is the first decision, which is defined in a tree → the decisions or better predictors with a certain threshold are the referred as nodes in a tree (also known as leaves)\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nhas a certain threshold, if the mean response value is higher than that threshold, the observations are assigned to one branch, if its lower, they are assigned to the other branch. Always only two branches (yes, no), binary decisions!\nthen, the subgroup, referred to as region\ntwo kinds of trees\ndecision trees: a complex decision is broken down in different criteria, to advise a decision. For example buying a car is a complex decision, can be parsed into: Have you tested the car on road? How many miles has been already driven with that car? How old is the car? Is the car environment friendly? And so on.\nprediction trees: Would you survive the titanic? Parse a complex decision into certain criteria?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#growing-decision-trees-from-data",
    "href": "tree-based.html#growing-decision-trees-from-data",
    "title": "7  Tree-based Methods",
    "section": "7.3 Growing decision trees from data",
    "text": "7.3 Growing decision trees from data\nGoal is to find regions, that minize the RSS.\nKey question: how can we find the best split?\nRecursive (binary) partitioning:\n\nFind the split that makes observations as similar as possible on the outcome within that split\n\n\nWhich feature should I use for splitting? Is this split the best? We do that for each feature. If we found our best split, we apply it and then we asked what is now the best split for the subgroup → stratify the observations in smaller and smaller regions, so that they become as homogeneous as possible\nwe divide the predictor space, the set of possible values for \\(X_1, X_2, \\dots, X_p\\) into \\(J\\)j distinct and non-overlapping regions for \\(R_1, R_2, \\dots, R_J\\). Then, for every observation that falls into the Region \\(R_j\\) we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\). The goal is to find boxes \\(R_1, R_2, \\dots, R_J\\) that minimize the RSS.\nUnfortunately, it is computationally unfeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting\nIn order to perform recursive binary splitting, we first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into regions \\({X|X_j &lt; s}\\) and \\({X|X_j &gt;= s}\\) leads to the greatest possible reduction in RSS.\n\n\ndo the first step in each resulting group over and over again.\n\nEarly stopping: Stratifying in too small groups is an overfitting:\n\nwe can implement stop points for splitting, so if a certain threshold is reached, we stop splitting.\nso stop, when fewer than \\(n_{min}\\) observations in the group (typically 10)\n\n\n7.3.1 Tree-building: top-down and greedy\nRecursive (binary) partitioning is a top-down and greedy algorithm:\n\ntop-down: algorithm begins at the top of the tree with all observations and then successively splits the predictor space. Each split is indicated via two new branches further down on the tree.\ngreedy (gefräßig, gierig): at each step, the best split for that step is made, instead of looking ahead and picking a split that will result in the best tree in a future step. At each split we take the best split possible, we do not look, which splits overall is the best! → so greedy method",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#regression-trees",
    "href": "tree-based.html#regression-trees",
    "title": "7  Tree-based Methods",
    "section": "7.4 Regression trees",
    "text": "7.4 Regression trees\nThe outcome is continuous, is a numerical. Instead of predicting class label in each box(partition), we predict the outcome in each partition:\n\nmean of the training observations in the partition, to which the test observations belong\ncutploints are selected such that splitting the predictor space into the reions leads to the greatest possible reduction in residual sums of squares (RSS)\n\nExample:\nWine quality, quality measured on a scale\n\nOutcome is a numerical. We start with alcohol as a root node and 100 % of the observations. At a certain threshold we divide the observations into two regions, the yes branch, if alcohol is &lt; 11 and the no one, if alcohol is &gt;=11. Then, the mean value for the divided regions is computed. Is For the one &lt;11, the wine quality is 5.4, where the other group has a quite higher quality with 6.1 and contains 39% of the observations. So instead of yes / no we have numbers as an outcome.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#cost-complexity-pruning",
    "href": "tree-based.html#cost-complexity-pruning",
    "title": "7  Tree-based Methods",
    "section": "7.5 Cost complexity Pruning",
    "text": "7.5 Cost complexity Pruning\nProcess described most likely overfits the data → poor test performance\nSolution 1: build the tree until the decrease in classification error / RSS exceeds some threshold\nHowever, a seemingly worthless split early on in the tree might be followed by a very good split\nSolution 2: build a very long tree, then prune it back in order to obtain a subtree\n\nnot efficient to consider every possible subtree\nso cost complexity pruning (simply use the idea of the lasso regression in adding a penalty): A penalty \\(|T| \\alpha\\) where \\(|T|\\) is the number of final nodes and \\(\\alpha\\) is a tuning parameter, is placed on the total number of final nodes:\n\\(\\alpha = 0\\) → subtree equals the original (long) tree as \\(\\alpha\\) increases, becomes more expensive to have a tree with many terminal nodes, resulting in a smaller subtree.\nuse K-fold cross-validation to choose \\(\\alpha\\)\n\nExamples for cost complexity:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#building-a-tree-summed-up",
    "href": "tree-based.html#building-a-tree-summed-up",
    "title": "7  Tree-based Methods",
    "section": "7.6 Building a tree summed up",
    "text": "7.6 Building a tree summed up",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#classification-trees",
    "href": "tree-based.html#classification-trees",
    "title": "7  Tree-based Methods",
    "section": "7.7 Classification trees",
    "text": "7.7 Classification trees\nThe outcome is a categorical: Would you survive the titanic? Would you pass the exam? Is it a cat or a dog on that picture? Quality of a product (when in categories like good, bad and so on)\nExample:\n\nDifferent predictors are included: sex, pclass, age. Each predictor has a certain threshold or a certain selection of category, which acts as criteria to divide the observations in regions through the branches yes / no.\nIn the root node the criteria is sex= male. For all observations (100%) it is checked, if they are male or not. 64% of the observations are male, but only have a 0.17 chance to survive the titanic. So they are a No (marked in red). 36 % of the observations are women and have a 0.77 chance to survive, which is above 50 %, so they are a yes (marked in blue).\nThen, a next decision level is included, the Pclass. Here, again a certain criteria is set and again, the observations are split into smaller regions. Again, the mean probability of the regions is computed, if they would survive the titanic and it is given, how many observations lay in this region.\nA possible predictor can be used multiple times in a tree in using different thresholds or selecting different categories.\nKey question: how can we find the best split?\nJust as in the regression setting, we use recursive binary splitting to grow a classification tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.\nLuckily, they are many different methods, with which we can explore, what is the best.\nOne method: Gini index or Gini impurity! Measurement, how good the split is compared to others. We do that for all subgroubs(regions) and sum up the values. The decision tree with the smallest value wins!\nCriteria for ‘as similar as possible’: reduction in classification error rate such as the Gini impurity or entropy\nGini impurity or Gini index:\n\\[\nG = \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})\n\\]\nwhere \\(\\hat{p}_{mk}\\) represent the proportion of training observations in partitions m with the category k, e.g. split the observations in proportion with the category age, sex, pclass and compute the survival (yes/ no)\n\nfor each split we look at the impurity of each side and the smaller the better Split can be decided by scientist or for all values of data set & but really time consuming!)\nsmall value: all values in the partition are either close to 0 or to 1\nHence, Gini index is a measure of node impurity; small values indicates that a partition contain predominantly oversvations from a single class\n\nExample:\n\nHere we can see, how we can split the observations with two predictors into regions, where the mean value of the regions define, if the observations in the region will survive or not:\n\n\n\nIf the observation has a fare &gt;= 50, the person is likely to survive and if the persion has a fare =&lt; than 50, but is younger than 8, the person has a good chance to survive.\n\n\n\nwe can continue with splitting over and over again, producing smaller and smaller groups → we see, that at a certain point, more nodes make not sense at all, because producing all over the same outcome for subgroups, that would be predicted in the larger subgroup, too.\nAttention! Splitting can only be done in not overlapping squares!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#compare-trees-vs.-linear-classifiers",
    "href": "tree-based.html#compare-trees-vs.-linear-classifiers",
    "title": "7  Tree-based Methods",
    "section": "7.8 Compare trees vs. linear classifiers",
    "text": "7.8 Compare trees vs. linear classifiers\n\n\nAlthough the decision trees can solve non-linear, complex relations, the decision trees not always better than linear classifiers.\nBecause looking for decision boundaries. if we have a linear problem, decision tree fails.\n→ only perform well for non-linear relationsships\n“Trees are very easy to explain and interpret”\n“Automatically detects” nonlinear relationships and interactions\nTrees can be easily displayed graphically (sometimes)\nUsually worse performance than other common statistical learning methods, because: Prone to overfitting (high variance): small change in the data can cause a large change in the final estimated tree.\n\nCan be improved by combining many trees: Bagging and Random Forests",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#bagging",
    "href": "tree-based.html#bagging",
    "title": "7  Tree-based Methods",
    "section": "7.9 Bagging",
    "text": "7.9 Bagging\nIntuition behind bagging: When you fiddle (herumspielen) with the observations just a little, 1. some things vary a lot, 2. some things stay pretty much the same\nOverfittung is caused by 1, but 1 happens randomly, causing predictions to go up or down hapharyardly (willkürlich, wahllos, zufällig). Therefore 1 should be canceled out by fiddling with the observations a little and averaging\nAveraging: “Wisdom of the crowd”\nBagging is a form of bootstrap aggregation\n\nA general-purpose procedure for reducing the variance of statistical learning methods. It is very useful and often applied in the context of decision trees\nwhen we have multiple sets (n) of independent observations with common variance \\(\\sigma^2\\) the variance over sets of the mean of all observations is given by \\(\\sigma^2 /n\\) .\nhence, averaging a set of independent observations reduces variance. But, what when only training data?\n\nWe can mimic having multiple training sets by bootstrapping:\n\nbootstrapping create resamplesof the sample S m times with replacement and acts like if we have different samples from population\ngenerate B different bootrtrapped training data sets → set the number of trees\ntrain a decision tree on each of the \\(b^th\\) bootrtrapped training set to get a prediction for each observation \\(x:\\hat{f}^{*(b)} (x)\\)\nwe average all predictions to obtain, so the output is the average of all trees B \\[\n\\hat{f_{avg}}(x)= \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{*(b)} (x)\n\\]\nwhen working with classification trees, we take the majority vote:the overall prediction is the most commonly occurring class among the B predictions.\ndo not need to prune! grow large trees in each bootstrapped sample and variance is decreased by averaging\n\n\n\nthree samples with 5 observations, selected with replacement, so randomly sampled in different features.\nResult: three random samples, three decision trees for the each random sample\ntest with a new observation all three trees: for the predicted input we have 3 different predicted values, we take the average of it\n\n\n\nsame procedure for classification → the majority wins!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#random-forest",
    "href": "tree-based.html#random-forest",
    "title": "7  Tree-based Methods",
    "section": "7.10 Random forest",
    "text": "7.10 Random forest\n\nBagging works because averaging independent observations reduces the variance to the tune of n.\nin bootstrapping, samples taken are independent.\nbut the predictions from the tree grown on the bootstrapped samples are not independent.\nshare the same features and can therefore create similarly overfitting in decision rules.\n“Wisdom of crowds who peek at each other´s answers”\neach sample has the same predictors and features! So the samples are independently but the features are not! &rarr features correlate, “tree correlation” (Breiman 2001)\ninstead: use a subset of features to decorrelate → random forest try to remove this coorrelation by feature sampling: randomly sampling both rows (bootstrapping) and columns.\nAs in bagging, we build a number forest of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors.\ntypically \\(m = /sqrt{p}\\) so number of predictors considered at each split approximately equal to total numbers of predictors\nUsing a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors\n\nDecorrelation obtained by:\n\nWhen building a tree, instread of using all variables when making a split, take a random selection of m predictors as candidates to base the split on\nat each split, take a fresh selection of m predictors m is typically set to \\(\\sqrt{p}\\)\nSimilar to bagging, the collection of trees (=forest) is build on bootstrapped training samples\n\nHence, bagging is a special case of a random forest m=p\n\n\nbootstrap again. not every feature is in every sample\nresult: three decision trees with different nodes on each decision level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#other-methods",
    "href": "tree-based.html#other-methods",
    "title": "7  Tree-based Methods",
    "section": "7.11 Other methods",
    "text": "7.11 Other methods\n\nIn boosting we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used.\nIn Bayesian Additive REgreesion Trees (BART) , we once again only make use of the original data, and we grow the trees successively. However, each tree is perturbed in order to avoid local minima and achieve a more thorough exploration of the model space.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#out-of-bag-error-estimation",
    "href": "tree-based.html#out-of-bag-error-estimation",
    "title": "7  Tree-based Methods",
    "section": "7.12 “Out-of-bag” error estimation",
    "text": "7.12 “Out-of-bag” error estimation\nWhen we do bagging and random forest, there is a very simple way to estimate the test error without the need to perform cross/validation or the validation set approach\n\nin both methods, we take multiple bootstrapped samples of the training data. On average, each tree uses about 2/3 of the observations\nramaing 1/3 left observations are referred to as the out-of-bag (OOB) observations. Left out for test\nIf we want to calculate the error for a particular observation, we can predict the response using each of the trees in which it was OOB. This will give B/3 predictions for this observation, which we average. When we do this for all observations we get the OOB error.\nIt can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\n\nAnd lastly, the OOB score is computed as the number of correctly predicted rows from the out of bag sample. out-of-bag error is an estimate of the error rate (which is 1 - accuracy) that this training approach has for new data from the same distribution. This estimate is based on the predictions that you get for each data point by using only averaging those trees, for which the record was not in the training data.\nIf you have a low number of trees the OOB error might be not a good estimate of the error rate that training an algorithm like this has on new data, because each tree in RF tends to be underfit and only once you combine enough trees the RF gets better (so if there’s only a small number of trees per record, it may underestimate performance). On the other hand, once you have a huge number of trees it becomes a pretty good estimate like you get from a train-validation split with a lot of data (or cross-validation).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#variance-importance-measure-mdi",
    "href": "tree-based.html#variance-importance-measure-mdi",
    "title": "7  Tree-based Methods",
    "section": "7.13 Variance importance measure (MDI)",
    "text": "7.13 Variance importance measure (MDI)\nIn both bagging and random forest, it can be difficult to interpret the resulting model\n\nWhen large number of trees, no longer possible to represent graphically the resulting statistical learning procedure using a single tree\nhow to find out which predictor(s) are most important?\n\nAlthough the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).\n→ variable importance measures: A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees\n\nunblackboxing the black box models!\nBecause when we have hundred trees, it is not easy to follow anymore.\n\nIt´s not traceable, which variables are important and how they weight in the model overall.\nHow “important is variable \\(x_j\\) to the prediction?\n\nrecall that trees are grown by minimizing “impurity” (e.g. Gini, RSS)\nidea: record the total amount that impurity is decreased due to splits over \\(x_j\\), averaged over all B trees\nAdvantage: obtained for free with estimation. We can do that easily, because we already have a tree\nDisadavantage: imporance of features used to overfit inflated, importannce of numerical features inflated\n\n\n7.13.1 Permutation-based feature importance\n\nidea: randomly shuffle one column and observe how much worse it makes the model\nadvantage: does not have the problems of MDI\ndisadvantage: can take a while, results vary and ignores correlations among predictors (perfectly correlated features are all “unimportant”)\n\nHow to interpret?\n![]figures/8.imp.png){width=“300”}\nThe Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#in-r",
    "href": "tree-based.html#in-r",
    "title": "7  Tree-based Methods",
    "section": "7.14 in R",
    "text": "7.14 in R\nlibrary(tree) #decision tree\n\nyour_tree_model &lt;-tree(bin_qual ~ fixed_acidity + residual_sugar + pH + sulphates, data=wine_train)\n\nNow, let's evaluate whether pruning this tree may lead to an improved tree. For this, you use the function `cv.tree()`. This runs the tree repeatedly, at each step reducing the number of terminal nodes to determine how this impacts the deviation of the data. You will need to use the argument `FUN = prune.misclass` to indicate we are looking at a classification tree. \n\nlibrary(randomForest) #random Forest\n\n#bagging= max number of predictors are included, randomForest not alle predictors, only subset is included\n\nr_bag &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula \n             data = wine_train,          # Data Set\n             mtry = 7,          # Number of predictors to be considered for each split \n             importance = TRUE)  # The Variable importance estimator   \n             \nlibrary(party) # importance measurement\nvarimp(model, conditional = TRUE) # shows the important predictors in a model\nIn this practical we will cover an introduction to building tree-based models, for the purposes of regression and classification. This will build upon, and review the topics covered in the lecture, in addition to Chapter 8: Tree Based Models in Introduction to Statistical Learning.\nYou can download the student zip including all needed files for practical 8 here. For this practical, you will need the following packages:\n\n# General Packages\nlibrary(tidyverse)\n\n# Creating validation and training set\nlibrary(caret)\n\n# Decision Trees\n#install.packages(\"tree\")\nlibrary(tree)\n\n# Random Forests & Bagging \n#install.packages(\"party\")\nlibrary(randomForest)\n\nThroughout this practical, we will be using the Red Wine Quality dataset from Kaggle, introduced within the Moving Beyond Linearity Lecture (Week 6).\n\nwine_dat &lt;- read.csv(\"data/winequality-red.csv\")\n\n\n7.14.1 Decision Trees\nWhen examining classification or regression based problems, it is possible to use decision trees to address them. As a whole, regression and classification trees, follow a similar construction procedure, however their main difference exists in their usage; with regression trees being used for continuous outcome variables, and classification trees being used for categorical outcome variables. The other differences exist at the construction level, with regression trees being based around the average of the numerical target variable, with classification tree being based around the majority vote.\nKnowing the difference between when to use a classification or regression tree is important, as it can influence the way you process and produce your decision trees.\n\nUsing this knowledge about regression and classification trees, determine whether each of these research questions would be best addressed with a regression or classification tree.\n\nHint: Check the data sets in the Help panel in the Rstudio GUI.\n\n1a. Using the Hitters data set; You would like to predict the Number of hits in 1986 (Hits Variable), based upon the number of number of years in the major leagues (Years Variable) and the number of times at bat in 1986 (AtBat Variable).\n\nRegression, because outcome is numeric variable\n\n1b. Using the Hitters data set; You would like to predict the players 1987 annual salary on opening day (Salary variable), based upon the number of hits in 1986 (Hits variable), the players league (League variable) and the number of number of years in the major leagues (Years Variable).\n\nRegression, because outcome is numeric variable\n\n1c. Using the Diamonds data set; You would like to predict the quality of the diamonds cut (cut variable) based upon the price in US dollars (price variable) and the weight of the diamond (carat variable).\n\nclassification, because outcome is categorical\n\n1d. Using the Diamonds data set; You would like to predict the price of the diamond in US dollars (price variable), based upon the diamonds colour (color variable) and weight (carat variable).\n\nRegression\n\n1e. Using the Diamonds data set; You would like to predict how clear a diamond would be (clarity variable), based upon its price in US dollars (price variable) and cut (cut variable).\n\nclassification\n\n1f. Using the Titanic data set; You would like to predict the survival of a specific passenger (survived variable), based upon their class (Class variable), sex (Sex variable) and age (Age variable).\n\nClassification\n\n\n\n\n7.14.2 Classification trees\nBefore we start growing our own decision trees, let us first explore the data set we will be using for these exercises. This as previously mentioned is a data set from Kaggle, looking at the Quality of Red Wine from Portugal. Using the functions str() and summary(), explore this data set (wine_dat).\nAs you can see this contains over 1500 observations across 12 variables, of which 11 can be considered continuous, and 1 can be considered categorical (quality).\nNow we have explored the data this practical will be structured around, let us focus on how to grow classification trees. The research question we will investigate is whether we can predict wine quality, classified as Good (Quality &gt; 5) or Poor (Quality &lt;= 5), by the Fixed Acidity (fixed_acidity), amount of residual sugars (residual_sugar), pH (pH) and amount of sulphates (sulphates).\nBefore we grow this tree, we must create an additional variable, which indicates whether a wine is of Good or Poor quality, based upon the quality of the data.\n\nsummary(wine_dat)\n\n fixed_acidity   volatile_acidity  citric_acid    residual_sugar  \n Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  \n 1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  \n Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  \n Mean   : 8.32   Mean   :0.5278   Mean   :0.271   Mean   : 2.539  \n 3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  \n Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  \n   chlorides       free_sulfur_dioxide total_sulfur_dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.87       Mean   : 46.47       Mean   :0.9967  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9978  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6581   Mean   :10.42   Mean   :5.636  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n\n\n\nstr(wine_dat)\n\n'data.frame':   1599 obs. of  12 variables:\n $ fixed_acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...\n $ volatile_acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...\n $ citric_acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...\n $ residual_sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...\n $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...\n $ free_sulfur_dioxide : num  11 25 15 17 11 13 15 15 9 17 ...\n $ total_sulfur_dioxide: num  34 67 54 60 34 40 59 21 18 102 ...\n $ density             : num  0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...\n $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...\n $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...\n $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...\n\n\n\nUsing the code below, create a new variable bin_qual (short for binary quality) as part of the wine_dat data set.\n\n\nwine_dat$bin_qual &lt;- ifelse(wine_dat$quality &lt;= \"5\", \"Poor\", \"Good\")\nwine_dat$bin_qual &lt;- as.factor(wine_dat$bin_qual)\n\nNext, we will split the data set into a training set and a validation set (for this practical, we are not using a test set). As previously discussed in other practicals these are incredibly important as these are what we will be using to develop (or train) our model before confirming them. As a general rule of thumb for machine learning, you should use a 80/20 split, however in reality use a split you are most comfortable with!\n\nUse the code given below to set a seed of 1003 (for reproducibility) and construct a training and validation set.\n\n\nset.seed(1003)\ntrain_index &lt;- createDataPartition(wine_dat$bin_qual, p = .8, \n                                  list = FALSE, \n                                  times = 1)\n\nwine_train &lt;- wine_dat[train_index,]\nwine_valid &lt;- wine_dat[-train_index,]\n\nThis should now give you the split data sets of train & validate, containing 1278 and 321 observations respectively.\n\n\n7.14.3 Building Classification Trees\nNow that you have split the quality of the wine into this dichotomous pair and created a training and validation set, you can grow a classification tree. In order to build up a classification tree, we will be using the function tree() from the tree package, it should be noted although there are multiple different methods of creating decision trees, we will focus on the tree() function. As such this requires the following minimum components:\n\nformula\ndata\nsubset\n\nWhen growing a tree using this function, it works in a similar way to the lm() function, regarding the input of a formula, specific of the data and additionally how the data should be sub-setted.\n\nUsing the tree() function, grow a tree to investigate whether we can predict wine quality classified as Good (Quality &gt; 5) or Poor (Quality &lt;= 5), by fixed_acidity, residual_sugar, pH and sulphates.\n\n\nyour_tree_model &lt;-tree(bin_qual ~ fixed_acidity + residual_sugar + pH + sulphates, data=wine_train)\n?tree\n\nHelp on topic 'tree' was found in the following packages:\n\n  Package               Library\n  xfun                  /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/xfun/0.49/8687398773806cfff9401a2feca96298\n  tree                  /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/tree/1.0-44/62b3f891c7f9be249e426bfc6cdb2eac\n  cli                   /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/cli/3.6.3/b21916dd77a27642b447374a5d30ecf3\n\n\nUsing the first match ...\n\n\n\n\n7.14.4 Plotting Classification Trees\nWhen plotting decision trees, most commonly this uses the base R’s plot() function, rather than any ggplot() function. As such to plot a regression tree, you simply need to run the function plot() including the model as its main argument.\n\nUsing the plot() function, plot the outcome object of your regression tree.\n\n\nplot(your_tree_model)\n\n\n\n\n\n\n\n\nAs you can see when you plot this, this only plots the empty decision tree, as such you will need to add a text() function separately.\n\nRepeat plotting the outcome object of your regression tree, with in the next line adding thetext() function, with as input your_tree_model and pretty = 0.\n\n\nplot(your_tree_model)\ntext(your_tree_model, pretty=0)\n\n\n\n\n\n\n\n\nThis now adds the text to the to the decision tree allowing it to be specified visually.\nAlthough plotting the regression tree can be useful for displaying how a topic is split, it only goes some way to answering the research question presented. As such, additional steps are required to ensure that the tree is efficiently fitted.\nFirstly, you can explore the layout of the current model using the summary() function. This displays the predictors used within the tree; the number of terminal nodes; the residual mean deviance and the distribution of the residuals.\n\nUsing the summary() function, examine the current decision tree, and report the number of terminal nodes and the residual mean deviance.\n\n\nsummary(your_tree_model)\n\n\nClassification tree:\ntree(formula = bin_qual ~ fixed_acidity + residual_sugar + pH + \n    sulphates, data = wine_train)\nVariables actually used in tree construction:\n[1] \"sulphates\"\nNumber of terminal nodes:  5 \nResidual mean deviance:  1.235 = 1575 / 1275 \nMisclassification error rate: 0.3336 = 427 / 1280 \n\n\n\n\n7.14.5 Assessing accuracy and pruning of classification trees\nDuring the homework part, you have fitted a classification tree on the Red Wine Quality dataset. In this first part during the lab, we will continue with your fitted tree, and inspect its overall accuracy and improvement through pruning. To examine the overall accuracy of the model, you should determine the prediction accuracy both for the training and the validation set. Using the predicted and observed outcome values, we can construct a confusion matrix, similar to what we’ve done in week 5 on Classification methods.\nSo let us build a confusion matrix for the training subset. To begin, once more you need to calculate the predicted value under the model. However, now you need to specify that you want to use type = \"class\"; before forming a table between the predicted values and the actual values. As shown below\n\n# Create the predictions \n    yhat_wine_train &lt;- predict(your_tree_model, newdata = wine_train, type = \"class\")\n\n# Obtain the observed outcomes of the training data \n    qual_train &lt;- wine_train[, \"bin_qual\"]\n    \n# Create the cross table:\n    tab_wine_train &lt;- table(yhat_wine_train, qual_train)\n    tab_wine_train\n\n               qual_train\nyhat_wine_train Good Poor\n           Good  511  254\n           Poor  173  342\n\n    sum(tab_wine_train)\n\n[1] 1280\n\n\nThe obtained confusion matrix indicates the frequency of a wine predicted as good while it is actually good or poor, and the frequency of wine predicted as poor while actually being good or poor. The frequencies in the confusion matrix are used to determine the accuracy through the formula:\nAccuracy = (Total Correct-True Predict [1,1] + Total Correct-False Predictions [2,2]) / total number of items\n\n# Calculate Accuracy accordingly: \n  accuracy_wine_train &lt;- (tab_wine_train[1] + tab_wine_train[4]) / 1280\n\n  accuracy_wine_train\n\n[1] 0.6664062\n\n\nFrom this, you can see that this model has an accuracy of around 67% meaning that 67% of the time, it is able to correctly predict from the predictors whether a wine will be of good or poor quality.\n\nUsing this format, create a confusion matrix for the validation subset, and calculate the associated accuracy.\n\nHint: you can obtain the predicted outcomes for the validation set using predict(your_tree_model, newdata = wine_valid, type = \"class\") and you can extract the observed outcomes of the validation data using wine_valid[, \"bin_qual\"].\n\n# predict\nyhat_wine_valid &lt;- predict(your_tree_model, newdata = wine_valid, type = \"class\")\n\n# obtain the observed outcomes to the validation set\nqual_valid &lt;-wine_valid[, \"bin_qual\"]\n\n#Create cross table\ntab_wine_valid &lt;- table(yhat_wine_valid, qual_valid)\ntab_wine_valid\n\n               qual_valid\nyhat_wine_valid Good Poor\n           Good  126   62\n           Poor   45   86\n\nsum(tab_wine_valid)\n\n[1] 319\n\n\n\n# accuracy\n  accuracy_wine_valid &lt;- (tab_wine_valid[1] + tab_wine_valid[4]) / 319\n\n  accuracy_wine_valid\n\n[1] 0.6645768\n\n\nOn the validation data set it performs slightly less well.\nNow, let’s evaluate whether pruning this tree may lead to an improved tree. For this, you use the function cv.tree(). This runs the tree repeatedly, at each step reducing the number of terminal nodes to determine how this impacts the deviation of the data. You will need to use the argument FUN = prune.misclass to indicate we are looking at a classification tree.\n\nRun the model through the cv.tree() function and examine the outcome using the plot() function using the code below.\n\n\n# Determine the cv.tree\n  cv_quality &lt;- cv.tree(your_tree_model, FUN=prune.misclass)\n\n?cv.tree\n\n# Plot the size vs dev\n  plot(cv_quality$size, cv_quality$dev, type = 'b')\n\nWhen you have run this code, you should observe a graph, which plots the size (the amount of nodes) against the dev (cross-validation error rate). This indicates how this error rate changes depending on how many nodes are used. In this case you should be able to observe a steep drop in dev between 1 and 2, before it slowing down from 2 to 5 (the maximum number of nodes used). If you would further like to inspect this, you could compare the accuracy (obtained from the confusion matrices) between these different models, to see which is best fitting. In order to prune the decision tree, you simply use the function prune.misclass(), providing both the model and best = number of nodes as your arguments.\n\n?prune.misclass\nmodel_tree2 &lt;- prune.misclass(your_tree_model, best= 2)\nplot(model_tree2);text(model_tree2)\n\n\n\n\n\n\n\n\nNote that in the same way as growing classification trees, you can use the function tree() to grow regression trees. Regarding the input of the function tree() nothing has to be changed: the function detects whether the outcome variable is categorical as seen in the above example, applying classification trees, or continuous, applying regression trees. Differences arise at evaluating the decision tree (inspecting the confusion matrix and accuracy for classification trees or inspecting the MSE for regression trees) and at pruning. To prune a classification tree, you use the function prune.mislcass(), while for regression trees the function prune.tree() is used.\n\n\n7.14.6 Bagging and Random Forests\nWhen examining the techniques of Bagging and Random Forests, it is important to remember that Bagging is simply a specialized case of Random Forests where the number of predictors randomly sampled as candidates at each split is equal to the number of predictors available, and the number of considered splits is equal to the number of predictors.\nSo for example, if you were looking to predict the quality of wine (as we have done during the classification tree section), based upon the predictors fixed acidity (fixed_acidity), citric acid (citric_acid), residual sugars (residual_sugar), pH (pH), total sulfur dioxide content (total_sulfar_dioxide), density (density) and alcohol (alcohol) content. If we were to undergo the bagging process we would limit the number of splits within the analysis to 7, whereas within random forest it could be any number of values you choose.\nAs such, the process of doing Bagging or Random Forests is similar, and both will be covered. When using these methods we get an additional measure for model accuracy in addition to the MSE: the out of bag (OOB) estimator. Also, we can use the variable importance measure to inspect which predictors in our model contribute most to accurately predicting our outcome.\nNote that we will focus on a classification example, while the ISLR textbook focuses on a regression tree example.\n\n\n7.14.7 Bagging\nBoth Bagging and Random Forest are based around the function randomForest() from the randomForest package. The function requires the following components:\nrandomForest(formula = ???,       # Tree Formula \n             data = ???,          # Data Set\n             subset = ???,        # How the data set should be subsetted \n             mtry = ???,          # Number of predictors to be considered for each split \n             importance = TRUE,   # The Variable importance estimator\n             ntree = ???)         # The number of trees within the random forest\nIn the case of bagging, the argument mtry should be set to the quantity of the predictors used within the model.\n\nCreate a bagging model for the research question: can we predict quality of wine bin_qual, by fixed_acidity, citric_acid, residual_sugar, pH, total_sulfur_dioxide, density and alcohol and inspect the output. Omit ntree from the functions input for now.\n\n\nr_bag &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula \n             data = wine_train,          # Data Set\n             mtry = 7,          # Number of predictors to be considered for each split \n             importance = TRUE)  # The Variable importance estimator       \n\nHow do we interpret the output of this classification example? From this output, you can observe several different components. Firstly, you should be able to observe that it recognizes that this is a Classification forest, with 500 trees (the default setting for number of trees) and 7 variables tried at each split. In addition, the OOB estimate is provided in the output as well as a classification confusion matrix.\nLet us examine the the accuracy level of our initial model, and compare it to the accuracy of models with a varying number of trees used.\n\nCalculate the accuracy of the bagged forest you just fitted\n\n\nacc_r_bag &lt;- (r_bag$confusion[1,1] + (r_bag$confusion[2,2]) / 1280)\n\nNow let’s have a look at the Out of Bag (OOB) estimator of error rate. The OOB estimator of the error rate is provided automatically with the latest version of randomForest(), and can be used as a valid estimator of the test error of the model. In the OOB estimator of the error rate, the left out data at each bootstrapped sample (hence, Out of Bag) is used as the validation set. That is, the response for each observation is predicted using each of the trees in which that observation was OOB. This score, like other indicates of accuracy deviation, you will want as low as possible, since it indicates the error rate.\n\nInspect the OOB scores of the bagged forest you fitted.\n\n\n#you can read that of the model\n#500 Trees = 22.46%\n\n\nFit two additional models, in which you set the number of trees used to 100 and 10000, and inspect the OOB scores. Which has the highest and lowest OOB estimate?\n\n\n#specifying the number of trees\nr_bag_100 &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula \n             data = wine_train,          # Data Set\n             mtry = 7,          # Number of predictors to be considered for each split \n             importance = TRUE,\n             ntree = 100)  # The Variable importance estimator   \n\n#specifying the number of trees\nr_bag_10000 &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula \n             data = wine_train,          # Data Set\n             mtry = 7,          # Number of predictors to be considered for each split \n             importance = TRUE,\n             ntree = 10000)  # The Variable importance estimator  \n\n#read it again from the output!\n\nplot(r_bag)\n\n\n\n\n\n\n\n\n\n\n7.14.8 Random Forests\nThe main difference between Bagging and Random Forests is that the number of predictors randomly sampled as candidates at each split is not equal to the number of predictors available. In this case, typically (by default from the randomForest() function), they determine mtry to be 1/3 the number of available predictors for regression trees and the square root of the number of available predictors for classification trees.\n\nUsing the randomForest() function, construct a random forest model using mtry = 3 and ntree = 500.\n\n\nr_forest &lt;- randomForest(formula = bin_qual ~ fixed_acidity + citric_acid + residual_sugar + pH + total_sulfur_dioxide + density + alcohol,       # Tree Formula \n             data = wine_train,          # Data Set\n             mtry = 3,          # Number of predictors to be considered for each split \n             importance = TRUE,\n             ntree = 500)  # The Variable importance estimator  \n\n\nInspect fitted random forest model and the corresponding the OOB estimate of the random forest model and compare to the OOB estimate of the bagged forest model with ntree = 500.\n\n\n# 21.33 %\n\nThe OOB estimate is really close to the bagging model. Randomforest prevents overfitting, but because we do not have a lot of predictors here, it is not the case here.\n\n\n7.14.9 Variable importance\nThe final (optional) part of this practical will look into how to actually interpret Random Forests, using the Variable Importance Measure. As you have probably worked out from section so far, physically representing these forests is incredibly difficult and harder to interpret them, in comparison to solo trees. As such, although creating random forests improves the prediction accuracy of a model, this is at the expense of interpretability. Therefore, to understand the role of different predictors within the forests as a whole it is important to examine the measure of Variable Importance.\nOverall, when looking at Regression Trees, this Variable Importance measure is calculated using the residual sum of squares (RSS) and via the Gini Index for classification trees. Conveniently, the correct version will be determined by the randomForest() function, as it can recognize whether you are creating a regression or classification tree forest.\nIn order to call this measure, we simply need to call the model into the function importance(). Within our case (looking at a classification forest) this will produce four columns, the binary outcome (Good/Poor) in addition to the Mean Decrease in Accuracy and the Mean Decrease in Gini Index. This is by contrast to those which you will find when examining Regression trees, examples of which can be found in ISLR Section 8.3.3.\nIn order to best interpret these findings, it is possible to plot, how important each predictor is using the function varImpPlot(). This will produce a sorted plot which will show the most to least important variables.\n\nUsing your random forest model, examine the importance of the predictors using importance() and use varImpPlot() to plot the result. Which predictor is most important to predict the quality of Wine?\n\n\n#table\nforest_imp &lt;- importance(r_forest)\n#plot\nvarImpPlot(r_forest)\n\n\n\n\n\n\n\n\nThis is a fundamental outcome of the random forest and it shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.\nGINI: GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. Permuting a useful variable, tend to give relatively large decrease in mean gini-gain. GINI importance is closely related to the local decision function, that random forest uses to select the best available split. Therefore, it does not take much extra time to compute. On the other hand, mean gini-gain in local splits, is not necessarily what is most useful to measure, in contrary to change of overall model performance. Gini importance is overall inferior to (permutation based) variable importance as it is relatively more biased, more unstable and tend to answer a more indirect question.\nE. g. alcohol is really relevant. Gini checks, how much the homogenity in nodes decreases, if a certain predictor is removed. The higher the the homogenity effect of one predictor is, the higher is the mean decrease in Gini.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "tree-based.html#conclusions",
    "href": "tree-based.html#conclusions",
    "title": "7  Tree-based Methods",
    "section": "7.15 Conclusions",
    "text": "7.15 Conclusions\n\nDecision trees are simple and useful for interpretation\nhowever, prone to overfitting. Solutions: pruning, bagging and random forests\nBagging: fit multiple trees to bootstrapped samples of the data, combine to yield a single consensus prediction\nRandom Forest: fit trees to bootstrapped samples from the data AND sample predictors. Combine all trees to yield a consensus prediction\nWhen using bagging and random forests\nWe can approximate the test error using the Out of Bag (OOB) estimate of the error\nWhich predictor is most influential on the outcome can be inferred from variable importance measures\nRandom forests often show top-tier performance out of the box, but the resulting model is difficult to interpret",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "text-mining.html",
    "href": "text-mining.html",
    "title": "8  Text Mining",
    "section": "",
    "text": "8.1 Readings\nWelcome to Text Mining with R",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#readings",
    "href": "text-mining.html#readings",
    "title": "8  Text Mining",
    "section": "",
    "text": "Chapter 1: The tidy text format – Chapter 2: Sentiment analysis with tidy data\nChapter 3: Analyzing word and document frequency: tf-idf",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#introduction",
    "href": "text-mining.html#introduction",
    "title": "8  Text Mining",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nLecturer: Ayoub Bagheri\nWhat is text mining?\n\nmost popular: “The discovery by computer of }new, previously unknown information}, by automatically extracting information from different written resources” (Hearst 1999)\nText mining is about looking for patterns in text, in a similar way that data mining can be loosely described as looking for patterns in data.\nText mining describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources (Wikipedia)\n\nWhy text mining?\nText data is everywhere, websites (e.g., news), social media (e.g., twitter), databases (e.g., doctors’ notes), digital scans of printed materials, … - A lot of world’s data is in unstructured text format - Applications in industry: search, machine translation, sentiment analysis, question answering, … - Applications in science: cognitive modeling , understanding bias in language, automated systematic literature reviews, …\nExamples for text mining:\n\nWho was the best friend in Friends?\nusing text mining instead of hand assignment in the automatic detection of ICD10 codes in cardiology discharge letters\n\nunderstanding language is difficult!\n(at least half of them are an open problem)\n\nDifferent things can mean more or less the same ( data science ” vs.statistics\nContext dependency (You have very nice shoes)\nSame words with different meanings (to sanction)\nLexical ambiguity (we saw her duck)\nIrony, sarcasm (You should swallow disinfectant)\nFigurative language (He has a heart of stone)\nNegation (not good ” vs. good ””), spelling variations, jargon, abbreviations\nAll the above is different over languages,99 % of work is on English!\n\nKey problem\n\nText, images, videos is unstructured data, not tidy\nunstructured text: information that either does not have a pre defined data model or is not organized in a pre defined manner.\nfor our analysis we need tidy data\neach variable is a column, each observation a row, each type of observational unit is a table → table with one-token-per-row\ntoken is a meaningful unit of text, such as a word, that we are interested in using for analysis\nhow is data stored in text mining approaches?\nstring, character vectors\ncorpus: contain raw string strings annotated with additional meta data and details\ndocument-term matrix: sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf. In other words: is a mathematical matrix that describes the frequency of terms that occur in a collection of documents\n→ How is it possible to convert this data to structured one?\nanswer: tokenization, the process of splitting text into tokens",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#preprocessing-data",
    "href": "text-mining.html#preprocessing-data",
    "title": "8  Text Mining",
    "section": "8.3 Preprocessing data",
    "text": "8.3 Preprocessing data\n\napproach for cleaning and noise removal of text data\nbrings text in analyzable form for statistical learning\nis useful, because:\nefficient\nremove stop words - def: words which are filtered out before or after processing of natural language data (text)\nreduce noise\nis tidy and structured\ndimensionality: words as features in columns\nmemory allocation\nincrease performance",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#typical-steps",
    "href": "text-mining.html#typical-steps",
    "title": "8  Text Mining",
    "section": "8.4 typical steps",
    "text": "8.4 typical steps\nnot all of these are appropriate at all times!\n\ntokenization Tokenization (“text”, “ming”, “is”, “the”, “best”, “!”)\nstemming (“lungs” to “lung”) or Lemmatization (“were”to “is”)\n\ndef: the process for reducing inflected (or sometimes derived) words to their word stem, base or root form generally a written word form\n\nlowercasing (“Disease” to “disease”)\nstopword removal (“text mining is best!”)\npunctual removal (“text mining is the best”)\nnumber removal (“I42” to “I”)\nspell correction (“hart” to “heart”)\n\nExample for steps, one step behind another with a Vector Space Model as a result:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#vector-space-model",
    "href": "text-mining.html#vector-space-model",
    "title": "8  Text Mining",
    "section": "8.5 Vector Space Model",
    "text": "8.5 Vector Space Model\nBasic idea: Represent the text as something that makes sense to a computer, makes it readable for the computer\n\nis a collection of vectors\nrepresents documents by concept vectors\neach concept defines one dimension (one dimension can be one word)\nk concepts define a high-dimensional space\nelement of vector corresponds to concept weight\nterms/ words are genereic features that can be extracted from text\ntypically, terms are single words, keywords, n/grams, or phrases\ndocuments are represented as vectors of terms\neach dimension (concept) corresponds to a separate term\n\n\\[\nd= (w_1, \\dots,  w_n)\n\\]\n\n\nThe process of converting text into numbers is called Vectorization\nDistance between the vectors in this concept space illustrate the relationsship between the documents",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#bag-of-words",
    "href": "text-mining.html#bag-of-words",
    "title": "8  Text Mining",
    "section": "8.6 Bag-of-Words",
    "text": "8.6 Bag-of-Words\nHow can we convert words in numerical values? You need a vocabulary that works for all articles, so a binary approach!\n\nTerms are words (more guenereally we can use n-grams)\nweights are numbers of occurrences of the terms int eh document\nbinary\nterm frequency (TF)\nterm Frequency inverse Document Frequency (TFiDF)\n\nLooking at all words in all articles and give zeros and ones, if the term occur or not:\n\n\n8.6.1 TFiDF\nA term is more discriminative if it occurs a lot but only in fewer documents → shows how often the words occur and how important it is!\n\nHow often? Let \\(n_{d,t}\\) denote the number of times the t-th term appear in the d-th document\n\n\\[\nTF_{d,t} = \\frac{n_{d,t}}{\\sum_{i}n_{d,i}}\n\\]\n\nhow important? Let N denote the number of documents and \\(N_t\\) denote the number of documents containing the t-th term.\n\n\\[\nIDF_t = log (\\frac{N}{N_t})\n\\]\nWhen a word appears in all of the documents, it cancelled out by how important that is.\nAnd then weight with TFiDF:\n\\[\nw_{d,t} = TF_{d,t} * IDF_t\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#overview-of-vsm-models",
    "href": "text-mining.html#overview-of-vsm-models",
    "title": "8  Text Mining",
    "section": "8.7 Overview of VSM models",
    "text": "8.7 Overview of VSM models\n\noverview about the method:\nthree categories of vector space model\n\nbag-of-words\nWe do not care about the order of words, so not about the meaning good to convert to a table high dimensional, high number of zeros (sparse)\ntopics we expect 10 topics or clusters use these topics as vectors\nword embeddings we care about the order and the meaning",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#text-classification",
    "href": "text-mining.html#text-classification",
    "title": "8  Text Mining",
    "section": "8.8 Text classification",
    "text": "8.8 Text classification\n\nsupervised learning: learning a function that maps an input to an output based on example input-output pairs\n\ninfer function from labeled training data\nuse inferred function to label new instances\n\ncommon: human experts annotate a set of text data as a training set\n\nhand-coded rules\n\nrules based on combinations of words or other features\naccuracy can be high if rules are carefully defined by experts\ndata/domain specific",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#algorithms",
    "href": "text-mining.html#algorithms",
    "title": "8  Text Mining",
    "section": "8.9 Algorithms",
    "text": "8.9 Algorithms\n\nNaïve Bayes\nLogistic regression\nSupport vector machines\nK nearest neighbors\nNeural networks\nDeep learning",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#word-represenation",
    "href": "text-mining.html#word-represenation",
    "title": "8  Text Mining",
    "section": "8.10 Word represenation",
    "text": "8.10 Word represenation\n\nhow can we represent the meaning of words?\n\nWords as vectors:\n\ncapture semantics:\nsimilar words should be close to each other in the vector space\nrelation between two vectors should reflect the relationship between the two words\nbe efficient, because smaller number of vectors and dimensions\nbe interpretable",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#in-r",
    "href": "text-mining.html#in-r",
    "title": "8  Text Mining",
    "section": "8.11 in R",
    "text": "8.11 in R\n## libraries\nlibrary(tidytext)\nlibrary(RColorBrewer)\nlibrary(gutenbergr)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(textdata)\nlibrary(tm)\nlibrary(NLP)\nlibrary(stringi)\nlibrary(e1071)\nlibrary(rpart)\nlibrary(caret)\n\n## Preprocessing tidy data apporoach\n  #each line get a number and each chapter reference a number, too in the book Alice in Wonderland\ntidy_AAIWL &lt;- AAIWL %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n## Tokenization, un-nesting Text\n  #each word in the column text gets is own row\ntidy_AAIWL &lt;- tidy_AAIWL %&gt;% unnest_tokens(word, text)\n\n#count the words\n  #from most used descending \ntidy_AAIWL.count &lt;- tidy_AAIWL %&gt;% count(word, sort=TRUE)\n\n#remove stopwords\ndata(\"stop_words\")\ntidy_AAIWL.stop &lt;- tidy_AAIWL %&gt;% anti_join(stop_words)\n\n#create a word cloud out of it\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))                                        \n\n#Vector Space Model\n  #set the seed to make your partition reproducible\nset.seed(123)\n\ndf_final$Content &lt;- iconv(df_final$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\n  #for documenttermmatrix we need a corpus format, a list ot content and the meta data \ndocs &lt;- Corpus(VectorSource(df_final$Content)) \n\n  ## alter the code from here onwards\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE))  #lower case\n\nview(dtm)\n  #in the rows are the entries, in the columns the frequency of a word\n\n  ## we are not interested in all the words, we are only interested in words that are more ofte used than 10 times\nnot.freq &lt;- findFreqTerms(dtm, lowfreq=11) \n  #run again \ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary = not.freq))  #lower case\n   ## define the training partition \ntrain_index &lt;- createDataPartition(df_final$Category, p = .8,  #category because outcome variable\n                                  list = FALSE, \n                                  times = 1)\n\n  ## split the data using the training partition to obtain training data\ndf_train &lt;- df_final[train_index,]\n\n  ## remainder of the split is the validation and test data (still) combined \ndf_test &lt;- df_final[-train_index,] \n\ndf_train$Content &lt;- iconv(df_train$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\ndf_test$Content &lt;- iconv(df_test$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs_train &lt;- Corpus(VectorSource(df_train$Category)) \ndocs_test &lt;- Corpus(VectorSource(df_test$Category)) \n\n## alter the code from here onwards\ndtm_train &lt;- DocumentTermMatrix(docs_train,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n## alter the code from here onwards\ndtm_test &lt;- DocumentTermMatrix(docs_test,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n#make a data frame \ndtm_train  &lt;- as.data.frame(as.matrix(dtm_train))\ndtm_test  &lt;- as.data.frame(as.matrix(dtm_test))\n\n#now you can run a decision tree on it with\nlibrary(rpart)\nfit_dt &lt;- rpart(cat~., data = dtm_train, method = 'class')\n \n## prediction on training data\npred_train &lt;- predict(fit_dt, dtm_train, type = 'class')\nfit_table  &lt;- table(dtm_train$cat, pred_train, dnn = c(\"Actual\", \"Predicted\"))\nfit_table\n\npred_test       &lt;- predict(fit_dt, dtm_test, type = 'class')\nfit_table_test  &lt;- table(dtm_test$cat, pred_test, dnn = c(\"Actual\", \"Predicted\"))\nfit_table_test\n\n## You can use this table to calculate Accuracy, Sensitivity, Specificity, Pos Pred Value, and Neg Pred Value. There are also many functions available for this purpose, for example the `confusionMatrix` function from the `caret` package.\nDuring this practical, we will cover an introduction to text mining. Topics covered are how to pre-process mined text (in both the tidy approach and using the tm package), different ways to visualize this the mined text, creating a document-term matrix and an introduction to one type of analysis you can conduct during text mining: text classification. As a whole, there are multiple ways to analysis mine & analyze text within R. However, for this practical we will discuss some of the techniques covered in the tm package and in the tidytext package, based upon the tidyverse.\nYou can download the student zip including all needed files for practical 9 here. In addition, for this practical, you will need the following packages:\n\n## General Packages\nlibrary(tidyverse)\n\n## Text Mining\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(RColorBrewer)\nlibrary(gutenbergr)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(textdata)\nlibrary(tm)\nlibrary(NLP)\nlibrary(stringi)\nlibrary(e1071)\nlibrary(rpart)\nlibrary(caret)\n\nFor the first part of the practical, we will be using text mined through the Gutenberg Project; briefly this project contains over 60,000 freely accessible eBooks, which through the package gutenberger, can be easily accessed and perfect for text mining and analysis.\nWe will be looking at several books from the late 1800s, in the mindset to compare and contrast the use of language within them. These books include:\n\nAlice’s Adventures in Wonderland by Lewis Carroll\nThe Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson\n\nDespite being from the late 1800s, these books still are popular and hold cultural significance in TV, Movies and the English Language. To access this novel suitable for this practical the following function should be used:\n\nAAIWL &lt;- gutenberg_download(28885) ## 28885 is the eBook number of Alice in Wonderland\nSCJH  &lt;- gutenberg_download(43)    ## 43 is the eBook number of Dr. Jekyll and Mr. Hyde\n\nAfter having loaded all of these books into your working directory (using the code above), examine one of these books using the View() function. When you view any of these data frames, you will see that these have an extremely messy layout and structure. As a result of this complex structure means that conducting any analysis would be extremely challenging, so pre-processing must be undertaken to get this into a format which is usable.\n\n8.11.1 Pre-Processing Text: Tidy approach\nIn order for text to be used effectively within statistical processing and analysis; it must be pre-processed so that it can be uniformly examined. Typical steps of pre-processing include:\n\nRemoving numbers\nConverting to lowercase\nRemoving stop words\nRemoving punctuation\nStemming\n\nThese steps are important as they allow the text to be presented uniformly for analysis (but remember we do not always need all of them); within this practical we will discuss how to undergo some of these steps.\n\n\n8.11.2 Step 1: Tokenization, un-nesting Text\nWhen we previously looked at this text, as we discovered it was extremely messy with it being attached one line per row in the data frame. As such, it is important to un-nest this text so that it attaches one word per row.\nBefore un-nesting text, it is useful to make a note of aspects such as the line which text is on, and the chapter each line falls within. This can be important when examining anthologies or making chapter comparisons as this can be specified within the analysis.\nIn order to specify the line number and chapter of the text, it is possible to use the mutuate function from the dplyr package.\n\nApply the code below, which uses the mutate function, to add line numbers and chapter references one of the books. Next, use the View() function to examine how this has changed the structure.\n\n\n## Template:\ntidy_AAIWL &lt;- AAIWL %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n\ntidy_SCJH &lt;- SCJH  %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE))))\n\n`\nFrom this, it is now possible to pass the function unnest_tokens() in order to split apart the sentence string, and apply each word to a new line. When using this function, you simply need to pass the arguments, word (as this is what you want selecting) and text (the name of the column you want to unnest).\nThe two basic arguments to unnest_tokens used here are column names. First we have the output column name that will be created as the text is unnested into it (word, in this case), and then the input column that the text comes from (text, in this case). Remember that text_df above has a column called text that contains the data of interest.\nAfter using unnest_tokens, we’ve split each row so that there is one token (word) in each row of the new data frame; the default tokenization in unnest_tokens() is for single words, as shown here. Also notice:\nOther columns, such as the line number each word came from, are retained. Punctuation has been stripped. By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).\n\nApply unnest_tokens to your tidied book to unnest this text. Next, once again use the View() function to examine the output.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL &lt;- tidy_AAIWL %&gt;% unnest_tokens(word, text)\n\ntidy_SCJH &lt;- tidy_SCJH %&gt;% unnest_tokens(word, text)\n\nThis results in one word being linked per row of the data frame. The benefit of using the tidytext package in comparison to other text mining packages, is that this automatically applies some of the basic steps to pre-process your text, including removing of capital letters, inter-word punctuation and numbers. However additional pre-processing is required.\n\n\n8.11.3 Intermezzo: Word clouds\nBefore continuing the pre-processing process, let’s have a first look at our text by making a simple visualization using word clouds. Typically these word clouds visualize the frequency of words in a text through relating the size of the displayed words to frequency, with the largest words indicating the most common words.\nTo plot word clouds, we first have to create a data frame containing the word frequencies.\n\nCreate a new data frame, which contains the frequencies of words from the unnested text. To do this, you can make use of the function count().\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL.count &lt;- tidy_AAIWL %&gt;% count(word, sort=TRUE)\n\ntidy_SCJH.count &lt;- tidy_SCJH %&gt;% count(word, sort=TRUE)\n\n\nUsing the wordcloud() function, create a word cloud for your book text. Use the argument max.words within the function to set the maximum number of words to be displayed in the word cloud.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function. Note: Ensure to use the function with(), is used after the piping operator.\n\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))\n\n\n\n\n\n\n\ntidy_SCJH.count %&gt;% with(wordcloud(word, n, max.words = 100))\n\n\n\n\n\n\n\n\n\nDiscuss with another individual or group, whether you can tell what text each word clouds come from, based on the popular words which occur.\n\n\n\n8.11.4 Step 2: Removing stop words\nAs discussed within the lecture, stop words are words in any language which have little or no meaning, and simply connect the words of importance. Such as the, a, also, as, were… etc. To understand the importance of removing these stop words, we can simply do a comparison between the text which has had them removed and those which have not been.\nTo remove the stop words, we use the function anti_join(). This function works through un-joining this table based upon the components, which when passed with the argument stop_words, which is a table containing these words across three lexicons. This removes all the stop words from the presented data frame.\n\nUse the function anti_join() to remove stop words from your tidied text attaching it to a new data frame.\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ndata(\"stop_words\")\ntidy_AAIWL.stop &lt;- tidy_AAIWL %&gt;% anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_SCJH.stop &lt;- tidy_SCJH %&gt;% anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\nIn order to examine the impact of removing these filler words, we can use the count() function to examine the frequencies of different words. This when sorted, will produce a table of frequencies in descending order. An other option is to redo the wordclouds on the updated data frame containing the word counts of the tidied book text without stop words.\n\nUse the function count() to compare the frequencies of words in the dataframes containing the tidied book text with and without stop words (use sort = TRUE within the count() function), or redo the wordclouds. Do you notice a difference in the (top 10) words which most commonly occur in the text?\n\nHint: As with Question 1, ensure to use the piping operator (%&gt;%) to easily apply the function.\n\ntidy_AAIWL.count &lt;- tidy_AAIWL.stop %&gt;% count(word, sort=TRUE)\ntidy_AAIWL.count %&gt;% with(wordcloud(word, n, max.words = 100))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "text-mining.html#vector-space-model-document-term-matrix",
    "href": "text-mining.html#vector-space-model-document-term-matrix",
    "title": "8  Text Mining",
    "section": "8.12 Vector space model: document-term matrix",
    "text": "8.12 Vector space model: document-term matrix\nIn this part of the practical we will build a text classification model for a multiclass classification task. To this end, we first need to perform text preprocessing, then using the idea of vector space model, convert the text data into a document-term (dtm) matrix, and finally train a classifier on the dtm matrix.\nThe data set used in this part of the practical is the BBC News data set. You can use the provided “news_dataset.rda” for this purpose.  This data set consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:\n\nBusiness\nEntertainment\nPolitics\nSport\nTech\n\n\nUse the code below to load the data set and inspect its first rows.\n\n\nload(\"data/news_dataset.rda\")\nhead(df_final)\n\n  File_Name\n1   001.txt\n2   002.txt\n3   003.txt\n4   004.txt\n5   005.txt\n6   006.txt\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Content\n1 Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\\n\\nTime Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\\n\\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n2                                                                                                                                                                                                                                                                                                                        Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\n\\nWorries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are with Rosneft, so it will have to pay real money to the creditors to avoid seizure of Yugansk assets,\" said Moscow-based US lawyer Jamie Firestone, who is not connected to the case. Menatep Group's managing director Tim Osborne told the Reuters news agency: \"If they default, we will fight them where the rule of law exists under the international arbitration clauses of the credit.\"\\n\\nRosneft officials were unavailable for comment. But the company has said it intends to take action against Menatep to recover some of the tax claims and debts owed by Yugansk. Yukos had filed for bankruptcy protection in a US court in an attempt to prevent the forced sale of its main production arm. The sale went ahead in December and Yugansk was sold to a little-known shell company which in turn was bought by Rosneft. Yukos claims its downfall was punishment for the political ambitions of its founder Mikhail Khodorkovsky and has vowed to sue any participant in the sale.\n4                                                                                                                                                               High fuel prices hit BA's profits\\n\\nBritish Airways has blamed high fuel prices for a 40% drop in profits.\\n\\nReporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of £75m ($141m) compared with £125m a year earlier. Rod Eddington, BA's chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by £106m or 47.3%. BA's profits were still better than market expectation of £59m, and it expects a rise in full-year revenues.\\n\\nTo help offset the increased price of aviation fuel, BA last year introduced a fuel surcharge for passengers.\\n\\nIn October, it increased this from £6 to £10 one-way for all long-haul flights, while the short-haul surcharge was raised from £2.50 to £4 a leg. Yet aviation analyst Mike Powell of Dresdner Kleinwort Wasserstein says BA's estimated annual surcharge revenues - £160m - will still be way short of its additional fuel costs - a predicted extra £250m. Turnover for the quarter was up 4.3% to £1.97bn, further benefiting from a rise in cargo revenue. Looking ahead to its full year results to March 2005, BA warned that yields - average revenues per passenger - were expected to decline as it continues to lower prices in the face of competition from low-cost carriers. However, it said sales would be better than previously forecast. \"For the year to March 2005, the total revenue outlook is slightly better than previous guidance with a 3% to 3.5% improvement anticipated,\" BA chairman Martin Broughton said. BA had previously forecast a 2% to 3% rise in full-year revenue.\\n\\nIt also reported on Friday that passenger numbers rose 8.1% in January. Aviation analyst Nick Van den Brul of BNP Paribas described BA's latest quarterly results as \"pretty modest\". \"It is quite good on the revenue side and it shows the impact of fuel surcharges and a positive cargo development, however, operating margins down and cost impact of fuel are very strong,\" he said. Since the 11 September 2001 attacks in the United States, BA has cut 13,000 jobs as part of a major cost-cutting drive. \"Our focus remains on reducing controllable costs and debt whilst continuing to invest in our products,\" Mr Eddington said. \"For example, we have taken delivery of six Airbus A321 aircraft and next month we will start further improvements to our Club World flat beds.\" BA's shares closed up four pence at 274.5 pence.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Pernod takeover talk lifts Domecq\\n\\nShares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.\\n\\nReports in the Wall Street Journal and the Financial Times suggested that the French spirits firm is considering a bid, but has yet to contact its target. Allied Domecq shares in London rose 4% by 1200 GMT, while Pernod shares in Paris slipped 1.2%. Pernod said it was seeking acquisitions but refused to comment on specifics.\\n\\nPernod's last major purchase was a third of US giant Seagram in 2000, the move which propelled it into the global top three of drinks firms. The other two-thirds of Seagram was bought by market leader Diageo. In terms of market value, Pernod - at 7.5bn euros ($9.7bn) - is about 9% smaller than Allied Domecq, which has a capitalisation of £5.7bn ($10.7bn; 8.2bn euros). Last year Pernod tried to buy Glenmorangie, one of Scotland's premier whisky firms, but lost out to luxury goods firm LVMH. Pernod is home to brands including Chivas Regal Scotch whisky, Havana Club rum and Jacob's Creek wine. Allied Domecq's big names include Malibu rum, Courvoisier brandy, Stolichnaya vodka and Ballantine's whisky - as well as snack food chains such as Dunkin' Donuts and Baskin-Robbins ice cream. The WSJ said that the two were ripe for consolidation, having each dealt with problematic parts of their portfolio. Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Japan narrowly escapes recession\\n\\nJapan's economy teetered on the brink of a technical recession in the three months to September, figures show.\\n\\nRevised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\\n\\nThe government was keen to play down the worrying implications of the data. \"I maintain the view that Japan's economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It's painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.\n  Category Complete_Filename\n1 business  001.txt-business\n2 business  002.txt-business\n3 business  003.txt-business\n4 business  004.txt-business\n5 business  005.txt-business\n6 business  006.txt-business\n\n\n\nFind out about the name of the categories and the number of observations in each of them.\n\n\nt_category &lt;- table(df_final$Category)\n\n\nt_name &lt;- table(df_final$File_Name)\n\n\nConvert the data set into a document-term matrix using the function DocumentTermMatrix() and subsequently use the findFreqTerms() function to keep the terms which their frequency is larger than 10. A start of the code is given below. It is also a good idea to apply some text preprocessing, for this inspect the control argument of the function DocumentTermMatrix() (e.g., convert the words into lowercase, remove punctuations, numbers, stopwords, and whitespaces).\n\n\n### set the seed to make your partition reproducible\nset.seed(123)\n\ndf_final$Content &lt;- iconv(df_final$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs &lt;- Corpus(VectorSource(df_final$Content)) #for documenttermmatrix we need a corpus format, a list ot content and the meta data \n\n## alter the code from here onwards\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE))  #lower case\n\n  #in the rows are the entries, in the columns the frequency of a word\n\n\nnot.freq &lt;- findFreqTerms(dtm, lowfreq=11) ## we are not interested in all the words, we are only interested in words that are used more than 10 times\ndtm &lt;- DocumentTermMatrix(docs,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary = not.freq))  #lower case\n\n\nPartition the original data into training and test sets with 80% for training and 20% for test.\n\n\n## define the training partition \ntrain_index &lt;- createDataPartition(df_final$Category, p = .8,  #category because outcome variable\n                                  list = FALSE, \n                                  times = 1)\n\n## split the data using the training partition to obtain training data\ndf_train &lt;- df_final[train_index,]\n\n## remainder of the split is the validation and test data (still) combined \ndf_test &lt;- df_final[-train_index,]\n\n\nCreate separate document-term matrices for the training and the test sets using the previous frequent terms as the input dictionary and convert them into data frames.\n\n\ndf_train$Content &lt;- iconv(df_train$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\ndf_test$Content &lt;- iconv(df_test$Content, from = \"UTF-8\", to = \"ASCII\", sub = \"\")\n\ndocs_train &lt;- Corpus(VectorSource(df_train$Category)) \ndocs_test &lt;- Corpus(VectorSource(df_test$Category)) \n\n\n\n\n## alter the code from here onwards\ndtm_train &lt;- DocumentTermMatrix(docs_train,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n## alter the code from here onwards\ndtm_test &lt;- DocumentTermMatrix(docs_test,\n                          control = list(stopwords= TRUE, \n                                        removeNumbers=TRUE,\n                                        removePunctuation=TRUE,\n                                        whitespace_tokenizer=TRUE,\n                                        tolower=TRUE,\n                                        dictionary= not.freq))  \n\n#make a data frame \ndtm_train  &lt;- as.data.frame(as.matrix(dtm_train))\ndtm_test  &lt;- as.data.frame(as.matrix(dtm_test))\n\n\nUse the cbind function to add the categories to the train_dtm data and name the column y.\n\n\ny &lt;- df_train$Category\ndtm_train &lt;- cbind(dtm_train,y)\ntable(dtm_train[5,6216])\n\n\nbusiness \n       1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  }
]